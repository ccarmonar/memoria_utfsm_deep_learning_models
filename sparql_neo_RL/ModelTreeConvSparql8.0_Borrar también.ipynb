{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBRERIAS y CONSTANTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS CUDA AVAILABLE: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import json, ast, sys, csv, random\n",
    "import plotly.express as px\n",
    "import math\n",
    "import datetime\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#Implement training process\n",
    "from model_trees_algebra import NeoRegression\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functions.tree_format import IterateBuildTree, InnerJoinsIntraBGPS, \\\n",
    "                                IterateBuildTreeBetweenBGPS, TreeFormat, TreeFormat_all\n",
    "\n",
    "from functions.aux import MetricTotalAccuraccy\n",
    "\n",
    "\n",
    "class BaoTrainingException(Exception):\n",
    "    pass\n",
    "csv.field_size_limit(sys.maxsize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"/media/data/ccarmona/memoria/dataset/\"\n",
    "csv_name = 'new_dataset_7.1_subqueries'\n",
    "x = [True,False]\n",
    "active_new_data = x[0]\n",
    "symbol = \"ᶲ\"\n",
    "optimizer = \"Adam\"\n",
    "#Este parametro sirve para elegir cierta cantidad de data ordenado por rangos de tiempo obtenidos.\n",
    "## Entre más bajo menos data se seleccionara. Si es muy alto se tendran demasiados valores outliners, \n",
    "## pero si es muy bajo podría tenerse una data no representativa y se aumenta el riesgo de overfitting.\n",
    "## Por otro lado min_data, simplemente da el valor minimo de tiempo de ejecución que tiene una consulta tomada\n",
    "## en cuenta para hacer el modelo\n",
    "#percent_of_data_or = 0.93\n",
    "#min_time_or = 15\n",
    "#max_time_or = 80\n",
    "#percent_of_data = 1\n",
    "#min_time = 5\n",
    "#max_time = 80\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to use.\n",
    "list_columns = ['total_bgps', 'triples', 'treesize', 'join', 'left_join']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = pd.read_csv(URL + csv_name + '_ds_train_ft.csv', engine='python', encoding='utf-8')\n",
    "ds_val = pd.read_csv(URL + csv_name + '_ds_val_ft.csv', engine='python', encoding='utf-8')\n",
    "ds_test = pd.read_csv(URL + csv_name + '_ds_test_ft.csv', engine='python', encoding='utf-8')\n",
    "ds_rl = pd.read_csv(URL + csv_name + '_ds_rl_ft.csv', engine='python', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ds_train.shape (9594, 15)\n",
      "ds_val.shape (4122, 15)\n",
      "ds_test.shape (1877, 15)\n",
      "ds_rl.shape (2531, 67)\n"
     ]
    }
   ],
   "source": [
    "print(\"ds_train.shape\",ds_train.shape)\n",
    "print(\"ds_val.shape\",ds_val.shape)\n",
    "print(\"ds_test.shape\",ds_test.shape)\n",
    "print(\"ds_rl.shape\",ds_rl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getpredictions_info(x_val_tree, x_val_query, y_val):\n",
    "    \"\"\"\n",
    "    Get statistics by a set of data. Need the previous trained model(availablre  form reg object).\n",
    "    :param x_val_tree: Plan level features.\n",
    "    :param x_val_query: Query level features.\n",
    "    :param y_val: Real execution time\n",
    "    :return: Dict with predictions and metrics (mae, rmse, mse)\n",
    "    \"\"\"\n",
    "    Xt, Xq, Yv = reg.json_loads(x_val_tree, x_val_query.values, y_val)\n",
    "    Xt = [reg.fix_tree(x) for x in Xt]\n",
    "    Xt = reg.tree_transform.transform(Xt)\n",
    "\n",
    "    pairs_val = list(zip(list(zip(Xt, Xq)), Yv))\n",
    "    dataset_val = DataLoader(pairs_val, batch_size=batch_size, num_workers=0, shuffle=False, collate_fn=reg.collate_with_card)\n",
    "    results_val = reg.predict_best(dataset_val)\n",
    "    y_pred_val, y_real_val = zip(*results_val)\n",
    "    mseval = mean_squared_error(y_real_val, y_pred_val)\n",
    "    maeval = mean_absolute_error(y_real_val, y_pred_val)\n",
    "    rmseval = np.sqrt(mseval)\n",
    "    return {\"pred\": y_pred_val, \"real\" : y_real_val, \"mse\": mseval, \"mae\": maeval, \"rmse\": rmseval, \"history\": reg.history}\n",
    "\n",
    "def getpredictions_info_nojc(x_val_tree, x_val_query, y_val):\n",
    "    \"\"\"\n",
    "    Get statistics by a set of data. Need the previous trained model(availablre  form reg object).\n",
    "    :param x_val_tree: Plan level features.\n",
    "    :param x_val_query: Query level features.\n",
    "    :param y_val: Real execution time\n",
    "    :return: Dict with predictions and metrics (mae, rmse, mse)\n",
    "    \"\"\"\n",
    "    Xt, Xq, Yv = reg.json_loads(x_val_tree, x_val_query.values, y_val)\n",
    "    Xt = [reg.fix_tree(x) for x in Xt]\n",
    "    Xt = reg.tree_transform.transform(Xt)\n",
    "\n",
    "    pairs_val = list(zip(list(zip(Xt, Xq)), Yv))\n",
    "    dataset_val = DataLoader(pairs_val, batch_size=batch_size, num_workers=0, shuffle=False, collate_fn=reg.collate)\n",
    "    results_val = reg.predict_best(dataset_val)\n",
    "    y_pred_val, y_real_val = zip(*results_val)\n",
    "    mseval = mean_squared_error(y_real_val, y_pred_val)\n",
    "    maeval = mean_absolute_error(y_real_val, y_pred_val)\n",
    "    rmseval = np.sqrt(mseval)\n",
    "    return {\"pred\": y_pred_val, \"real\" : y_real_val, \"mse\": mseval, \"mae\": maeval, \"rmse\": rmseval, \"history\": reg.history}\n",
    "\n",
    "\n",
    "\n",
    "def getmax(x):\n",
    "    lista=  list(x.values())\n",
    "    maximo = 0\n",
    "    for el in lista:\n",
    "        if (maximo < float(el)):\n",
    "            maximo = float(el)\n",
    "    return maximo\n",
    "\n",
    "def pred2index_dict(x, pred_to_index, maxcardinality):\n",
    "    \"\"\"\n",
    "    get histogram from cardinality features. the values is normalized using the max cardinality of predicate in dataset.\n",
    "    :param x: Tree data from x row sample.\n",
    "    :param pred_to_index: dict with predicates and their index.\n",
    "    :param maxcardinality: Max cardiniality in the dataset.\n",
    "    :return: dictionary with feature json_cardinality.\n",
    "    \"\"\"\n",
    "    resp = {}\n",
    "    x = json.loads(x)\n",
    "    for el in x.keys():\n",
    "        if el in pred_to_index:\n",
    "            resp[pred_to_index[el]] = float(x[el])/maxcardinality\n",
    "    return resp\n",
    "\n",
    "def prepare_query_level_data(x_train_query, x_val_query, x_test_query):\n",
    "    \"\"\" Apply StandardScaller to columns except for json_cardinality that need other proccess\"\"\"\n",
    "    maxcardinality =  x_train_query['json_cardinality'].apply(lambda x: json.loads(x)).apply(lambda x: getmax(x)).max()\n",
    "    #Scale x_query data.\n",
    "    xqtrain = x_train_query.drop(columns=['json_cardinality'])\n",
    "    xqval   = x_val_query.drop(columns=['json_cardinality'])\n",
    "    xqtest   = x_test_query.drop(columns=['json_cardinality'])\n",
    "\n",
    "    scalerx = StandardScaler()\n",
    "    x_train_scaled = scalerx.fit_transform(xqtrain)\n",
    "    x_val_scaled = scalerx.transform(xqval)\n",
    "    x_test_scaled = scalerx.transform(xqtest)\n",
    "\n",
    "    x_train_query =pd.concat([pd.DataFrame(x_train_scaled, index=xqtrain.index, columns=xqtrain.columns),x_train_query[['json_cardinality']]], axis=1)\n",
    "    x_val_query =  pd.concat([pd.DataFrame(x_val_scaled,   index=xqval.index, columns=xqval.columns),x_val_query[['json_cardinality']]], axis=1)\n",
    "    x_test_query =  pd.concat([pd.DataFrame(x_test_scaled,   index=xqtest.index, columns=xqtest.columns),x_test_query[['json_cardinality']]], axis=1)\n",
    "\n",
    "    x_train_query['json_cardinality'] = x_train_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(),maxcardinality))\n",
    "    x_val_query['json_cardinality'] = x_val_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(), maxcardinality))\n",
    "    x_test_query['json_cardinality'] = x_test_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(), maxcardinality))\n",
    "\n",
    "    \n",
    "    \n",
    "    return x_train_query, x_val_query, x_test_query\n",
    "\n",
    "def prepare_query_level_only_json_cardinality(x_train_query, x_val_query, x_test_query, x_rl_query):\n",
    "    \"\"\" Apply StandardScaller to columns except for json_cardinality that need other proccess\"\"\"\n",
    "    maxcardinality =  x_train_query['json_cardinality'].apply(lambda x: json.loads(x)).apply(lambda x: getmax(x)).max()\n",
    "    \n",
    "    x_train_query = x_train_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(),maxcardinality))\n",
    "    x_val_query = x_val_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(), maxcardinality))\n",
    "    x_test_query = x_test_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(), maxcardinality))\n",
    "    x_rl_query = x_rl_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(), maxcardinality))\n",
    "    \n",
    "    \n",
    "    return x_train_query, x_val_query, x_test_query, x_rl_query\n",
    "\n",
    "\n",
    "def prepare_query_level_data_full(x_train_query, x_val_query, x_test_query, x_rl_query):\n",
    "    \"\"\" Apply StandardScaller to columns except for json_cardinality that need other proccess\"\"\"\n",
    "    maxcardinality =  x_train_query['json_cardinality'].apply(lambda x: json.loads(x)).apply(lambda x: getmax(x)).max()\n",
    "    #Scale x_query data.\n",
    "    xqtrain = x_train_query.drop(columns=['json_cardinality'])\n",
    "    xqval   = x_val_query.drop(columns=['json_cardinality'])\n",
    "    xqtest   = x_test_query.drop(columns=['json_cardinality'])\n",
    "    xqrl = x_rl_query.drop(columns=['json_cardinality'])\n",
    "    \n",
    "    scalerx = StandardScaler()\n",
    "    x_train_scaled = scalerx.fit_transform(xqtrain)\n",
    "    x_val_scaled = scalerx.transform(xqval)\n",
    "    x_test_scaled = scalerx.transform(xqtest)\n",
    "    x_rl_scaled = scalerx.transform(xqrl)\n",
    "    \n",
    "    \n",
    "    x_train_query = pd.concat([pd.DataFrame(x_train_scaled, index=xqtrain.index, columns=xqtrain.columns),x_train_query[['json_cardinality']]], axis=1)\n",
    "    x_val_query =  pd.concat([pd.DataFrame(x_val_scaled,   index=xqval.index, columns=xqval.columns),x_val_query[['json_cardinality']]], axis=1)\n",
    "    x_test_query =  pd.concat([pd.DataFrame(x_test_scaled,   index=xqtest.index, columns=xqtest.columns),x_test_query[['json_cardinality']]], axis=1)\n",
    "    x_rl_query = pd.concat([pd.DataFrame(x_rl_scaled,   index=xqrl.index, columns=xqrl.columns),x_rl_query[['json_cardinality']]], axis=1)\n",
    "    \n",
    "    \n",
    "    x_train_query['json_cardinality'] = x_train_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(),maxcardinality))\n",
    "    x_val_query['json_cardinality'] = x_val_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(), maxcardinality))\n",
    "    x_test_query['json_cardinality'] = x_test_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(), maxcardinality))\n",
    "    x_rl_query['json_cardinality'] = x_rl_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(), maxcardinality))\n",
    "    \n",
    "    \n",
    "    return x_train_query, x_val_query, x_test_query, x_rl_query\n",
    "\n",
    "def prepare_query_level_data_full_no_jc(x_train_query, x_val_query, x_test_query, x_rl_query):\n",
    "    \"\"\" Apply StandardScaller to columns except for json_cardinality that need other proccess\"\"\"\n",
    "    scalerx = StandardScaler()\n",
    "    columns_train, index_train = x_train_query.columns, x_train_query.index\n",
    "    columns_val, index_val = x_val_query.columns, x_val_query.index\n",
    "    columns_test, index_test = x_test_query.columns, x_test_query.index\n",
    "    columns_rl, index_rl = x_rl_query.columns, x_rl_query.index\n",
    "    \n",
    "    x_train_scaled = scalerx.fit_transform(x_train_query)\n",
    "    x_val_scaled = scalerx.transform(x_val_query)\n",
    "    x_test_scaled = scalerx.transform(x_test_query)\n",
    "    x_rl_scaled = scalerx.transform(x_rl_query)\n",
    "    \n",
    "    x_train_query = pd.DataFrame(x_train_scaled,   index=index_train, columns=columns_test)\n",
    "    x_val_query = pd.DataFrame(x_val_scaled,   index=index_val, columns=columns_val)\n",
    "    x_test_query = pd.DataFrame(x_test_scaled,   index=index_test, columns=columns_test)\n",
    "    x_rl_query = pd.DataFrame(x_rl_scaled,   index=index_rl, columns=columns_rl)\n",
    "    return x_train_query, x_val_query, x_test_query, x_rl_query\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### TreeConv Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size Train: 9594, Val 4122\n"
     ]
    }
   ],
   "source": [
    "folds_execution = {}\n",
    "print(\"Size Train: {}, Val {}\".format(ds_train.shape[0], ds_val.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get query level data\n",
    "x_train_query = ds_train[list_columns]\n",
    "x_val_query   = ds_val[list_columns]\n",
    "\n",
    "x_train_query_str = ds_train['query']\n",
    "x_val_query_str = ds_train['query']\n",
    "\n",
    "# get plan level datba\n",
    "x_train_tree = ds_train['trees'].values\n",
    "x_val_tree = ds_val['trees'].values\n",
    "\n",
    "y_train = ds_train['time'].values\n",
    "y_val = ds_val['time'].values\n",
    "\n",
    "x_test_tree = ds_test['trees'].values\n",
    "y_test = ds_test['time'].values\n",
    "x_test_query   = ds_test[list_columns]\n",
    "\n",
    "x_rl_tree = ds_rl['trees'].values\n",
    "y_rl = ds_rl['time'].values\n",
    "x_rl_query   = ds_rl[list_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------SHAPES-----------\n",
      "----------CLEAN-----------\n",
      "shape ds_train: (9594, 15)\n",
      "shape ds_val  : (4122, 15)\n",
      "shape ds_test : (1877, 15)\n",
      "shape ds_rl : (2531, 67)\n",
      "\n",
      "-----TRAIN AND VAL DATA-----\n",
      "----------x_query_data----------\n",
      "shape x_val_query  : (4122, 5)\n",
      "shape x_train_query: (9594, 5)\n",
      "----------x_plan_level_data----------\n",
      "shape x_val_tree  : (4122,)\n",
      "shape x_train_tree: (9594,)\n",
      "----------y_data------------\n",
      "shape y_val  : (4122,)\n",
      "shape y_train: (9594,)\n",
      "\n",
      "----------TEST DATA----------\n",
      "shape x_test_tree : (1877,)\n",
      "shape x_test_query: (1877, 5)\n",
      "shape y_test      : (1877,)\n",
      "-----------------------\n",
      "\n",
      "----------RL DATA----------\n",
      "shape x_rl_tree : (2531,)\n",
      "shape x_rl_query: (2531, 5)\n",
      "shape y_rl      : (2531,)\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"---------SHAPES-----------\")\n",
    "print(\"----------CLEAN-----------\")\n",
    "print(f'shape ds_train: {ds_train.shape}')\n",
    "print(f'shape ds_val  : {ds_val.shape}')\n",
    "print(f'shape ds_test : {ds_test.shape}')\n",
    "print(f'shape ds_rl : {ds_rl.shape}')\n",
    "print(\"\")\n",
    "print(\"-----TRAIN AND VAL DATA-----\")\n",
    "print(\"----------x_query_data----------\")\n",
    "print(f'shape x_val_query  : {x_val_query.shape}')\n",
    "print(f'shape x_train_query: {x_train_query.shape}')\n",
    "print(\"----------x_plan_level_data----------\")\n",
    "print(f'shape x_val_tree  : {x_val_tree.shape}')\n",
    "print(f'shape x_train_tree: {x_train_tree.shape}')\n",
    "print(\"----------y_data------------\")\n",
    "print(f'shape y_val  : {y_val.shape}')\n",
    "print(f'shape y_train: {y_train.shape}')\n",
    "print(\"\")\n",
    "print(\"----------TEST DATA----------\")\n",
    "print(f'shape x_test_tree : {x_test_tree.shape}')\n",
    "print(f'shape x_test_query: {x_test_query.shape}')\n",
    "print(f'shape y_test      : {y_test.shape}')\n",
    "print(\"-----------------------\")\n",
    "print(\"\")\n",
    "print(\"----------RL DATA----------\")\n",
    "print(f'shape x_rl_tree : {x_rl_tree.shape}')\n",
    "print(f'shape x_rl_query: {x_rl_query.shape}')\n",
    "print(f'shape y_rl      : {y_rl.shape}')\n",
    "print(\"-----------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxcardinality = 0\n",
    "#maxcardinality =  x_train_query['json_cardinality'].apply(lambda x: json.loads(x)).apply(lambda x: getmax(x)).max()\n",
    "maxcardinality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeoRegression\n",
    "Esta en model_trees_algebra.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END PREPARE QUERY LEVEL DATA\n"
     ]
    }
   ],
   "source": [
    "#with io.capture_output() as captured:\n",
    "#x_train_query, x_val_query, x_test_query, x_rl_query =  prepare_query_level_data_full(x_train_query, x_val_query, x_test_query, x_rl_query)\n",
    "x_train_query, x_val_query, x_test_query, x_rl_query =  prepare_query_level_data_full_no_jc(x_train_query, x_val_query, x_test_query, x_rl_query)\n",
    "print(\"END PREPARE QUERY LEVEL DATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_554672/2844858741.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Save stats in val set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile_to_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./execution_model_stats_fullADAM-7.0_V5.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetpredictions_info_nojc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_to_store\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mfile_to_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_554672/346242396.py\u001b[0m in \u001b[0;36mgetpredictions_info_nojc\u001b[0;34m(x_val_tree, x_val_query, y_val)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \"\"\"\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_loads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reg' is not defined"
     ]
    }
   ],
   "source": [
    "#Save best model\n",
    "#import torch\n",
    "#torch.save(reg.best_model.state_dict(), \"./best_model_fullADAM-7.0_V5.pt\")\n",
    "#Save stats in val set\n",
    "file_to_store = open(\"./execution_model_stats_fullADAM-7.0_V5.pickle\", \"wb\")\n",
    "pickle.dump(getpredictions_info_nojc(x_val_tree, x_val_query, y_val), file_to_store)\n",
    "file_to_store.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#val_stats = getpredictions_info(x_val_tree, x_val_query, y_val)\n",
    "val_stats = getpredictions_info_nojc(x_val_tree, x_val_query, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "ds_val['y_pred'] = val_stats['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_stats = getpredictions_info_nojc(x_test_tree, x_test_query, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "ds_test['y_realcheck'] = test_stats['real']\n",
    "ds_test['y_pred'] = test_stats['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def tag_points(x):\n",
    "    \"\"\"Add quality tags of predictions. Used to plot with plotly\"\"\"\n",
    "    difference = x['time'] - x['y_pred'][0]\n",
    "    abs_diff = np.abs(difference)\n",
    "    x['y_pred'] = x['y_pred'][0]\n",
    "    x['query2'] = x['query'].replace(\" . \", ' . <br>').replace(\" FILTER\", '<br> FILTER').replace(\" { \", ' { <br>').replace(\" } \", ' <br> }').replace(\" ; \", ' ; <br>') \n",
    "    p20 = x['time'] * 0.2\n",
    "    p40 = x['time'] * 0.4\n",
    "    if abs_diff < p20:\n",
    "        x['color'] = \"good prediction\"\n",
    "    elif abs_diff < p40:\n",
    "        x['color'] = \"aceptable prediction\"\n",
    "    else:\n",
    "        x['color'] = \"bad prediction\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_points2(x):\n",
    "    \"\"\"Add quality tags of predictions. Used to plot with plotly\"\"\"\n",
    "    difference = x['time'] - x['y_realcheck'][0]\n",
    "    abs_diff = np.abs(difference)\n",
    "    x['y_realcheck'] = x['y_realcheck'][0]\n",
    "    x['query2'] = x['query'].replace(\" . \", ' . <br>').replace(\" FILTER\", '<br> FILTER').replace(\" { \", ' { <br>').replace(\" } \", ' <br> }').replace(\" ; \", ' ; <br>') \n",
    "    p20 = x['time'] * 0.2\n",
    "    p40 = x['time'] * 0.4\n",
    "    if abs_diff < p20:\n",
    "        x['color'] = \"good prediction\"\n",
    "    elif abs_diff < p40:\n",
    "        x['color'] = \"aceptable prediction\"\n",
    "    else:\n",
    "        x['color'] = \"bad prediction\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#tag_points(ds_test.values[0])\n",
    "other = ds_test.apply(lambda x: tag_points(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "other.to_pickle(\"./predictions_test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "otherval = ds_val.apply(lambda x: tag_points(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "otherval.to_pickle(\"./predictions_val.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = px.scatter(otherval[['query','query2','time','y_pred','color']], x=\"y_pred\", y=\"time\", color=\"color\", hover_data=['query2'])\n",
    "fig.update_layout(height=800, width=1000, title_text=\"Predictions on Val Set\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = px.scatter(other[['query','query2','time','y_pred','y_realcheck','color']], x=\"y_pred\", y=\"time\", color=\"color\", hover_data=['query2'])\n",
    "fig.update_layout(height=800, width=1000, title_text=\"Predictions on Test Set\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_val,b_val,a_val,g_val,bp_val,ap_val,gp_val = MetricTotalAccuraccy(otherval)\n",
    "print(f\"Total predictions: {tot_val}\")\n",
    "print(f\"Bad predictions: {b_val}, percentage {bp_val}%\")\n",
    "print(f\"Acceptable predictions: {a_val}, percentage {ap_val}%\")\n",
    "print(f\"Good predictions: {g_val}, percentage {gp_val}%\")\n",
    "\n",
    "print(f\"Accuraccy: {100*(a_val+g_val)/tot_val}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_val,b_val,a_val,g_val,bp_val,ap_val,gp_val = MetricTotalAccuraccy(other)\n",
    "print(f\"Total predictions: {tot_val}\")\n",
    "print(f\"Bad predictions: {b_val}, percentage {bp_val}%\")\n",
    "print(f\"Acceptable predictions: {a_val}, percentage {ap_val}%\")\n",
    "print(f\"Good predictions: {g_val}, percentage {gp_val}%\")\n",
    "\n",
    "print(f\"Accuraccy: {100*(a_val+g_val)/tot_val}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTriplesSubtree(subtree_as_str):\n",
    "    code_tpf = ['VAR_VAR_VAR', 'VAR_VAR_URI', 'VAR_URI_VAR', 'VAR_URI_URI', 'VAR_URI_LITERAL', 'VAR_VAR_LITERAL',\n",
    "                'URI_URI_LITERAL', 'URI_URI_VAR', 'URI_URI_URI', 'URI_VAR_VAR', 'URI_VAR_URI', 'URI_VAR_LITERAL',\n",
    "                'LITERAL_URI_VAR', 'LITERAL_URI_URI', 'LITERAL_URI_LITERAL']\n",
    "    total_triples = 0\n",
    "    for i in code_tpf:\n",
    "        total_triples += subtree_as_str.count(i)\n",
    "    #if subtree_as_str in code_tpf:\n",
    "    #    total_triples += 1\n",
    "    return total_triples\n",
    "\n",
    "def GetTreeSize(subtrees, treesize):\n",
    "    if len(subtrees) == 1:\n",
    "        return treesize\n",
    "    else:\n",
    "        treesize += 1\n",
    "        left_treesize = GetTreeSize(subtrees[1], treesize)\n",
    "        right_treesize = GetTreeSize(subtrees[2], treesize)\n",
    "        treesize = max(left_treesize,right_treesize)\n",
    "    return treesize\n",
    "\n",
    "def GetAllJoins(subtree_as_str):\n",
    "    join = subtree_as_str.count('JOIN')\n",
    "    left_join = subtree_as_str.count('LEFT_JOIN')\n",
    "    return join-left_join, left_join\n",
    "\n",
    "def GetIter(subtree_as_str):\n",
    "    if \"iter\" in subtree_as_str:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def GetTotalBgp(state):\n",
    "    bgp_list = []\n",
    "    for s in state:\n",
    "        bgp_list.append(s[1])\n",
    "    bgp_list = set(bgp_list)\n",
    "    return len(bgp_list)\n",
    "\n",
    "def GetDataframe(subtree, state, columns):\n",
    "    subtree_str = str(subtree)\n",
    "    subtree_list = ast.literal_eval(subtree[0].tolist())\n",
    "    total_bgps = GetTotalBgp(state)\n",
    "    treesize = GetTreeSize(subtree_list, 1)\n",
    "    triples = GetTriplesSubtree(subtree_str)\n",
    "    join, left_join = GetAllJoins(subtree_str)\n",
    "    iters = GetIter(subtree_str)\n",
    "    \n",
    "    values = [total_bgps, triples, treesize, join, left_join]\n",
    "    x_rl_query = pd.DataFrame([values], columns = columns)\n",
    "    return x_rl_query\n",
    "    #print(x_rl_query)\n",
    "    #scalerx = StandardScaler()\n",
    "    #values_scaled = scalerx.fit_transform(x_rl_query)\n",
    "    #x_rl_query = pd.DataFrame(values_scaled, columns = columns)\n",
    "    #print(x_rl_query)\n",
    "    \n",
    "def RL_Actions(bgp):\n",
    "    actions = []\n",
    "    idx = 0\n",
    "    for k,v in bgp.items():\n",
    "        bgp_list = v['bgp_list']\n",
    "        for b in v['bgp_list']:\n",
    "            actions.append((idx,k,b['P'],b['triple_type']))\n",
    "            idx += 1\n",
    "    return actions\n",
    "\n",
    "\n",
    "def RL_Initial_Step(actions):\n",
    "    initial_state = []\n",
    "    random_index = random.randint(0, len(actions)-1)\n",
    "    random_action = actions[random_index]\n",
    "    initial_state.append(random_action)\n",
    "    return initial_state\n",
    "\n",
    "def RL_available_actions(actions, current_state):\n",
    "    if not current_state:\n",
    "        return actions\n",
    "    available_actions = sorted(list(set(actions) - set(current_state)))\n",
    "    same_bgp_actions = []\n",
    "    other_bgp_actions = []\n",
    "    for i in available_actions:\n",
    "        if current_state[-1][1] == i[1]:\n",
    "            same_bgp_actions.append(i)\n",
    "        else:\n",
    "            other_bgp_actions.append(i)\n",
    "    if same_bgp_actions:\n",
    "        available_actions = same_bgp_actions\n",
    "    else:\n",
    "        available_actions = other_bgp_actions\n",
    "    return available_actions\n",
    "\n",
    "\n",
    "def RL_Argmax(array):\n",
    "    argmax_list = np.argwhere(array == np.amax(array))\n",
    "    argmax_list_flatten = [item for sublist in argmax_list for item in sublist]\n",
    "    return int(random.choice(argmax_list_flatten))\n",
    "\n",
    "def RL_Argmax_available(q_value,available_actions):\n",
    "    available_idx = [i[0] for i in available_actions]\n",
    "    #print(\"available_actions\",available_actions)\n",
    "    #print(\"available_idx\",available_idx)\n",
    "    q_value_available = [q_value[i] for i in available_idx]\n",
    "    #print(\"q_values\",q_value, type(q_value))\n",
    "    #print(\"q_values_available\",q_value_available, type(q_value_available))\n",
    "    \n",
    "    argmax_list = np.argwhere(q_value_available == np.amax(q_value_available))\n",
    "    argmax_list_flatten = [item for sublist in argmax_list for item in sublist]\n",
    "    #print(\"argmax_list\",argmax_list, type(argmax_list))\n",
    "    #print(\"argmax_list_flatten\",argmax_list_flatten, type(argmax_list_flatten))\n",
    "    value = int(random.choice(argmax_list_flatten))\n",
    "    #print(\"value\",value)\n",
    "    \n",
    "    #print(\"..........................\")\n",
    "    return value\n",
    "\n",
    "\n",
    "def RL_Next_step(actions,\n",
    "                current_state,\n",
    "                reward,\n",
    "                q_value,\n",
    "                x_rl_tree_ind,\n",
    "                x_rl_query_ind,\n",
    "                y_rl_ind,\n",
    "                bgp_ind,\n",
    "                epsilon\n",
    "                ):\n",
    "    random_num = np.random.random()\n",
    "    available_actions = RL_available_actions(actions, current_state)\n",
    "    actual_state = len(current_state) - 1\n",
    "    \n",
    "    if random_num < epsilon:\n",
    "        chosen_action_available_idx = random.randint(0, len(available_actions)-1)\n",
    "        chosen_action = available_actions[chosen_action_available_idx]\n",
    "        chosen_action_idx = chosen_action[0]\n",
    "        #print(chosen_action_idx)\n",
    "        current_state.append(chosen_action)\n",
    "        reward, terminal = RL_Reward(actions,\n",
    "                                     available_actions,\n",
    "                                     current_state,\n",
    "                                     chosen_action,\n",
    "                                     reward,\n",
    "                                     x_rl_tree_ind,\n",
    "                                     x_rl_query_ind,\n",
    "                                     y_rl_ind,\n",
    "                                     bgp_ind)\n",
    "    else:\n",
    "        #print(current_state)\n",
    "        chosen_action_available_idx = RL_Argmax_available(q_value[actual_state],available_actions)\n",
    "        chosen_action = available_actions[chosen_action_available_idx]\n",
    "        chosen_action_idx = chosen_action[0]\n",
    "        current_state.append(chosen_action)\n",
    "        reward, terminal = RL_Reward(actions,\n",
    "                                     available_actions,\n",
    "                                     current_state,\n",
    "                                     chosen_action,\n",
    "                                     reward,\n",
    "                                     x_rl_tree_ind,\n",
    "                                     x_rl_query_ind,\n",
    "                                     y_rl_ind,\n",
    "                                     bgp_ind)\n",
    "    return current_state, reward, terminal, chosen_action, chosen_action_idx\n",
    "\n",
    "def RL_Reward(actions,\n",
    "              available_actions,\n",
    "              current_state,\n",
    "              chosen_action,\n",
    "              reward,\n",
    "              x_rl_tree_ind,\n",
    "              x_rl_query_ind,\n",
    "              y_rl_ind,\n",
    "              bgp_ind):\n",
    "    terminal = False\n",
    "    #print(available_actions)\n",
    "    #if chosen_action not in available_actions:\n",
    "    #    reward -= 100000\n",
    "    if len(current_state) == len(actions):\n",
    "        terminal = True\n",
    "    new_dicto = RL_Rebuild_Dictionary(bgp_ind, current_state)\n",
    "    #tree_format = TreeFormat_all(new_dicto,symbol)\n",
    "    tree_format = TreeFormat(new_dicto,symbol)\n",
    "    #tree_format = ast.literal_eval(tree_format)\n",
    "    new_tree = np.array([str(tree_format).replace('\"', ';').replace(\"'\", '\"')])\n",
    "    x_rl_query = GetDataframe(new_tree, current_state, x_rl_query_ind.columns)\n",
    "    pred = getpredictions_info_nojc(new_tree, x_rl_query, y_rl_ind)['pred']\n",
    "    reward -= pred[0][0]\n",
    "    return reward, terminal\n",
    "\n",
    "def RL_Rebuild_Dictionary(bgp, final_state):\n",
    "    new_dicto = {}\n",
    "    bgp_names = list(set([i[1] for i in final_state]))\n",
    "    ### Keys\n",
    "    for i in bgp_names:\n",
    "        new_dicto[i] = {\"bgp_list\" : [], \"opt\" : bgp[i]['opt']}\n",
    "    for i in final_state:\n",
    "        new_dicto[i[1]][\"bgp_list\"].append({'P' : i[2], 'triple_type' : i[3]})\n",
    "\n",
    "    return new_dicto\n",
    "\n",
    "\n",
    "def RL_First_Policy(actions):\n",
    "    actions_length = len(actions)\n",
    "    return np.zeros((actions_length,actions_length))\n",
    "\n",
    "def RL_bgp_format(ds_rl):\n",
    "    bgps_rl = ds_rl['bgps'].values\n",
    "    bgps_rl = [ast.literal_eval(bgp) for bgp in bgps_rl]\n",
    "    return bgps_rl\n",
    "\n",
    "def RL_get_data(x_rl_tree, x_rl_query, y_rl, bgps_rl,idx):\n",
    "    x_rl_tree_c = x_rl_tree.copy()\n",
    "    x_rl_query_c = x_rl_query.copy()\n",
    "    y_rl_c = y_rl.copy()\n",
    "    bgps_rl_c = bgps_rl.copy()\n",
    "    x_rl_query_c = x_rl_query_c.reset_index(drop=True)\n",
    "    columns = x_rl_query_c.columns\n",
    "    values = x_rl_query_c.values[idx]    \n",
    "    x_rl_tree_test = np.array([x_rl_tree_c[idx]])\n",
    "    x_rl_query_test = pd.DataFrame([values],columns=columns)\n",
    "    y_rl_test = np.array([y_rl_c[idx]])\n",
    "    bgps_test = bgps_rl_c[idx]\n",
    "    return x_rl_tree_test, x_rl_query_test, y_rl_test, bgps_test\n",
    "\n",
    "\n",
    "def RL_get_final_state_bgp_tree(q_value,actions,bgp,symbol):\n",
    "    q_value = q_value.tolist()\n",
    "    best_state = []\n",
    "    arg_max = RL_Argmax(q_value[0])\n",
    "    best_state.append(actions[arg_max])\n",
    "        \n",
    "    for q_val in q_value[1:]:\n",
    "        available_actions = RL_available_actions(actions, best_state)\n",
    "        chosen_action_available_idx = RL_Argmax_available(q_val,available_actions)\n",
    "        chosen_action = available_actions[chosen_action_available_idx]\n",
    "        chosen_action_idx = chosen_action[0]\n",
    "        best_state.append(chosen_action) \n",
    "    new_dicto = RL_Rebuild_Dictionary(bgp, best_state)\n",
    "    #new_tree = np.array([str(TreeFormat_all(new_dicto,symbol)).replace('\"', ';').replace(\"'\", '\"')])\n",
    "    new_tree = np.array([str(TreeFormat(new_dicto,symbol)).replace('\"', ';').replace(\"'\", '\"')])\n",
    "    \n",
    "    return best_state, new_dicto, new_tree\n",
    "\n",
    "\n",
    "def RL_results_functions(idx,pred_old, pred_new,prl_tol1,prl_tol2):\n",
    "    difference = pred_new - pred_old\n",
    "    abs_diff = np.abs(difference)\n",
    "    best_of_the_pred = []\n",
    "    similar_of_the_pred = []\n",
    "    worst_of_the_pred = []\n",
    "    if abs_diff < prl_tol1:\n",
    "        best_of_the_pred.append(idx)\n",
    "    elif abs_diff < prl_tol2:\n",
    "        similar_of_the_pred.append(idx)\n",
    "    else:\n",
    "        worst_of_the_pred.append(idx)\n",
    "    return best_of_the_pred, similar_of_the_pred, worst_of_the_pred\n",
    "    \n",
    "def RLNeo_Next_step(actions,\n",
    "                current_state,\n",
    "                reward,\n",
    "                q_value,\n",
    "                x_rl_tree_ind,\n",
    "                x_rl_query_ind,\n",
    "                y_rl_ind,\n",
    "                bgp_ind,\n",
    "                epsilon\n",
    "                ):\n",
    "    random_num = np.random.random()\n",
    "    available_actions = RL_available_actions(actions, current_state)\n",
    "    actual_state = len(current_state) - 1\n",
    "    \n",
    "    if random_num < epsilon:\n",
    "        chosen_action_available_idx = random.randint(0, len(available_actions)-1)\n",
    "        chosen_action = available_actions[chosen_action_available_idx]\n",
    "        chosen_action_idx = chosen_action[0]\n",
    "        #print(chosen_action_idx)\n",
    "        current_state.append(chosen_action)\n",
    "        reward, terminal = RLNeo_Reward(actions,\n",
    "                                     available_actions,\n",
    "                                     current_state,\n",
    "                                     chosen_action,\n",
    "                                     reward,\n",
    "                                     x_rl_tree_ind,\n",
    "                                     x_rl_query_ind,\n",
    "                                     y_rl_ind,\n",
    "                                     bgp_ind)\n",
    "    else:\n",
    "        #print(current_state)\n",
    "        chosen_action_available_idx = RL_Argmax_available(q_value[actual_state],available_actions)\n",
    "        chosen_action = available_actions[chosen_action_available_idx]\n",
    "        chosen_action_idx = chosen_action[0]\n",
    "        current_state.append(chosen_action)\n",
    "        reward, terminal = RLNeo_Reward(actions,\n",
    "                                     available_actions,\n",
    "                                     current_state,\n",
    "                                     chosen_action,\n",
    "                                     reward,\n",
    "                                     x_rl_tree_ind,\n",
    "                                     x_rl_query_ind,\n",
    "                                     y_rl_ind,\n",
    "                                     bgp_ind)\n",
    "    return current_state, reward, terminal, chosen_action, chosen_action_idx\n",
    "\n",
    "def RLNeo_Reward(actions,\n",
    "              available_actions,\n",
    "              current_state,\n",
    "              chosen_action,\n",
    "              reward,\n",
    "              x_rl_tree_ind,\n",
    "              x_rl_query_ind,\n",
    "              y_rl_ind,\n",
    "              bgp_ind):\n",
    "    terminal = False\n",
    "    reward = 0\n",
    "    #print(available_actions)\n",
    "    #if chosen_action not in available_actions:\n",
    "    #    reward -= 100000\n",
    "    if len(current_state) == len(actions):\n",
    "        terminal = True\n",
    "        new_dicto = RL_Rebuild_Dictionary(bgp_ind, current_state)\n",
    "        tree_format = TreeFormat(new_dicto,symbol)\n",
    "        #tree_format = ast.literal_eval(tree_format)\n",
    "        new_tree = np.array([str(tree_format).replace('\"', ';').replace(\"'\", '\"')])\n",
    "        x_rl_query = GetDataframe(new_tree, current_state, x_rl_query_ind.columns)\n",
    "        pred = getpredictions_info_nojc(new_tree, x_rl_query, y_rl_ind)['pred']\n",
    "        reward -= pred[0][0]\n",
    "    return reward, terminal\n",
    "    \n",
    "    \n",
    "def RLNeo_q_values(q_value_neo,current_state, reward):\n",
    "    for q_val, act in zip(q_value_neo,current_state):\n",
    "        if q_val[act[0]] == 0:\n",
    "            q_val[act[0]] = reward\n",
    "        else:\n",
    "            q_val[act[0]] = max(reward,q_val[act[0]])\n",
    "    return q_value_neo\n",
    "\n",
    "def RL_max_available_actions(q_value, old_states, actions):\n",
    "    available_actions = RL_available_actions(actions, old_states)\n",
    "    index_av = [i[0] for i in available_actions]\n",
    "    q_value_acotado = q_value[index_av[0]:index_av[-1]+1]\n",
    "    return np.max(q_value_acotado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"----------RL DATA----------\")\n",
    "print(f'shape ds_rl : {ds_rl.shape}')\n",
    "print(f'shape x_rl_tree : {x_rl_tree.shape}')\n",
    "print(f'shape x_rl_query: {x_rl_query.shape}')\n",
    "print(f'shape y_rl      : {y_rl.shape}')\n",
    "print(\"-----------------------\")\n",
    "bgps_rl = RL_bgp_format(ds_rl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTI-VAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seleccionar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_num_actions = -1\n",
    "buenos_idx = []\n",
    "aaa = []\n",
    "for i in range(0,x_rl_tree.shape[0]):\n",
    "    x_rl_tree_ind, x_rl_query_ind, y_rl_ind, bgp_ind = RL_get_data(x_rl_tree, x_rl_query, y_rl, bgps_rl,i)\n",
    "    actions = RL_Actions(bgp_ind)\n",
    "    if len(actions) >= min_num_actions:\n",
    "        aaa.append(y_rl_ind)\n",
    "        buenos_idx.append(i)\n",
    "total_buenos_idx = len(buenos_idx)\n",
    "print(\"total_idx\",total_buenos_idx)\n",
    "rl_columns = [\n",
    "            'index',\n",
    "            'iteration',\n",
    "            'pred_old',\n",
    "            'pred_new',\n",
    "            'real_old',\n",
    "            'real_new',\n",
    "            'mse_old',\n",
    "            'mse_new',\n",
    "            'mae_old',\n",
    "            'mae_new',\n",
    "            'rmse_old',\n",
    "            'rmse_new',\n",
    "            'old_tree',\n",
    "            'new_tree',\n",
    "            'history_old',\n",
    "            'history_new'\n",
    "             ]\n",
    " \n",
    "no_history = [\n",
    "            'index',\n",
    "            'iteration',\n",
    "            'pred_old',\n",
    "            'pred_new',\n",
    "            'real_old',\n",
    "            'real_new',\n",
    "            'mse_old',\n",
    "            'mse_new',\n",
    "            'mae_old',\n",
    "            'mae_new',\n",
    "            'rmse_old',\n",
    "            'rmse_new',\n",
    "            'old_tree',\n",
    "            'new_tree'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.7 ## EPSILON\n",
    "lr_rl = 0.9 ## LEARNING RATE\n",
    "disc_fac = 0.99 ## DISCOUNT FACTOR\n",
    "epoch_rl = 200 ### TOTAL EPOCH POR DATA\n",
    "n = 6 ##SALT Nombre archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q - Learning - multival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_ql = epsilon ## EPSILON\n",
    "lr_ql = lr_rl ## LEARNING RATE\n",
    "disc_fac_ql = disc_fac ## DISCOUNT FACTOR\n",
    "epoch_ql = epoch_rl ### TOTAL EPOCH POR DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_ql = []\n",
    "values_final_ql = []\n",
    "c_ql = 1\n",
    "print(f'Start: {str(datetime.datetime.now())}')\n",
    "for idx in buenos_idx:\n",
    "    x_rl_tree_ind, x_rl_query_ind, y_rl_ind, bgp_ind = RL_get_data(x_rl_tree, x_rl_query, y_rl, bgps_rl,idx)\n",
    "    actions = RL_Actions(bgp_ind)\n",
    "    num_actions = len(actions)\n",
    "    q_value_ql = RL_First_Policy(actions) ##INITIAL POLICITY\n",
    "    if c_ql % 10 == 0:\n",
    "        print(f'{str(datetime.datetime.now())} : {c_ql}/{total_buenos_idx}, index {idx} evaluating')\n",
    "    ####################################Q LEARNING###############################################\n",
    "    for i in range(0,epoch_ql):\n",
    "        current_state = []\n",
    "        reward = 0\n",
    "        terminal = False\n",
    "        current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,   \n",
    "                                                                                        current_state,\n",
    "                                                                                        reward,\n",
    "                                                                                        q_value_ql,\n",
    "                                                                                        x_rl_tree_ind,\n",
    "                                                                                        x_rl_query_ind,\n",
    "                                                                                        y_rl_ind,\n",
    "                                                                                        bgp_ind,\n",
    "                                                                                        epsilon_ql)\n",
    "        old_state = len(current_state)-1\n",
    "        prev_action = chosen_action_idx\n",
    "        while not terminal:\n",
    "            old_states_val = current_state.copy()\n",
    "            current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,\n",
    "                                                                                            current_state,\n",
    "                                                                                            reward,\n",
    "                                                                                            q_value_ql,\n",
    "                                                                                            x_rl_tree_ind,\n",
    "                                                                                            x_rl_query_ind,\n",
    "                                                                                            y_rl_ind,\n",
    "                                                                                            bgp_ind,\n",
    "                                                                                            epsilon_ql)\n",
    "            \n",
    "            new_state = len(current_state)-1\n",
    "            old_q_value = q_value_ql[old_state,prev_action]\n",
    "            q_max = RL_max_available_actions(q_value_ql[new_state,:], old_states_val, actions)\n",
    "            temporal_difference = reward + disc_fac_ql * q_max - old_q_value\n",
    "            #temporal_difference = reward + disc_fac_ql * np.max(q_value_ql[new_state,:]) - old_q_value\n",
    "            new_q_value = old_q_value + lr_ql * temporal_difference\n",
    "            q_value_ql[old_state,prev_action] = new_q_value\n",
    "            old_state = new_state\n",
    "            prev_action = chosen_action_idx\n",
    "        old_q_value = q_value_ql[old_state,prev_action]\n",
    "        new_q_value = old_q_value + lr_ql * (reward - old_q_value)\n",
    "        q_value_ql[old_state,prev_action] = new_q_value\n",
    "        ################################################################################################\n",
    "        \n",
    "        best_state, new_dicto, new_tree = RL_get_final_state_bgp_tree(q_value_ql,actions,bgp_ind,symbol)\n",
    "        rl_stats_old = getpredictions_info_nojc(x_rl_tree_ind, x_rl_query_ind, y_rl_ind)\n",
    "        rl_stats_new = getpredictions_info_nojc(new_tree, x_rl_query_ind, y_rl_ind)\n",
    "        \n",
    "        row = [\n",
    "                    idx,\n",
    "                    i,\n",
    "                    rl_stats_old['pred'][0][0],\n",
    "                    rl_stats_new['pred'][0][0],\n",
    "                    float(rl_stats_old['real'][0][0]),\n",
    "                    float(rl_stats_new['real'][0][0]), \n",
    "                    rl_stats_old['mse'],\n",
    "                    rl_stats_new['mse'],\n",
    "                    rl_stats_old['mae'],\n",
    "                    rl_stats_new['mae'],\n",
    "                    rl_stats_old['rmse'],\n",
    "                    rl_stats_new['rmse'],\n",
    "                    str(x_rl_tree_ind),\n",
    "                    str(new_tree),\n",
    "                    rl_stats_old['history'],\n",
    "                    rl_stats_new['history'],\n",
    "           ]\n",
    "\n",
    "        values_ql.append(row)\n",
    "    values_final_ql.append(row)\n",
    "    c_ql += 1\n",
    "print(\"len(values_ql[0])\", len(values_ql[0]))\n",
    "print(\"len(values_ql)\", len(values_ql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql_df = pd.DataFrame(values_ql,columns=rl_columns)\n",
    "ql_df_no_history = ql_df[no_history]\n",
    "ql_df_no_history.to_csv(URL + 'rl_csvs/ql_df_no_history'+str(n)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql_df_final = pd.DataFrame(values_final_ql,columns=rl_columns)\n",
    "ql_df_final.to_csv(URL + 'rl_csvs/ql_df_final'+str(n)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql_df_no_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql_df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL-Neo Multival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_neo = epsilon ## EPSILON\n",
    "epoch_neo = epoch_rl ### TOTAL EPOCH POR DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_neo = []\n",
    "values_final_neo = []\n",
    "c_neo = 1\n",
    "print(f'Start: {str(datetime.datetime.now())}')\n",
    "for idx in buenos_idx:\n",
    "    x_rl_tree_ind, x_rl_query_ind, y_rl_ind, bgp_ind = RL_get_data(x_rl_tree, x_rl_query, y_rl, bgps_rl,idx)\n",
    "    actions = RL_Actions(bgp_ind)\n",
    "    num_actions = len(actions)\n",
    "    q_value_neo = RL_First_Policy(actions) ##INITIAL POLICITY\n",
    "    if c_neo % 10 == 0:\n",
    "        print(f'{str(datetime.datetime.now())} : {c_neo}/{total_buenos_idx}, index {idx} evaluating')\n",
    "    ############# ALGORITMO NEO #####################3\n",
    "    for i in range(0,epoch_neo):\n",
    "        current_state = []\n",
    "        reward = 0\n",
    "        terminal = False\n",
    "        current_state, reward, terminal, chosen_action, chosen_action_idx = RLNeo_Next_step(actions,\n",
    "                                                                                        current_state,\n",
    "                                                                                        reward,\n",
    "                                                                                        q_value_neo,\n",
    "                                                                                        x_rl_tree_ind,\n",
    "                                                                                        x_rl_query_ind,\n",
    "                                                                                        y_rl_ind,\n",
    "                                                                                        bgp_ind,\n",
    "                                                                                        epsilon_neo)    \n",
    "        while not terminal:\n",
    "            current_state, reward, terminal, chosen_action, chosen_action_idx = RLNeo_Next_step(actions,\n",
    "                                                                                            current_state,\n",
    "                                                                                            reward,\n",
    "                                                                                            q_value_neo,\n",
    "                                                                                            x_rl_tree_ind,\n",
    "                                                                                            x_rl_query_ind,\n",
    "                                                                                            y_rl_ind,\n",
    "                                                                                            bgp_ind,\n",
    "                                                                                            epsilon_neo)\n",
    "        q_value_neo = RLNeo_q_values(q_value_neo,current_state, reward)\n",
    "        ##############################################################\n",
    "        best_state, new_dicto, new_tree = RL_get_final_state_bgp_tree(q_value_neo,actions,bgp_ind,symbol)\n",
    "        rl_stats_old = getpredictions_info_nojc(x_rl_tree_ind, x_rl_query_ind, y_rl_ind)\n",
    "        rl_stats_new = getpredictions_info_nojc(new_tree, x_rl_query_ind, y_rl_ind)\n",
    "        row = [\n",
    "                    idx,\n",
    "                    i,\n",
    "                    rl_stats_old['pred'][0][0],\n",
    "                    rl_stats_new['pred'][0][0],\n",
    "                    float(rl_stats_old['real'][0][0]),\n",
    "                    float(rl_stats_new['real'][0][0]), \n",
    "                    rl_stats_old['mse'],\n",
    "                    rl_stats_new['mse'],\n",
    "                    rl_stats_old['mae'],\n",
    "                    rl_stats_new['mae'],\n",
    "                    rl_stats_old['rmse'],\n",
    "                    rl_stats_new['rmse'],\n",
    "                    str(x_rl_tree_ind),\n",
    "                    str(new_tree),\n",
    "                    rl_stats_old['history'],\n",
    "                    rl_stats_new['history'],\n",
    "           ]\n",
    "        values_neo.append(row)\n",
    "    values_final_neo.append(row)\n",
    "    c_neo += 1\n",
    "#print(\"values_neo \", values_neo)\n",
    "print(\"len(values_neo[0])\",len(values_neo[0]) )\n",
    "print(\"len(values_neo)\", len(values_neo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo_df = pd.DataFrame(values_neo,columns=rl_columns)\n",
    "neo_df_no_history = neo_df[no_history]\n",
    "neo_df_no_history.to_csv(URL + 'rl_csvs/neo_df_no_history'+str(n)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo_df_final = pd.DataFrame(values_final_neo,columns=rl_columns)\n",
    "neo_df_final.to_csv(URL + 'rl_csvs/neo_df_final'+str(n)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "neo_df_no_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo_df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa Multival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_sar = epsilon ## EPSILON\n",
    "lr_sar = lr_rl ## LEARNING RATE\n",
    "disc_fac_sar = disc_fac ## DISCOUNT FACTOR\n",
    "epoch_sar = epoch_rl ### TOTAL EPOCH POR DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_sar = []\n",
    "values_final_sar = []\n",
    "c_sar = 1\n",
    "print(f'Start: {str(datetime.datetime.now())}')\n",
    "for idx in buenos_idx:\n",
    "    x_rl_tree_ind, x_rl_query_ind, y_rl_ind, bgp_ind = RL_get_data(x_rl_tree, x_rl_query, y_rl, bgps_rl,idx)\n",
    "    actions = RL_Actions(bgp_ind)\n",
    "    num_actions = len(actions)\n",
    "    q_value_sar = RL_First_Policy(actions) ##INITIAL POLICITY\n",
    "    if c_sar % 10 == 0:\n",
    "        print(f'{str(datetime.datetime.now())} : {c_sar}/{total_buenos_idx}, index {idx} evaluating')\n",
    "    ############################ALGORITMO SARSA##########################################\n",
    "    for i in range(0,epoch_sar):\n",
    "        current_state = []\n",
    "        reward = 0\n",
    "        terminal = False\n",
    "        current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,\n",
    "                                                                                        current_state,\n",
    "                                                                                        reward,\n",
    "                                                                                        q_value_sar,\n",
    "                                                                                        x_rl_tree_ind,\n",
    "                                                                                        x_rl_query_ind,\n",
    "                                                                                        y_rl_ind,\n",
    "                                                                                        bgp_ind,\n",
    "                                                                                        epsilon_sar)\n",
    "        old_state = len(current_state)-1\n",
    "        prev_action = chosen_action_idx\n",
    "        while not terminal:\n",
    "            current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,\n",
    "                                                                                            current_state,\n",
    "                                                                                            reward,\n",
    "                                                                                            q_value_sar,\n",
    "                                                                                            x_rl_tree_ind,\n",
    "                                                                                            x_rl_query_ind,\n",
    "                                                                                            y_rl_ind,\n",
    "                                                                                            bgp_ind,\n",
    "                                                                                            epsilon_sar)\n",
    "\n",
    "            old_q_value = q_value_sar[old_state,prev_action]\n",
    "            temporal_difference = reward + (disc_fac_sar * q_value_sar[new_state,chosen_action_idx]) - old_q_value\n",
    "            new_q_value = old_q_value + lr_sar * temporal_difference\n",
    "            q_value_sar[old_state,prev_action] = new_q_value\n",
    "            old_state = new_state\n",
    "            prev_action = chosen_action_idx\n",
    "        old_q_value = q_value_sar[old_state,prev_action]\n",
    "        new_q_value = old_q_value + lr_sar * (reward - old_q_value)\n",
    "        q_value_sar[old_state,prev_action] = new_q_value\n",
    "        ###############################################################################################33\n",
    "        best_state, new_dicto, new_tree = RL_get_final_state_bgp_tree(q_value_sar,actions,bgp_ind,symbol)\n",
    "        rl_stats_old = getpredictions_info_nojc(x_rl_tree_ind, x_rl_query_ind, y_rl_ind)\n",
    "        rl_stats_new = getpredictions_info_nojc(new_tree, x_rl_query_ind, y_rl_ind)\n",
    "        row = [\n",
    "                    idx,\n",
    "                    i,\n",
    "                    rl_stats_old['pred'][0][0],\n",
    "                    rl_stats_new['pred'][0][0],\n",
    "                    float(rl_stats_old['real'][0][0]),\n",
    "                    float(rl_stats_new['real'][0][0]), \n",
    "                    rl_stats_old['mse'],\n",
    "                    rl_stats_new['mse'],\n",
    "                    rl_stats_old['mae'],\n",
    "                    rl_stats_new['mae'],\n",
    "                    rl_stats_old['rmse'],\n",
    "                    rl_stats_new['rmse'],\n",
    "                    str(x_rl_tree_ind),\n",
    "                    str(new_tree),\n",
    "                    rl_stats_old['history'],\n",
    "                    rl_stats_new['history'],\n",
    "           ]\n",
    "\n",
    "        values_sar.append(row)\n",
    "    values_final_sar.append(row)\n",
    "    c_sar += 1\n",
    "#print(\"values_sar \", values_sar)\n",
    "print(\"len(values_sar[0])\",len(values_sar[0]) )\n",
    "print(\"len(values_sar)\", len(values_sar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_df = pd.DataFrame(values_sar,columns=rl_columns)\n",
    "sarsa_df_no_history = sarsa_df[no_history]\n",
    "sarsa_df_no_history.to_csv(URL + 'rl_csvs/sarsa_df_no_history'+str(n)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_df_final = pd.DataFrame(values_final_sar,columns=rl_columns)\n",
    "sarsa_df_final.to_csv(URL + 'rl_csvs/sarsa_df_final'+str(n)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sarsa_df_no_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected-Sarsa Multival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_exsar = epsilon ## EPSILON\n",
    "epsilon_exsar_alg = 0.9\n",
    "lr_exsar = lr_rl ## LEARNING RATE\n",
    "disc_fac_exsar = disc_fac ## DISCOUNT FACTOR\n",
    "epoch_exsar = epoch_rl ### TOTAL EPOCH POR DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_exsar = []\n",
    "values_final_exsar = []\n",
    "c_exsar = 1\n",
    "print(f'Start: {str(datetime.datetime.now())}')\n",
    "for idx in buenos_idx:\n",
    "    x_rl_tree_ind, x_rl_query_ind, y_rl_ind, bgp_ind = RL_get_data(x_rl_tree, x_rl_query, y_rl, bgps_rl,idx)\n",
    "    actions = RL_Actions(bgp_ind)\n",
    "    num_actions = len(actions)\n",
    "    q_value_exsar = RL_First_Policy(actions) ##INITIAL POLICITY\n",
    "    if c_exsar % 10 == 0:\n",
    "        print(f'{str(datetime.datetime.now())} : {c_exsar}/{total_buenos_idx}, index {idx} evaluating')\n",
    "        \n",
    "    ##### ALGORITMO EXPECTED SARSA\n",
    "    for i in range(0,epoch_exsar):\n",
    "        current_state = []\n",
    "        reward = 0\n",
    "        terminal = False\n",
    "        current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,\n",
    "                                                                                        current_state,\n",
    "                                                                                        reward,\n",
    "                                                                                        q_value_exsar,\n",
    "                                                                                        x_rl_tree_ind,\n",
    "                                                                                        x_rl_query_ind,\n",
    "                                                                                        y_rl_ind,\n",
    "                                                                                        bgp_ind,\n",
    "                                                                                        epsilon_exsar)    \n",
    "        old_state = len(current_state)-1\n",
    "        prev_action = chosen_action_idx\n",
    "        while not terminal:\n",
    "            old_states_val = current_state.copy()\n",
    "            current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,\n",
    "                                                                                            current_state,\n",
    "                                                                                            reward,\n",
    "                                                                                            q_value_exsar,\n",
    "                                                                                            x_rl_tree_ind,\n",
    "                                                                                            x_rl_query_ind,\n",
    "                                                                                            y_rl_ind,\n",
    "                                                                                            bgp_ind,\n",
    "                                                                                            epsilon_exsar)\n",
    "            new_state = len(current_state)-1\n",
    "            array_state = np.array(current_state)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            expected_q = 0\n",
    "            old_q_value = q_value_exsar[old_state,prev_action]\n",
    "            q_max = RL_max_available_actions(q_value_exsar[new_state,:], old_states_val, actions)\n",
    "            #q_max = np.max(q_value_exsar[new_state,:])\n",
    "            pi = np.ones(num_actions)*epsilon_exsar_alg/num_actions \\\n",
    "                + (q_value_exsar[new_state,:] == q_max)*(1-epsilon_exsar_alg)/np.sum(q_value_exsar[new_state,:] == q_max)\n",
    "    \n",
    "            expected_q = np.sum(q_value_exsar[new_state,:]*pi)\n",
    "            temporal_difference = reward + disc_fac_exsar * expected_q - old_q_value\n",
    "            new_q_value = old_q_value +  lr_exsar*temporal_difference\n",
    "            q_value_exsar[old_state,prev_action] = new_q_value\n",
    "            old_state = new_state\n",
    "            prev_action = chosen_action_idx\n",
    "        #########################################################\n",
    "        old_q_value = q_value_exsar[old_state,prev_action]\n",
    "        new_q_value = old_q_value + lr_exsar * (reward - old_q_value)\n",
    "        q_value_exsar[old_state,prev_action] = new_q_value\n",
    "\n",
    "        best_state, new_dicto, new_tree = RL_get_final_state_bgp_tree(q_value_exsar,actions,bgp_ind,symbol)\n",
    "        rl_stats_old = getpredictions_info_nojc(x_rl_tree_ind, x_rl_query_ind, y_rl_ind)\n",
    "        rl_stats_new = getpredictions_info_nojc(new_tree, x_rl_query_ind, y_rl_ind)\n",
    "        row = [\n",
    "                    idx,\n",
    "                    i,\n",
    "                    rl_stats_old['pred'][0][0],\n",
    "                    rl_stats_new['pred'][0][0],\n",
    "                    float(rl_stats_old['real'][0][0]),\n",
    "                    float(rl_stats_new['real'][0][0]), \n",
    "                    rl_stats_old['mse'],\n",
    "                    rl_stats_new['mse'],\n",
    "                    rl_stats_old['mae'],\n",
    "                    rl_stats_new['mae'],\n",
    "                    rl_stats_old['rmse'],\n",
    "                    rl_stats_new['rmse'],\n",
    "                    str(x_rl_tree_ind),\n",
    "                    str(new_tree),\n",
    "                    rl_stats_old['history'],\n",
    "                    rl_stats_new['history'],\n",
    "           ]\n",
    "\n",
    "        values_exsar.append(row)\n",
    "    values_final_exsar.append(row)\n",
    "    c_exsar += 1\n",
    "#print(\"values_exsar \", values_exsar)\n",
    "print(\"len(values_exsar[0])\",len(values_exsar[0]) )\n",
    "print(\"len(values_exsar)\", len(values_exsar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exsarsa_df = pd.DataFrame(values_exsar,columns=rl_columns)\n",
    "exsarsa_df_no_history = exsarsa_df[no_history]\n",
    "exsarsa_df_no_history.to_csv(URL + 'rl_csvs/exsarsa_df_no_history'+str(n)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exsarsa_df_final = pd.DataFrame(values_final_exsar,columns=rl_columns)\n",
    "exsarsa_df_final.to_csv(URL + 'rl_csvs/exsarsa_df_final'+str(n)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exsarsa_df_no_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exsarsa_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archivos enteros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"/home/school/math/final_analysis.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(URL + 'rl_csvs/ql_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(URL + 'rl_csvs/sarsa_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(URL + 'rl_csvs/neo_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(URL + 'rl_csvs/exsarsa_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql_df.to_csv(URL + 'rl_csvs/ql_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_df.to_csv(URL + 'rl_csvs/sarsa_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo_df.to_csv(URL + 'rl_csvs/neo_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exsarsa_df.to_csv(URL + 'rl_csvs/exsarsa_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
