{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBRERIAS y CONSTANTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import json, ast, sys, csv, random\n",
    "import plotly.express as px\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "#Implement training process\n",
    "from model_trees_algebra import NeoRegression\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functions.tree_format import IterateBuildTree, InnerJoinsIntraBGPS, \\\n",
    "                                IterateBuildTreeBetweenBGPS, TreeFormat\n",
    "from functions.RL_functions import RL_Actions, RL_Initial_Step, RL_available_actions, \\\n",
    "                                    RL_Next_step, RL_Reward, RL_Rebuild_Dictionary\n",
    "\n",
    "from functions.aux import MetricTotalAccuraccy\n",
    "\n",
    "\n",
    "class BaoTrainingException(Exception):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"/media/data/ccarmona/memoria/dataset/\"\n",
    "csv_name = 'new_dataset_6.4_subqueries'\n",
    "x = [True,False]\n",
    "active_new_data = x[0]\n",
    "symbol = \"ᶲ\"\n",
    "#learning_rate = 0.00001\n",
    "\n",
    "#Este parametro sirve para elegir cierta cantidad de data ordenado por rangos de tiempo obtenidos.\n",
    "## Entre más bajo menos data se seleccionara. Si es muy alto se tendran demasiados valores outliners, \n",
    "## pero si es muy bajo podría tenerse una data no representativa y se aumenta el riesgo de overfitting.\n",
    "## Por otro lado min_data, simplemente da el valor minimo de tiempo de ejecución que tiene una consulta tomada\n",
    "## en cuenta para hacer el modelo\n",
    "#percent_of_data_or = 0.93\n",
    "min_time_or = 50\n",
    "max_time_or = 150\n",
    "#percent_of_data = 1\n",
    "min_time = 2\n",
    "max_time = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ds(all_data, val_rate, seed):\n",
    "    \"\"\"\n",
    "    Used  to keep a balance of sets with respect to runtime of queries. \n",
    "    test_rate is a rate of the total,\n",
    "    val_rate is a rate of the (total - test_rate)\n",
    "    :param all_data: Pandas dataframe with data\n",
    "    :param val_rate: Rate of the (total - test_rate)\n",
    "    :param seed: For replication of results, this fixes the seed of split method. \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    ranges = {}\n",
    "    ranges['1_2'] = all_data[(all_data[\"time\"] >= min_time)    & (all_data[\"time\"] <= 2)]\n",
    "    ranges['2_3'] = all_data[(all_data[\"time\"] > 2)    & (all_data[\"time\"] <= 3)]\n",
    "    ranges['3_4'] = all_data[(all_data[\"time\"] > 3)    & (all_data[\"time\"] <= 4)]\n",
    "    ranges['4_5'] = all_data[(all_data[\"time\"] > 4)    & (all_data[\"time\"] <= 5)]\n",
    "    ranges['5_8'] = all_data[(all_data[\"time\"] > 5)    & (all_data[\"time\"] <= 8)]\n",
    "    ranges['8_10'] = all_data[(all_data[\"time\"] > 8)   & (all_data[\"time\"] <= 10)]\n",
    "    ranges['10_20'] =   all_data[(all_data[\"time\"] > 10) & (all_data[\"time\"] <= 20)]\n",
    "    ranges['20_30'] =   all_data[(all_data[\"time\"] > 20) & (all_data[\"time\"] <= 30)]\n",
    "    ranges['30_40'] =   all_data[(all_data[\"time\"] > 30) & (all_data[\"time\"] <= 40)]\n",
    "    ranges['40_50'] =   all_data[(all_data[\"time\"] > 40) & (all_data[\"time\"] <= 50)]\n",
    "    ranges['50_60'] =   all_data[(all_data[\"time\"] > 50) & (all_data[\"time\"] <= 60)]\n",
    "    ranges['60_80'] =   all_data[(all_data[\"time\"] > 60) & (all_data[\"time\"] <= 80)]\n",
    "    ranges['80_100'] =  all_data[(all_data[\"time\"] > 80) & (all_data[\"time\"] <= 100)]\n",
    "    ranges['100_150'] = all_data[(all_data[\"time\"] > 100) & (all_data[\"time\"] <= 150)]\n",
    "    ranges['150_200'] = all_data[(all_data[\"time\"] > 150) & (all_data[\"time\"] <= 200)]\n",
    "    ranges['200_250'] = all_data[(all_data[\"time\"] > 200) & (all_data[\"time\"] <= 250)]\n",
    "    ranges['250_450'] = all_data[(all_data[\"time\"] > 250) & (all_data[\"time\"] <= 450)]\n",
    "    ranges['450_last'] = all_data[(all_data[\"time\"] > 450)]\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    for rang in ranges.values():\n",
    "        if rang.shape[0] >= 3:\n",
    "            X_train, X_val = train_test_split(\n",
    "                rang, test_size=val_rate, shuffle=True,random_state=seed)\n",
    "\n",
    "            train_data.append(X_train)\n",
    "            val_data.append(X_val)\n",
    "    train_data_list = pd.concat(train_data)\n",
    "    val_data_list = pd.concat(val_data)\n",
    "    #print(\"Shapes : Train: {} Val: {}\".format(train_data_list.shape, val_data_list.shape))\n",
    "    return train_data_list, val_data_list\n",
    "def clear_error_tuples(x):\n",
    "    try:\n",
    "        json.loads(x)\n",
    "        return True\n",
    "    except:\n",
    "        print(\"Error in data ignored!\", x)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to use.\n",
    "#list_columns = ['limit', 'group_by',\n",
    "#       'distinct', 'order_by', 'union', 'left_join', 'join', 'iter', 'filter',\n",
    "#       'num_filter', 'filter_eq', 'filter_gt', 'filter_ge', 'filter_lt',\n",
    "#       'filter_le', 'filter_neq', 'filter_iri', 'filter_neq.1', 'filter_bound',\n",
    "#       'filter_contains', 'filter_exists', 'filter_isBlank', 'filter_isIRI',\n",
    "#       'filter_isLiteral', 'filter_lang', 'filter_langMatches', 'filter_not',\n",
    "#       'filter_notexists', 'filter_regex', 'filter_sameTerm', 'filter_str',\n",
    "#       'filter_strstarts', 'filter_or', 'filter_and', 'json_cardinality']\n",
    "\n",
    "list_columns = ['total_bgps', 'triples', 'treesize', 'join', 'left_join']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEAN DATA AND CREATE NEW DATASET TRAIN-TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(URL + csv_name + \".csv\", engine='python', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_raw[df_raw['time'] <= max_time_or]\n",
    "df_raw = df_raw[df_raw['time'] >= min_time_or]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_raw.shape (3285, 65)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49.899, 60.0]    0.209741\n",
       "(60.0, 70.0]      0.129680\n",
       "(70.0, 80.0]      0.119635\n",
       "(110.0, 120.0]    0.113546\n",
       "(80.0, 90.0]      0.095282\n",
       "(100.0, 110.0]    0.091629\n",
       "(90.0, 100.0]     0.071233\n",
       "(130.0, 140.0]    0.069406\n",
       "(120.0, 130.0]    0.050533\n",
       "(140.0, 150.0]    0.049315\n",
       "Name: time, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"df_raw.shape\", df_raw.shape)\n",
    "df_raw['time'].value_counts(bins=10, sort=True, normalize=True)\n",
    "#for i in df_raw['time'].value_counts(bins=100, sort=True, normalize=True):\n",
    "#    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3285.000000\n",
       "mean       89.596651\n",
       "std        28.834562\n",
       "min        50.000000\n",
       "25%        64.000000\n",
       "50%        85.000000\n",
       "75%       113.000000\n",
       "max       150.000000\n",
       "Name: time, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw['time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df_raw.reset_index(drop=True)\n",
    "f = open(\"copy_queries.txt\", \"a\")\n",
    "for i in range(len(df_raw)):\n",
    "    f.write(df_raw['query'][i])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unique_id', 'filename', 'query', 'profile', 'limit', 'group_by',\n",
       "       'distinct', 'order_by', 'union', 'left_join', 'join', 'iter', 'filter',\n",
       "       'num_filter', 'filter_eq', 'filter_gt', 'filter_ge', 'filter_lt',\n",
       "       'filter_le', 'filter_neq', 'filter_iri', 'filter_neq.1', 'filter_bound',\n",
       "       'filter_contains', 'filter_exists', 'filter_isBlank', 'filter_isIRI',\n",
       "       'filter_isLiteral', 'filter_lang', 'filter_langMatches', 'filter_not',\n",
       "       'filter_notexists', 'filter_regex', 'filter_sameTerm', 'filter_str',\n",
       "       'filter_strstarts', 'filter_or', 'filter_and', 'time', 'cpu_p', 'rnd',\n",
       "       'seq', 'same_seg_p', 'same_page_p', 'disk_reads', 'read_ahead', 'wait',\n",
       "       'comp_msec', 'comp_reads', 'comp_read_p', 'comp_messages', 'comp_clw',\n",
       "       'triples', 'total_bgps', 'treesize', 'matrix_format', 'trees',\n",
       "       'json_time_predicate', 'json_fanout_predicate',\n",
       "       'json_input_rows_predicate', 'json_cardinality_fanout',\n",
       "       'json_cardinality', 'scan_queries', 'bgps', 'matrix_subtrees'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_model, ds_rl_prev = split_ds(df_raw, 0.15,seed=None)\n",
    "ds_rl_prev.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtree_format(df_raw):\n",
    "    \n",
    "    df_raw_unique_id = df_raw['unique_id']\n",
    "    df_raw_filename = df_raw['filename']\n",
    "    df_raw_query = df_raw['query']\n",
    "    df_raw_json_cardinality = df_raw['json_cardinality']\n",
    "    df_raw_subtrees = df_raw['matrix_subtrees']\n",
    "    \n",
    "    \n",
    "    columns = ['unique_id', 'filename', 'query', 'trees', 'time', 'total_bgps', 'triples', 'treesize', 'join', 'left_join', 'iter', 'json_cardinality_original_query']\n",
    "    values = []\n",
    "    for dfrs in range(0,len(df_raw_subtrees)):\n",
    "        unique_id = df_raw_unique_id[dfrs]\n",
    "        filename = df_raw_filename[dfrs]\n",
    "        query = df_raw_query[dfrs]\n",
    "        json_cardinality = df_raw_json_cardinality[dfrs]\n",
    "        lists_type = ast.literal_eval(df_raw_subtrees[dfrs])\n",
    "        for ls in lists_type:\n",
    "            str_subtree = str(ls[0]).replace('\"', ';').replace(\"'\", '\"')\n",
    "            row = [unique_id, filename, query, str_subtree] + ls[1:] + [json_cardinality]\n",
    "            values.append(row)\n",
    "            \n",
    "    df_subtrees = pd.DataFrame(values, columns=columns)\n",
    "    \n",
    "    \n",
    "    return df_subtrees\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_cardinality_subtree(df_subtrees):\n",
    "    dfsq_unique_id = df_subtrees['unique_id']\n",
    "    dfsq_filename = df_subtrees['filename']\n",
    "    dfsq_query = df_subtrees['query']\n",
    "    dfsq_trees = df_subtrees['trees']\n",
    "    dfsq_time = df_subtrees['time']\n",
    "    dfsq_total_bgps = df_subtrees['total_bgps']\n",
    "    dfsq_triples = df_subtrees['triples']\n",
    "    dfsq_treesize = df_subtrees['treesize']\n",
    "    dfsq_join = df_subtrees['join']\n",
    "    dfsq_left_join = df_subtrees['left_join']\n",
    "    dfsq_iter = df_subtrees['iter']\n",
    "    dfsq_json_cardinality_original = df_subtrees['json_cardinality_original_query']\n",
    "    columns = ['unique_id', 'filename', 'query', 'trees', 'time', 'total_bgps', 'triples', 'treesize', 'join', 'left_join', 'iter', 'json_cardinality_original_query', 'json_cardinality']\n",
    "    values = []\n",
    "    for df in range(len(dfsq_trees)):\n",
    "        json_cardinality = {}\n",
    "        tree_as_str = str(dfsq_trees[df])\n",
    "        json_cardinality_original = ast.literal_eval(dfsq_json_cardinality_original[df])\n",
    "        for k,v in json_cardinality_original.items():\n",
    "            #fix = k.replace(';','\"')\n",
    "            if k in tree_as_str:\n",
    "                json_cardinality[str(k)] = str(v)\n",
    "        values.append(str(json_cardinality).replace('\"', ';').replace(\"'\", '\"'))\n",
    "    \n",
    "    df_subtrees['json_cardinality'] = values\n",
    "    \n",
    "    return df_subtrees\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_model = ds_model.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtrees = subtree_format(ds_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_subtrees = json_cardinality_subtree(df_subtrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8255, 13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subtrees.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8255.000000\n",
       "mean       26.725020\n",
       "std        24.218160\n",
       "min         0.002262\n",
       "25%         0.840554\n",
       "50%        27.000000\n",
       "75%        43.213824\n",
       "max        99.816736\n",
       "Name: time, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subtrees['time'] = df_subtrees.time.astype(float)\n",
    "df_subtrees['time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 26.725019558740154\n",
      "std 24.218160104413318\n",
      "df_raw.shape (8255, 13)\n",
      "max 99.816736\n",
      "FIRST CLEAN\n",
      "mean 37.491209490462886\n",
      "std 19.78068593207435\n",
      "df_raw.shape (5725, 13)\n",
      "max 80.0\n"
     ]
    }
   ],
   "source": [
    "print(\"mean\", df_subtrees['time'].mean())\n",
    "print(\"std\", df_subtrees['time'].std())\n",
    "print('df_raw.shape',df_subtrees.shape)\n",
    "print(\"max\", df_subtrees['time'].max())\n",
    "#bins = []\n",
    "#btw = 100\n",
    "#for i in range(int(df_subtrees['time'].max())):\n",
    "#    if i % btw == 0:\n",
    "#        bins.append(i)\n",
    "#bins.append(bins[-1] + btw)\n",
    "#c = 0\n",
    "#idx = 0\n",
    "#for i in list(df_subtrees['time'].value_counts(bins=bins, sort=False, normalize=True)):\n",
    "#    if c > percent_of_data:\n",
    "#        break\n",
    "#    c += i\n",
    "#    idx += 1\n",
    "#df_subtrees = df_subtrees[df_subtrees['time'] <=(bins[idx])]\n",
    "df_subtrees = df_subtrees[df_subtrees['time'] <= max_time]\n",
    "df_subtrees = df_subtrees[df_subtrees['time'] >= min_time]\n",
    "df_subtrees = df_subtrees.reset_index(drop=True)\n",
    "\n",
    "print(\"FIRST CLEAN\")\n",
    "print(\"mean\", df_subtrees['time'].mean())\n",
    "print(\"std\", df_subtrees['time'].std())\n",
    "print('df_raw.shape',df_subtrees.shape)\n",
    "print(\"max\", df_subtrees['time'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5725.000000\n",
       "mean       37.491209\n",
       "std        19.780686\n",
       "min         2.004519\n",
       "25%        26.000000\n",
       "50%        36.000000\n",
       "75%        51.000000\n",
       "max        80.000000\n",
       "Name: time, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subtrees['time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------SHAPES-----------\n",
      "----------RAW-----------\n",
      "shape df_raw: (4192, 65)\n",
      "----------PREV----------\n",
      "shape ds_train_val_prev: (4576, 13)\n",
      "shape ds_train_prev: (3429, 13)\n",
      "shape ds_val_prev: (1147, 13)\n",
      "shape ds_test_prev: (1149, 13)\n",
      "shape ds_rl_prev: (631, 65)\n",
      "----------CLEAN----------\n",
      "shape ds_train: (3429, 13)\n",
      "shape ds_val: (1147, 13)\n",
      "shape ds_test: (1149, 13)\n",
      "shape ds_rl: (631, 65)\n"
     ]
    }
   ],
   "source": [
    "ds_train_val_prev, ds_test_prev = split_ds(df_subtrees, 0.2,seed=None)\n",
    "ds_train_prev, ds_val_prev = split_ds(ds_train_val_prev, 0.25,seed=None)\n",
    "\n",
    "#ds_train_val_prev, ds_test_prev = split_ds(df_raw, 0.2,seed=None)\n",
    "#ds_train_prev, ds_val_prev = split_ds(ds_train_val_prev, 0.25,seed=None)\n",
    "#ds_rl_prev = ds_test_prev.copy()\n",
    "\n",
    "#Remove bad rows\n",
    "ds_train  = ds_train_prev[ds_train_prev['trees'].apply(lambda x: clear_error_tuples(x))]\n",
    "ds_val  = ds_val_prev[ds_val_prev['trees'].apply(lambda x: clear_error_tuples(x))]\n",
    "ds_test  = ds_test_prev[ds_test_prev['trees'].apply(lambda x: clear_error_tuples(x))]\n",
    "ds_rl = ds_rl_prev[ds_rl_prev['trees'].apply(lambda x: clear_error_tuples(x))]\n",
    "\n",
    "print(\"---------SHAPES-----------\")\n",
    "print(\"----------RAW-----------\")\n",
    "print(f'shape df_raw: {df_raw.shape}')\n",
    "print(\"----------PREV----------\")\n",
    "print(f'shape ds_train_val_prev: {ds_train_val_prev.shape}')\n",
    "print(f'shape ds_train_prev: {ds_train_prev.shape}')\n",
    "print(f'shape ds_val_prev: {ds_val_prev.shape}')\n",
    "print(f'shape ds_test_prev: {ds_test_prev.shape}')\n",
    "print(f'shape ds_rl_prev: {ds_rl_prev.shape}')\n",
    "print(\"----------CLEAN----------\")\n",
    "print(f'shape ds_train: {ds_train.shape}')\n",
    "print(f'shape ds_val: {ds_val.shape}')\n",
    "print(f'shape ds_test: {ds_test.shape}')\n",
    "print(f'shape ds_rl: {ds_rl.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if active_new_data:\n",
    "#    ds_train.to_csv(URL + csv_name + '_ds_train.csv')\n",
    "#    ds_val.to_csv(URL + csv_name + '_ds_val.csv')\n",
    "#    ds_test.to_csv(URL + csv_name + '_ds_test.csv')\n",
    "#    ds_rl.to_csv(URL + csv_name + '_ds_rl.csv')\n",
    "#    print(\"New csv generates\")\n",
    "#else:\n",
    "#    print(\"Not csv generates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_0_1 = []\n",
    "list_2_10 = []\n",
    "list_10_20 = []\n",
    "list_20_30 = []\n",
    "list_30_40 = []\n",
    "list_40_50 = []\n",
    "list_50_60 = []\n",
    "list_60_70 = []\n",
    "list_70_80 = []\n",
    "list_80_90 = []\n",
    "list_90_100 = []\n",
    "list_100_110 = []\n",
    "list_110_120 = []\n",
    "list_120 = []\n",
    "\n",
    "for i in df_raw['time']:\n",
    "    x = int(i)\n",
    "    if x >= 0 and x < 2:\n",
    "        list_0_1.append(x)\n",
    "    if x >= 2 and x < 10:\n",
    "        list_2_10.append(x)\n",
    "    if x >= 10 and x < 20:\n",
    "        list_10_20.append(x)\n",
    "    if x >= 20 and x < 30:\n",
    "        list_20_30.append(x)\n",
    "    if x >= 30 and x < 40:\n",
    "        list_30_40.append(x)\n",
    "    if x >= 40 and x < 50:\n",
    "        list_40_50.append(x)\n",
    "    if x >= 50 and x < 60:\n",
    "        list_50_60.append(x)\n",
    "    if x >= 60 and x < 70:\n",
    "        list_60_70.append(x)\n",
    "    if x >= 70 and x < 80:\n",
    "        list_70_80.append(x)\n",
    "    if x >= 80 and x < 90:\n",
    "        list_80_90.append(x)\n",
    "    if x >= 90 and x < 100:\n",
    "        list_90_100.append(x)\n",
    "    if x >= 100 and x < 110:\n",
    "        list_100_110.append(x)\n",
    "    if x >= 110 and x < 120:\n",
    "        list_110_120.append(x)\n",
    "    if x > 120:\n",
    "        list_120.append(x)\n",
    "total = {\n",
    "    \"list_0_1\" : list_0_1,\n",
    "    \"list_2_10\" : list_2_10,\n",
    "    \"list_10_20\" : list_10_20,\n",
    "    \"list_20_30\" : list_20_30,\n",
    "    \"list_30_40\" : list_30_40,\n",
    "    \"list_40_50\" : list_40_50,\n",
    "    \"list_50_60\" : list_50_60,\n",
    "    \"list_60_70\" : list_60_70,\n",
    "    \"list_70_80\" : list_70_80,\n",
    "    \"list_80_90\" : list_80_90,\n",
    "    \"list_90_100\" : list_90_100,\n",
    "    \"list_100_110\" : list_100_110,\n",
    "    \"list_110_120\" : list_110_120,\n",
    "    \"list_120\" : list_120,\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
