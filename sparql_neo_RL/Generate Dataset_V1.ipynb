{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBRERIAS y CONSTANTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS CUDA AVAILABLE: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import json, ast, sys, csv, random\n",
    "import plotly.express as px\n",
    "import math\n",
    "import datetime\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#Implement training process\n",
    "from model_trees_algebra import NeoRegression\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functions.tree_format import IterateBuildTree, InnerJoinsIntraBGPS, \\\n",
    "                                IterateBuildTreeBetweenBGPS, TreeFormat, TreeFormat_all\n",
    "from functions.RL_functions import GetTriplesSubtree, \\\n",
    "                                        GetTreeSize, \\\n",
    "                                        GetAllJoins, \\\n",
    "                                        GetIter, \\\n",
    "                                        GetTotalBgp, \\\n",
    "                                        GetDataframe\n",
    "\n",
    "from functions.aux import MetricTotalAccuraccy\n",
    "\n",
    "\n",
    "class BaoTrainingException(Exception):\n",
    "    pass\n",
    "csv.field_size_limit(sys.maxsize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"/media/data/ccarmona/memoria/dataset/\"\n",
    "csv_name = 'new_dataset_7.2_subqueries'\n",
    "x = [True,False]\n",
    "active_new_data = x[0]\n",
    "symbol = \"ᶲ\"\n",
    "optimizer = \"Adam\"\n",
    "#Este parametro sirve para elegir cierta cantidad de data ordenado por rangos de tiempo obtenidos.\n",
    "## Entre más bajo menos data se seleccionara. Si es muy alto se tendran demasiados valores outliners, \n",
    "## pero si es muy bajo podría tenerse una data no representativa y se aumenta el riesgo de overfitting.\n",
    "## Por otro lado min_data, simplemente da el valor minimo de tiempo de ejecución que tiene una consulta tomada\n",
    "## en cuenta para hacer el modelo\n",
    "#percent_of_data_or = 0.93\n",
    "#min_time_or = 15\n",
    "#max_time_or = 80\n",
    "#percent_of_data = 1\n",
    "#min_time = 5\n",
    "#max_time = 80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ds(all_data, val_rate, seed):\n",
    "    \"\"\"\n",
    "    Used  to keep a balance of sets with respect to runtime of queries. \n",
    "    test_rate is a rate of the total,\n",
    "    val_rate is a rate of the (total - test_rate)\n",
    "    :param all_data: Pandas dataframe with data\n",
    "    :param val_rate: Rate of the (total - test_rate)\n",
    "    :param seed: For replication of results, this fixes the seed of split method. \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    ranges = {}\n",
    "    ranges['1_2'] = all_data[(all_data[\"time\"] > 0)    & (all_data[\"time\"] <= 2)]\n",
    "    ranges['2_3'] = all_data[(all_data[\"time\"] > 2)    & (all_data[\"time\"] <= 3)]\n",
    "    ranges['3_4'] = all_data[(all_data[\"time\"] > 3)    & (all_data[\"time\"] <= 4)]\n",
    "    ranges['4_5'] = all_data[(all_data[\"time\"] > 4)    & (all_data[\"time\"] <= 5)]\n",
    "    ranges['5_8'] = all_data[(all_data[\"time\"] > 5)    & (all_data[\"time\"] <= 8)]\n",
    "    ranges['8_10'] = all_data[(all_data[\"time\"] > 8)   & (all_data[\"time\"] <= 10)]\n",
    "    ranges['10_20'] =   all_data[(all_data[\"time\"] > 10) & (all_data[\"time\"] <= 20)]\n",
    "    ranges['20_30'] =   all_data[(all_data[\"time\"] > 20) & (all_data[\"time\"] <= 30)]\n",
    "    ranges['30_40'] =   all_data[(all_data[\"time\"] > 30) & (all_data[\"time\"] <= 40)]\n",
    "    ranges['40_50'] =   all_data[(all_data[\"time\"] > 40) & (all_data[\"time\"] <= 50)]\n",
    "    ranges['50_60'] =   all_data[(all_data[\"time\"] > 50) & (all_data[\"time\"] <= 60)]\n",
    "    ranges['60_80'] =   all_data[(all_data[\"time\"] > 60) & (all_data[\"time\"] <= 80)]\n",
    "    ranges['80_100'] =  all_data[(all_data[\"time\"] > 80) & (all_data[\"time\"] <= 100)]\n",
    "    ranges['100_150'] = all_data[(all_data[\"time\"] > 100) & (all_data[\"time\"] <= 150)]\n",
    "    ranges['150_200'] = all_data[(all_data[\"time\"] > 150) & (all_data[\"time\"] <= 200)]\n",
    "    ranges['200_250'] = all_data[(all_data[\"time\"] > 200) & (all_data[\"time\"] <= 250)]\n",
    "    ranges['250_450'] = all_data[(all_data[\"time\"] > 250) & (all_data[\"time\"] <= 450)]\n",
    "    ranges['450_last'] = all_data[(all_data[\"time\"] > 450)]\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    for rang in ranges.values():\n",
    "        if rang.shape[0] >= 3:\n",
    "            X_train, X_val = train_test_split(\n",
    "                rang, test_size=val_rate, shuffle=True,random_state=seed)\n",
    "\n",
    "            train_data.append(X_train)\n",
    "            val_data.append(X_val)\n",
    "    train_data_list = pd.concat(train_data)\n",
    "    val_data_list = pd.concat(val_data)\n",
    "    #print(\"Shapes : Train: {} Val: {}\".format(train_data_list.shape, val_data_list.shape))\n",
    "    return train_data_list, val_data_list\n",
    "def clear_error_tuples(x):\n",
    "    try:\n",
    "        json.loads(x)\n",
    "        return True\n",
    "    except:\n",
    "        print(\"Error in data ignored!\", x)\n",
    "        return False\n",
    "    \n",
    "def subtree_format(df_raw):\n",
    "    df_raw_unique_id = df_raw['unique_id']\n",
    "    df_raw_filename = df_raw['filename']\n",
    "    df_raw_query = df_raw['query']\n",
    "    df_raw_bgp = df_raw['bgps']\n",
    "    df_raw_json_cardinality = df_raw['json_cardinality']\n",
    "    df_raw_subtrees = df_raw['matrix_subtrees']\n",
    "    columns = ['unique_id', 'filename', 'query', 'trees',  'bgps' ,'time', 'total_bgps', 'triples', 'treesize', 'join', 'left_join', 'iter', 'json_cardinality_original_query']\n",
    "    values = []\n",
    "    for dfrs in range(0,len(df_raw_subtrees)):\n",
    "        unique_id = df_raw_unique_id[dfrs]\n",
    "        filename = df_raw_filename[dfrs]\n",
    "        query = df_raw_query[dfrs]\n",
    "        bgp = df_raw_bgp[dfrs]\n",
    "        json_cardinality = df_raw_json_cardinality[dfrs]\n",
    "        lists_type = ast.literal_eval(df_raw_subtrees[dfrs])\n",
    "        for ls in lists_type:\n",
    "            str_subtree = str(ls[0]).replace('\"', ';').replace(\"'\", '\"')\n",
    "            row = [unique_id, filename, query, str_subtree, bgp] + ls[1:] + [json_cardinality]\n",
    "            values.append(row)\n",
    "            \n",
    "    df_subtrees = pd.DataFrame(values, columns=columns)\n",
    "    return df_subtrees\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to use.\n",
    "list_columns = ['total_bgps', 'triples', 'treesize', 'join', 'left_join']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDENTIFICACION DE DATASET, CLEAN DATA Y CREAR NEW DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(URL + csv_name + \".csv\", engine='python', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unique_id', 'filename', 'query', 'profile', 'limit', 'group_by',\n",
       "       'distinct', 'order_by', 'union', 'left_join', 'join', 'iter', 'filter',\n",
       "       'num_filter', 'filter_eq', 'filter_gt', 'filter_ge', 'filter_lt',\n",
       "       'filter_le', 'filter_neq', 'filter_iri', 'filter_neq.1', 'filter_bound',\n",
       "       'filter_contains', 'filter_exists', 'filter_isBlank', 'filter_isIRI',\n",
       "       'filter_isLiteral', 'filter_lang', 'filter_langMatches', 'filter_not',\n",
       "       'filter_notexists', 'filter_regex', 'filter_sameTerm', 'filter_str',\n",
       "       'filter_strstarts', 'filter_or', 'filter_and', 'time', 'cpu_p', 'rnd',\n",
       "       'seq', 'same_seg_p', 'same_page_p', 'disk_reads', 'read_ahead', 'wait',\n",
       "       'comp_msec', 'comp_reads', 'comp_read_p', 'comp_messages', 'comp_clw',\n",
       "       'triples', 'total_bgps', 'treesize', 'matrix_format', 'trees',\n",
       "       'trees_old_format', 'json_time_predicate', 'json_fanout_predicate',\n",
       "       'json_input_rows_predicate', 'json_cardinality_fanout',\n",
       "       'json_cardinality', 'scan_queries', 'bgps', 'matrix_subtrees',\n",
       "       'matrix_subtrees_full'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_raw = df_raw.drop(['trees'],axis=1)\n",
    "#df_raw = df_raw.drop(['matrix_subtrees'],axis=1)\n",
    "#df_raw.rename(columns={'trees_old_format': 'trees'}, inplace=True)\n",
    "#df_raw.rename(columns={'matrix_subtrees_full': 'matrix_subtrees'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"df_raw.shape\", df_raw.shape)\n",
    "df_raw['time'].value_counts(bins=100, sort=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df_raw['time'].describe())\n",
    "df_raw = df_raw[df_raw['time'] >= 1]\n",
    "df_raw = df_raw[df_raw['time'] <= 500]\n",
    "print(df_raw['time'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "df_time_analysis = df_raw['time'].copy()\n",
    "df_time_analysis = df_time_analysis.to_numpy()\n",
    "\n",
    "f,ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "ax.hist(df_time_analysis, bins=500)\n",
    "ax.set_title(\"Histograma de latencia de cada consulta Log Scale\")\n",
    "ax.set_xlabel(\"Latencia de las consultas\")\n",
    "ax.set_ylabel(\"Cantidad de Consultas\")\n",
    "ax.set_yscale('log')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean\", df_raw['time'].mean())\n",
    "print(\"std\", df_raw['time'].std())\n",
    "print('df_raw.shape',df_raw.shape)\n",
    "print(\"max\", df_raw['time'].max())\n",
    "print(\"df_raw.shape\", df_raw.shape)\n",
    "df_raw = df_raw[df_raw['time'] >= 15]\n",
    "df_raw = df_raw[df_raw['time'] <= 80]\n",
    "df_raw = df_raw.reset_index(drop=True)\n",
    "print(\"CLEAN by TIME\")\n",
    "print(\"mean\", df_raw['time'].mean())\n",
    "print(\"std\", df_raw['time'].std())\n",
    "print(\"max\", df_raw['time'].max())\n",
    "print(\"df_raw.shape\", df_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "df_time_analysis2 = df_raw['time'].copy()\n",
    "df_time_analysis2 = df_time_analysis2.to_numpy()\n",
    "\n",
    "f,ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "ax.hist(df_time_analysis2, bins=100)\n",
    "ax.set_title(\"Histograma de latencia de cada consulta Log Scale\")\n",
    "ax.set_xlabel(\"Latencia de las consultas\")\n",
    "ax.set_ylabel(\"Cantidad de Consultas\")\n",
    "#ax.set_yscale('log')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sparql_file' in df_raw.columns:\n",
    "    df_raw = df_raw.rename(columns = {'sparql_file': 'query'}, inplace = False)\n",
    "df_raw['cpu_p'] = df_raw['cpu_p'].apply(lambda x: float(x.strip('%')))\n",
    "df_raw['same_seg_p'] = df_raw['same_seg_p'].apply(lambda x: float(x.strip('%')))\n",
    "df_raw['same_page_p'] = df_raw['same_page_p'].apply(lambda x: float(x.strip('%')))\n",
    "df_raw['wait'] = df_raw['wait'].apply(lambda x: float(x.strip('%')))\n",
    "df_raw['comp_read_p'] = df_raw['comp_read_p'].apply(lambda x: float(x.strip('%')))\n",
    "df_raw['comp_clw'] = df_raw['comp_clw'].apply(lambda x: float(x.strip('%')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_raw['time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_model, ds_rl_prev = split_ds(df_raw, 0.20,seed=None)\n",
    "ds_model = ds_model.reset_index(drop=True)\n",
    "ds_rl_prev = ds_rl_prev.reset_index(drop=True)\n",
    "print(\"ds_model.shape\",ds_model.shape)\n",
    "print(\"ds_rl_prev.shape\",ds_rl_prev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_subtrees = subtree_format(ds_model)\n",
    "df_subtrees['time'] = df_subtrees.time.astype(float)\n",
    "print(ds_model.shape)\n",
    "print(df_subtrees.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_subtrees['time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtrees = df_subtrees[df_subtrees['time'] >= 1]\n",
    "df_subtrees = df_subtrees[df_subtrees['time'] <= 75]\n",
    "df_subtrees = df_subtrees.reset_index(drop=True)\n",
    "df_subtrees['time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "df_time_analysis3 = df_subtrees['time'].copy()\n",
    "df_time_analysis3 = df_time_analysis3.to_numpy()\n",
    "\n",
    "f,ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "ax.hist(df_time_analysis3, bins=100)\n",
    "ax.set_title(\"Histograma de latencia de cada consulta Log Scale\")\n",
    "ax.set_xlabel(\"Latencia de las consultas\")\n",
    "ax.set_ylabel(\"Cantidad de Consultas\")\n",
    "#ax.set_yscale('log')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_val_prev, ds_test_prev = split_ds(df_subtrees, 0.12,seed=None)\n",
    "ds_train_prev, ds_val_prev = split_ds(ds_train_val_prev, 0.30,seed=None)\n",
    "print(\"ds_train_val_prev.shape\",ds_train_val_prev.shape)\n",
    "print(\"ds_test_prev.shape\",ds_test_prev.shape)\n",
    "print(\"ds_train_prev.shape\",ds_train_prev.shape)\n",
    "print(\"ds_val_prev.shape\",ds_val_prev.shape)\n",
    "ds_train_prev = ds_train_prev.reset_index()\n",
    "ds_val_prev = ds_val_prev.reset_index()\n",
    "ds_test_prev = ds_test_prev.reset_index()\n",
    "ds_rl_prev = ds_rl_prev.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove bad rows\n",
    "ds_train  = ds_train_prev[ds_train_prev['trees'].apply(lambda x: clear_error_tuples(x))]\n",
    "ds_val  = ds_val_prev[ds_val_prev['trees'].apply(lambda x: clear_error_tuples(x))]\n",
    "ds_test  = ds_test_prev[ds_test_prev['trees'].apply(lambda x: clear_error_tuples(x))]\n",
    "ds_rl = ds_rl_prev[ds_rl_prev['trees'].apply(lambda x: clear_error_tuples(x))]\n",
    "\n",
    "print(\"---------SHAPES-----------\")\n",
    "print(\"----------RAW-----------\")\n",
    "print(f'shape df_raw: {df_raw.shape}')\n",
    "print(\"----------PREV----------\")\n",
    "print(f'shape ds_train_val_prev: {ds_train_val_prev.shape}')\n",
    "print(f'shape ds_train_prev: {ds_train_prev.shape}')\n",
    "print(f'shape ds_val_prev: {ds_val_prev.shape}')\n",
    "print(f'shape ds_test_prev: {ds_test_prev.shape}')\n",
    "print(f'shape ds_rl_prev: {ds_rl_prev.shape}')\n",
    "print(\"----------CLEAN----------\")\n",
    "print(f'shape ds_train: {ds_train.shape}')\n",
    "print(f'shape ds_val: {ds_val.shape}')\n",
    "print(f'shape ds_test: {ds_test.shape}')\n",
    "print(f'shape ds_rl: {ds_rl.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train.to_csv(URL + csv_name + '_ds_train.csv')\n",
    "ds_val.to_csv(URL + csv_name + '_ds_val.csv')\n",
    "ds_test.to_csv(URL + csv_name + '_ds_test.csv')\n",
    "ds_rl.to_csv(URL + csv_name + '_ds_rl.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
