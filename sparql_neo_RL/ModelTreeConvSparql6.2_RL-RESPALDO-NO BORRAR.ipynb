{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBRERIAS y CONSTANTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import json, ast, sys, csv, random\n",
    "import plotly.express as px\n",
    "import math\n",
    "import datetime\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "#Implement training process\n",
    "from model_trees_algebra import NeoRegression\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functions.tree_format import IterateBuildTree, InnerJoinsIntraBGPS, \\\n",
    "                                IterateBuildTreeBetweenBGPS, TreeFormat\n",
    "from functions.RL_functions import GetTriplesSubtree, \\\n",
    "                                        GetTreeSize, \\\n",
    "                                        GetAllJoins, \\\n",
    "                                        GetIter, \\\n",
    "                                        GetTotalBgp, \\\n",
    "                                        GetDataframe\n",
    "\n",
    "from functions.aux import MetricTotalAccuraccy\n",
    "\n",
    "\n",
    "class BaoTrainingException(Exception):\n",
    "    pass\n",
    "csv.field_size_limit(sys.maxsize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"/media/data/ccarmona/memoria/dataset/\"\n",
    "csv_name = 'new_dataset_7.1_subqueries'\n",
    "x = [True,False]\n",
    "active_new_data = x[0]\n",
    "symbol = \"ᶲ\"\n",
    "\n",
    "#Este parametro sirve para elegir cierta cantidad de data ordenado por rangos de tiempo obtenidos.\n",
    "## Entre más bajo menos data se seleccionara. Si es muy alto se tendran demasiados valores outliners, \n",
    "## pero si es muy bajo podría tenerse una data no representativa y se aumenta el riesgo de overfitting.\n",
    "## Por otro lado min_data, simplemente da el valor minimo de tiempo de ejecución que tiene una consulta tomada\n",
    "## en cuenta para hacer el modelo\n",
    "#percent_of_data_or = 0.93\n",
    "min_time_or = 15\n",
    "max_time_or = 80\n",
    "#percent_of_data = 1\n",
    "min_time = 5\n",
    "max_time = 80\n",
    "optimizer = \"Adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_ds(all_data, val_rate, seed):\n",
    "    \"\"\"\n",
    "    Used  to keep a balance of sets with respect to runtime of queries. \n",
    "    test_rate is a rate of the total,\n",
    "    val_rate is a rate of the (total - test_rate)\n",
    "    :param all_data: Pandas dataframe with data\n",
    "    :param val_rate: Rate of the (total - test_rate)\n",
    "    :param seed: For replication of results, this fixes the seed of split method. \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    ranges = {}\n",
    "    ranges['1_2'] = all_data[(all_data[\"time\"] >= min_time)    & (all_data[\"time\"] <= 2)]\n",
    "    ranges['2_3'] = all_data[(all_data[\"time\"] > 2)    & (all_data[\"time\"] <= 3)]\n",
    "    ranges['3_4'] = all_data[(all_data[\"time\"] > 3)    & (all_data[\"time\"] <= 4)]\n",
    "    ranges['4_5'] = all_data[(all_data[\"time\"] > 4)    & (all_data[\"time\"] <= 5)]\n",
    "    ranges['5_8'] = all_data[(all_data[\"time\"] > 5)    & (all_data[\"time\"] <= 8)]\n",
    "    ranges['8_10'] = all_data[(all_data[\"time\"] > 8)   & (all_data[\"time\"] <= 10)]\n",
    "    ranges['10_20'] =   all_data[(all_data[\"time\"] > 10) & (all_data[\"time\"] <= 20)]\n",
    "    ranges['20_30'] =   all_data[(all_data[\"time\"] > 20) & (all_data[\"time\"] <= 30)]\n",
    "    ranges['30_40'] =   all_data[(all_data[\"time\"] > 30) & (all_data[\"time\"] <= 40)]\n",
    "    ranges['40_50'] =   all_data[(all_data[\"time\"] > 40) & (all_data[\"time\"] <= 50)]\n",
    "    ranges['50_60'] =   all_data[(all_data[\"time\"] > 50) & (all_data[\"time\"] <= 60)]\n",
    "    ranges['60_80'] =   all_data[(all_data[\"time\"] > 60) & (all_data[\"time\"] <= 80)]\n",
    "    ranges['80_100'] =  all_data[(all_data[\"time\"] > 80) & (all_data[\"time\"] <= 100)]\n",
    "    ranges['100_150'] = all_data[(all_data[\"time\"] > 100) & (all_data[\"time\"] <= 150)]\n",
    "    ranges['150_200'] = all_data[(all_data[\"time\"] > 150) & (all_data[\"time\"] <= 200)]\n",
    "    ranges['200_250'] = all_data[(all_data[\"time\"] > 200) & (all_data[\"time\"] <= 250)]\n",
    "    ranges['250_450'] = all_data[(all_data[\"time\"] > 250) & (all_data[\"time\"] <= 450)]\n",
    "    ranges['450_last'] = all_data[(all_data[\"time\"] > 450)]\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    for rang in ranges.values():\n",
    "        if rang.shape[0] >= 3:\n",
    "            X_train, X_val = train_test_split(\n",
    "                rang, test_size=val_rate, shuffle=True,random_state=seed)\n",
    "\n",
    "            train_data.append(X_train)\n",
    "            val_data.append(X_val)\n",
    "    train_data_list = pd.concat(train_data)\n",
    "    val_data_list = pd.concat(val_data)\n",
    "    #print(\"Shapes : Train: {} Val: {}\".format(train_data_list.shape, val_data_list.shape))\n",
    "    return train_data_list, val_data_list\n",
    "def clear_error_tuples(x):\n",
    "    try:\n",
    "        json.loads(x)\n",
    "        return True\n",
    "    except:\n",
    "        print(\"Error in data ignored!\", x)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to use.\n",
    "list_columns = ['total_bgps', 'triples', 'treesize', 'join', 'left_join']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLEAN DATA AND CREATE NEW DATASET TRAIN-TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(URL + csv_name + \".csv\", engine='python', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_raw.shape\", df_raw.shape)\n",
    "df_raw['time'].value_counts(bins=100, sort=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sparql_file' in df_raw.columns:\n",
    "    df_raw = df_raw.rename(columns = {'sparql_file': 'query'}, inplace = False)\n",
    "df_raw['cpu_p'] = df_raw['cpu_p'].apply(lambda x: float(x.strip('%')))\n",
    "df_raw['same_seg_p'] = df_raw['same_seg_p'].apply(lambda x: float(x.strip('%')))\n",
    "df_raw['same_page_p'] = df_raw['same_page_p'].apply(lambda x: float(x.strip('%')))\n",
    "df_raw['wait'] = df_raw['wait'].apply(lambda x: float(x.strip('%')))\n",
    "df_raw['comp_read_p'] = df_raw['comp_read_p'].apply(lambda x: float(x.strip('%')))\n",
    "df_raw['comp_clw'] = df_raw['comp_clw'].apply(lambda x: float(x.strip('%')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mean\", df_raw['time'].mean())\n",
    "print(\"std\", df_raw['time'].std())\n",
    "print('df_raw.shape',df_raw.shape)\n",
    "print(\"max\", df_raw['time'].max())\n",
    "print(\"df_raw.shape\", df_raw.shape)\n",
    "df_raw = df_raw[df_raw['time'] <= max_time_or]\n",
    "df_raw = df_raw[df_raw['time'] >= min_time_or]\n",
    "df_raw = df_raw.reset_index(drop=True)\n",
    "print(\"CLEAN by TIME\")\n",
    "print(\"mean\", df_raw['time'].mean())\n",
    "print(\"std\", df_raw['time'].std())\n",
    "print(\"max\", df_raw['time'].max())\n",
    "print(\"df_raw.shape\", df_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_model, ds_rl_prev = split_ds(df_raw, 0.15,seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ds_model.shape\",ds_model.shape)\n",
    "print(\"ds_rl_prev.shape\",ds_rl_prev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_val_prev, ds_test_prev = split_ds(ds_model, 0.2,seed=None)\n",
    "ds_train_prev, ds_val_prev = split_ds(ds_train_val_prev, 0.25,seed=None)\n",
    "print(\"ds_train_val_prev.shape\",ds_train_val_prev.shape)\n",
    "print(\"ds_test_prev.shape\",ds_test_prev.shape)\n",
    "print(\"ds_train_prev.shape\",ds_train_prev.shape)\n",
    "print(\"ds_val_prev.shape\",ds_val_prev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtree_format(df_raw):\n",
    "    df_raw_unique_id = df_raw['unique_id']\n",
    "    df_raw_filename = df_raw['filename']\n",
    "    df_raw_query = df_raw['query']\n",
    "    df_raw_bgp = df_raw['bgps']\n",
    "    df_raw_json_cardinality = df_raw['json_cardinality']\n",
    "    df_raw_subtrees = df_raw['matrix_subtrees']\n",
    "    columns = ['unique_id', 'filename', 'query', 'trees',  'bgps' ,'time', 'total_bgps', 'triples', 'treesize', 'join', 'left_join', 'iter', 'json_cardinality_original_query']\n",
    "    values = []\n",
    "    for dfrs in range(0,len(df_raw_subtrees)):\n",
    "        unique_id = df_raw_unique_id[dfrs]\n",
    "        filename = df_raw_filename[dfrs]\n",
    "        query = df_raw_query[dfrs]\n",
    "        bgp = df_raw_bgp[dfrs]\n",
    "        json_cardinality = df_raw_json_cardinality[dfrs]\n",
    "        lists_type = ast.literal_eval(df_raw_subtrees[dfrs])\n",
    "        for ls in lists_type:\n",
    "            str_subtree = str(ls[0]).replace('\"', ';').replace(\"'\", '\"')\n",
    "            row = [unique_id, filename, query, str_subtree, bgp] + ls[1:] + [json_cardinality]\n",
    "            values.append(row)\n",
    "            \n",
    "    df_subtrees = pd.DataFrame(values, columns=columns)\n",
    "    return df_subtrees\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_cardinality_subtree(df_subtrees):\n",
    "    dfsq_unique_id = df_subtrees['unique_id']\n",
    "    dfsq_filename = df_subtrees['filename']\n",
    "    dfsq_query = df_subtrees['query']\n",
    "    dfsq_trees = df_subtrees['trees']\n",
    "    dfsq_time = df_subtrees['time']\n",
    "    dfsq_total_bgps = df_subtrees['total_bgps']\n",
    "    dfsq_triples = df_subtrees['triples']\n",
    "    dfsq_treesize = df_subtrees['treesize']\n",
    "    dfsq_join = df_subtrees['join']\n",
    "    dfsq_left_join = df_subtrees['left_join']\n",
    "    dfsq_iter = df_subtrees['iter']\n",
    "    dfsq_json_cardinality_original = df_subtrees['json_cardinality_original_query']\n",
    "    columns = ['unique_id', 'filename', 'query', 'trees', 'time', 'total_bgps', 'triples', 'treesize', 'join', 'left_join', 'iter', 'json_cardinality_original_query', 'json_cardinality']\n",
    "    values = []\n",
    "    for df in range(len(dfsq_trees)):\n",
    "        json_cardinality = {}\n",
    "        tree_as_str = str(dfsq_trees[df])\n",
    "        json_cardinality_original = ast.literal_eval(dfsq_json_cardinality_original[df])\n",
    "        for k,v in json_cardinality_original.items():\n",
    "            #fix = k.replace(';','\"')\n",
    "            if k in tree_as_str:\n",
    "                json_cardinality[str(k)] = str(v)\n",
    "        values.append(str(json_cardinality).replace('\"', ';').replace(\"'\", '\"'))  \n",
    "    df_subtrees['json_cardinality'] = values\n",
    "    return df_subtrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_prev = ds_train_prev.reset_index()\n",
    "ds_val_prev = ds_val_prev.reset_index()\n",
    "ds_test_prev = ds_test_prev.reset_index()\n",
    "ds_rl_prev = ds_rl_prev.reset_index()\n",
    "\n",
    "ds_train_prev_subtree = subtree_format(ds_train_prev)\n",
    "ds_val_prev_subtree = subtree_format(ds_val_prev)\n",
    "ds_test_prev_subtree = subtree_format(ds_test_prev)\n",
    "ds_rl_prev_subtree = subtree_format(ds_rl_prev)\n",
    "\n",
    "ds_train_prev_subtree['time'] = ds_train_prev_subtree.time.astype(float)\n",
    "ds_val_prev_subtree['time'] = ds_val_prev_subtree.time.astype(float)\n",
    "ds_test_prev_subtree['time'] = ds_test_prev_subtree.time.astype(float)\n",
    "ds_rl_prev_subtree['time'] = ds_rl_prev_subtree.time.astype(float)\n",
    "\n",
    "#ds_train_prev_subtree = json_cardinality_subtree(ds_train_prev_subtree)\n",
    "#ds_val_prev_subtree = json_cardinality_subtree(ds_val_prev_subtree)\n",
    "#ds_test_prev_subtree = json_cardinality_subtree(ds_test_prev_subtree)\n",
    "#ds_rl_prev_subtree = json_cardinality_subtree(ds_rl_prev_subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_time_limit(df_st, max_time, min_time, printing=True):\n",
    "    if printing:\n",
    "        print(\"+++++++++++++++++++++++++++++++\")\n",
    "        print(\"+++++++++++++++++++++++++++++++\")\n",
    "        print(\"+++++++++++++++++++++++++++++++\")\n",
    "        print(\"---------NO CLEAN------------\")\n",
    "        print(df_st['time'].describe())\n",
    "        print(\"-----------------------------\")\n",
    "        print(\"mean\", df_st['time'].mean())\n",
    "        print(\"std\", df_st['time'].std())\n",
    "        print('shape',df_st.shape)\n",
    "        print(\"max\", df_st['time'].max())\n",
    "    df_st = df_st[df_st['time'] <= max_time]\n",
    "    df_st = df_st[df_st['time'] >= min_time]\n",
    "    df_st = df_st.reset_index(drop=True)\n",
    "    if printing:\n",
    "        print(\"--------FIRST CLEAN-------------\")\n",
    "        print(df_st['time'].describe())\n",
    "        print(\"--------------------------------\")\n",
    "        print(\"mean\", df_st['time'].mean())\n",
    "        print(\"std\", df_st['time'].std())\n",
    "        print('shape',df_st.shape)\n",
    "        print(\"max\", df_st['time'].max())\n",
    "    return df_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printing = True\n",
    "print(\"++++++++++++++++TRAIN++++++++++++++++++\")\n",
    "ds_train_prev_subtree = define_time_limit(ds_train_prev_subtree, max_time, min_time, printing)\n",
    "print(\"++++++++++++++++VALIDATION++++++++++++++++++\")\n",
    "ds_val_prev_subtree = define_time_limit(ds_val_prev_subtree, max_time, min_time, printing)\n",
    "print(\"++++++++++++++++TEST++++++++++++++++++\")\n",
    "ds_test_prev_subtree = define_time_limit(ds_test_prev_subtree, max_time, min_time, printing)\n",
    "print(\"++++++++++++++++RL++++++++++++++++++\")\n",
    "ds_rl_prev_subtree = define_time_limit(ds_rl_prev_subtree, max_time, min_time, printing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove bad rows\n",
    "ds_train  = ds_train_prev_subtree[ds_train_prev_subtree['trees'].apply(lambda x: clear_error_tuples(x))]\n",
    "ds_val  = ds_val_prev_subtree[ds_val_prev_subtree['trees'].apply(lambda x: clear_error_tuples(x))]\n",
    "ds_test  = ds_test_prev_subtree[ds_test_prev_subtree['trees'].apply(lambda x: clear_error_tuples(x))]\n",
    "ds_rl = ds_rl_prev_subtree[ds_rl_prev_subtree['trees'].apply(lambda x: clear_error_tuples(x))]\n",
    "\n",
    "print(\"---------SHAPES-----------\")\n",
    "print(\"----------RAW-----------\")\n",
    "print(f'shape df_raw: {df_raw.shape}')\n",
    "print(\"----------PREV----------\")\n",
    "print(f'shape ds_train_val_prev: {ds_train_val_prev.shape}')\n",
    "print(f'shape ds_train_prev: {ds_train_prev.shape}')\n",
    "print(f'shape ds_val_prev: {ds_val_prev.shape}')\n",
    "print(f'shape ds_test_prev: {ds_test_prev.shape}')\n",
    "print(f'shape ds_rl_prev: {ds_rl_prev.shape}')\n",
    "print(\"----------CLEAN----------\")\n",
    "print(f'shape ds_train: {ds_train.shape}')\n",
    "print(f'shape ds_val: {ds_val.shape}')\n",
    "print(f'shape ds_test: {ds_test.shape}')\n",
    "print(f'shape ds_rl: {ds_rl.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rl.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if active_new_data:\n",
    "#    ds_train.to_csv(URL + csv_name + '_ds_train.csv')\n",
    "#    ds_val.to_csv(URL + csv_name + '_ds_val.csv')\n",
    "#    ds_test.to_csv(URL + csv_name + '_ds_test.csv')\n",
    "#    ds_rl.to_csv(URL + csv_name + '_ds_rl.csv')\n",
    "#    print(\"New csv generates\")\n",
    "#else:\n",
    "#    print(\"Not csv generates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getpredictions_info(x_val_tree, x_val_query, y_val):\n",
    "    \"\"\"\n",
    "    Get statistics by a set of data. Need the previous trained model(availablre  form reg object).\n",
    "    :param x_val_tree: Plan level features.\n",
    "    :param x_val_query: Query level features.\n",
    "    :param y_val: Real execution time\n",
    "    :return: Dict with predictions and metrics (mae, rmse, mse)\n",
    "    \"\"\"\n",
    "    Xt, Xq, Yv = reg.json_loads(x_val_tree, x_val_query.values, y_val)\n",
    "    Xt = [reg.fix_tree(x) for x in Xt]\n",
    "    Xt = reg.tree_transform.transform(Xt)\n",
    "\n",
    "    pairs_val = list(zip(list(zip(Xt, Xq)), Yv))\n",
    "    dataset_val = DataLoader(pairs_val, batch_size=batch_size, num_workers=0, shuffle=False, collate_fn=reg.collate_with_card)\n",
    "    results_val = reg.predict_best(dataset_val)\n",
    "    y_pred_val, y_real_val = zip(*results_val)\n",
    "    mseval = mean_squared_error(y_real_val, y_pred_val)\n",
    "    maeval = mean_absolute_error(y_real_val, y_pred_val)\n",
    "    rmseval = np.sqrt(mseval)\n",
    "    return {\"pred\": y_pred_val, \"real\" : y_real_val, \"mse\": mseval, \"mae\": maeval, \"rmse\": rmseval, \"history\": reg.history}\n",
    "\n",
    "def getpredictions_info_nojc(x_val_tree, x_val_query, y_val):\n",
    "    \"\"\"\n",
    "    Get statistics by a set of data. Need the previous trained model(availablre  form reg object).\n",
    "    :param x_val_tree: Plan level features.\n",
    "    :param x_val_query: Query level features.\n",
    "    :param y_val: Real execution time\n",
    "    :return: Dict with predictions and metrics (mae, rmse, mse)\n",
    "    \"\"\"\n",
    "    Xt, Xq, Yv = reg.json_loads(x_val_tree, x_val_query.values, y_val)\n",
    "    Xt = [reg.fix_tree(x) for x in Xt]\n",
    "    Xt = reg.tree_transform.transform(Xt)\n",
    "\n",
    "    pairs_val = list(zip(list(zip(Xt, Xq)), Yv))\n",
    "    dataset_val = DataLoader(pairs_val, batch_size=batch_size, num_workers=0, shuffle=False, collate_fn=reg.collate)\n",
    "    results_val = reg.predict_best(dataset_val)\n",
    "    y_pred_val, y_real_val = zip(*results_val)\n",
    "    mseval = mean_squared_error(y_real_val, y_pred_val)\n",
    "    maeval = mean_absolute_error(y_real_val, y_pred_val)\n",
    "    rmseval = np.sqrt(mseval)\n",
    "    return {\"pred\": y_pred_val, \"real\" : y_real_val, \"mse\": mseval, \"mae\": maeval, \"rmse\": rmseval, \"history\": reg.history}\n",
    "\n",
    "\n",
    "\n",
    "def getmax(x):\n",
    "    lista=  list(x.values())\n",
    "    maximo = 0\n",
    "    for el in lista:\n",
    "        if (maximo < float(el)):\n",
    "            maximo = float(el)\n",
    "    return maximo\n",
    "\n",
    "def pred2index_dict(x, pred_to_index, maxcardinality):\n",
    "    \"\"\"\n",
    "    get histogram from cardinality features. the values is normalized using the max cardinality of predicate in dataset.\n",
    "    :param x: Tree data from x row sample.\n",
    "    :param pred_to_index: dict with predicates and their index.\n",
    "    :param maxcardinality: Max cardiniality in the dataset.\n",
    "    :return: dictionary with feature json_cardinality.\n",
    "    \"\"\"\n",
    "    resp = {}\n",
    "    x = json.loads(x)\n",
    "    for el in x.keys():\n",
    "        if el in pred_to_index:\n",
    "            resp[pred_to_index[el]] = float(x[el])/maxcardinality\n",
    "    return resp\n",
    "\n",
    "def prepare_query_level_data(x_train_query, x_val_query, x_test_query):\n",
    "    \"\"\" Apply StandardScaller to columns except for json_cardinality that need other proccess\"\"\"\n",
    "    maxcardinality =  x_train_query['json_cardinality'].apply(lambda x: json.loads(x)).apply(lambda x: getmax(x)).max()\n",
    "    #Scale x_query data.\n",
    "    xqtrain = x_train_query.drop(columns=['json_cardinality'])\n",
    "    xqval   = x_val_query.drop(columns=['json_cardinality'])\n",
    "    xqtest   = x_test_query.drop(columns=['json_cardinality'])\n",
    "\n",
    "    scalerx = StandardScaler()\n",
    "    x_train_scaled = scalerx.fit_transform(xqtrain)\n",
    "    x_val_scaled = scalerx.transform(xqval)\n",
    "    x_test_scaled = scalerx.transform(xqtest)\n",
    "\n",
    "    x_train_query =pd.concat([pd.DataFrame(x_train_scaled, index=xqtrain.index, columns=xqtrain.columns),x_train_query[['json_cardinality']]], axis=1)\n",
    "    x_val_query =  pd.concat([pd.DataFrame(x_val_scaled,   index=xqval.index, columns=xqval.columns),x_val_query[['json_cardinality']]], axis=1)\n",
    "    x_test_query =  pd.concat([pd.DataFrame(x_test_scaled,   index=xqtest.index, columns=xqtest.columns),x_test_query[['json_cardinality']]], axis=1)\n",
    "\n",
    "    x_train_query['json_cardinality'] = x_train_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(),maxcardinality))\n",
    "    x_val_query['json_cardinality'] = x_val_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(), maxcardinality))\n",
    "    x_test_query['json_cardinality'] = x_test_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(), maxcardinality))\n",
    "\n",
    "    \n",
    "    \n",
    "    return x_train_query, x_val_query, x_test_query\n",
    "\n",
    "def prepare_query_level_only_json_cardinality(x_train_query, x_val_query, x_test_query, x_rl_query):\n",
    "    \"\"\" Apply StandardScaller to columns except for json_cardinality that need other proccess\"\"\"\n",
    "    maxcardinality =  x_train_query['json_cardinality'].apply(lambda x: json.loads(x)).apply(lambda x: getmax(x)).max()\n",
    "    \n",
    "    x_train_query = x_train_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(),maxcardinality))\n",
    "    x_val_query = x_val_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(), maxcardinality))\n",
    "    x_test_query = x_test_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(), maxcardinality))\n",
    "    x_rl_query = x_rl_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(), maxcardinality))\n",
    "    \n",
    "    \n",
    "    return x_train_query, x_val_query, x_test_query, x_rl_query\n",
    "\n",
    "\n",
    "def prepare_query_level_data_full(x_train_query, x_val_query, x_test_query, x_rl_query):\n",
    "    \"\"\" Apply StandardScaller to columns except for json_cardinality that need other proccess\"\"\"\n",
    "    maxcardinality =  x_train_query['json_cardinality'].apply(lambda x: json.loads(x)).apply(lambda x: getmax(x)).max()\n",
    "    #Scale x_query data.\n",
    "    xqtrain = x_train_query.drop(columns=['json_cardinality'])\n",
    "    xqval   = x_val_query.drop(columns=['json_cardinality'])\n",
    "    xqtest   = x_test_query.drop(columns=['json_cardinality'])\n",
    "    xqrl = x_rl_query.drop(columns=['json_cardinality'])\n",
    "    \n",
    "    scalerx = StandardScaler()\n",
    "    x_train_scaled = scalerx.fit_transform(xqtrain)\n",
    "    x_val_scaled = scalerx.transform(xqval)\n",
    "    x_test_scaled = scalerx.transform(xqtest)\n",
    "    x_rl_scaled = scalerx.transform(xqrl)\n",
    "    \n",
    "    \n",
    "    x_train_query = pd.concat([pd.DataFrame(x_train_scaled, index=xqtrain.index, columns=xqtrain.columns),x_train_query[['json_cardinality']]], axis=1)\n",
    "    x_val_query =  pd.concat([pd.DataFrame(x_val_scaled,   index=xqval.index, columns=xqval.columns),x_val_query[['json_cardinality']]], axis=1)\n",
    "    x_test_query =  pd.concat([pd.DataFrame(x_test_scaled,   index=xqtest.index, columns=xqtest.columns),x_test_query[['json_cardinality']]], axis=1)\n",
    "    x_rl_query = pd.concat([pd.DataFrame(x_rl_scaled,   index=xqrl.index, columns=xqrl.columns),x_rl_query[['json_cardinality']]], axis=1)\n",
    "    \n",
    "    \n",
    "    x_train_query['json_cardinality'] = x_train_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(),maxcardinality))\n",
    "    x_val_query['json_cardinality'] = x_val_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(), maxcardinality))\n",
    "    x_test_query['json_cardinality'] = x_test_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(), maxcardinality))\n",
    "    x_rl_query['json_cardinality'] = x_rl_query['json_cardinality'].apply(lambda x: pred2index_dict(x, reg.get_pred(), maxcardinality))\n",
    "    \n",
    "    \n",
    "    return x_train_query, x_val_query, x_test_query, x_rl_query\n",
    "\n",
    "def prepare_query_level_data_full_no_jc(x_train_query, x_val_query, x_test_query, x_rl_query):\n",
    "    \"\"\" Apply StandardScaller to columns except for json_cardinality that need other proccess\"\"\"\n",
    "    scalerx = StandardScaler()\n",
    "    columns_train, index_train = x_train_query.columns, x_train_query.index\n",
    "    columns_val, index_val = x_val_query.columns, x_val_query.index\n",
    "    columns_test, index_test = x_test_query.columns, x_test_query.index\n",
    "    columns_rl, index_rl = x_rl_query.columns, x_rl_query.index\n",
    "    \n",
    "    x_train_scaled = scalerx.fit_transform(x_train_query)\n",
    "    x_val_scaled = scalerx.transform(x_val_query)\n",
    "    x_test_scaled = scalerx.transform(x_test_query)\n",
    "    x_rl_scaled = scalerx.transform(x_rl_query)\n",
    "    \n",
    "    x_train_query = pd.DataFrame(x_train_scaled,   index=index_train, columns=columns_test)\n",
    "    x_val_query = pd.DataFrame(x_val_scaled,   index=index_val, columns=columns_val)\n",
    "    x_test_query = pd.DataFrame(x_test_scaled,   index=index_test, columns=columns_test)\n",
    "    x_rl_query = pd.DataFrame(x_rl_scaled,   index=index_rl, columns=columns_rl)\n",
    "    return x_train_query, x_val_query, x_test_query, x_rl_query\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### TreeConv Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folds_execution = {}\n",
    "print(\"Size Train: {}, Val {}\".format(ds_train.shape[0], ds_val.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get query level data\n",
    "x_train_query = ds_train[list_columns]\n",
    "x_val_query   = ds_val[list_columns]\n",
    "\n",
    "x_train_query_str = ds_train['query']\n",
    "x_val_query_str = ds_train['query']\n",
    "\n",
    "# get plan level datba\n",
    "x_train_tree = ds_train['trees'].values\n",
    "x_val_tree = ds_val['trees'].values\n",
    "\n",
    "y_train = ds_train['time'].values\n",
    "y_val = ds_val['time'].values\n",
    "\n",
    "x_test_tree = ds_test['trees'].values\n",
    "y_test = ds_test['time'].values\n",
    "x_test_query   = ds_test[list_columns]\n",
    "\n",
    "x_rl_tree = ds_rl['trees'].values\n",
    "y_rl = ds_rl['time'].values\n",
    "x_rl_query   = ds_rl[list_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"---------SHAPES-----------\")\n",
    "print(\"----------CLEAN-----------\")\n",
    "print(f'shape ds_train: {ds_train.shape}')\n",
    "print(f'shape ds_val  : {ds_val.shape}')\n",
    "print(f'shape ds_test : {ds_test.shape}')\n",
    "print(f'shape ds_rl : {ds_rl.shape}')\n",
    "print(\"\")\n",
    "print(\"-----TRAIN AND VAL DATA-----\")\n",
    "print(\"----------x_query_data----------\")\n",
    "print(f'shape x_val_query  : {x_val_query.shape}')\n",
    "print(f'shape x_train_query: {x_train_query.shape}')\n",
    "print(\"----------x_plan_level_data----------\")\n",
    "print(f'shape x_val_tree  : {x_val_tree.shape}')\n",
    "print(f'shape x_train_tree: {x_train_tree.shape}')\n",
    "print(\"----------y_data------------\")\n",
    "print(f'shape y_val  : {y_val.shape}')\n",
    "print(f'shape y_train: {y_train.shape}')\n",
    "print(\"\")\n",
    "print(\"----------TEST DATA----------\")\n",
    "print(f'shape x_test_tree : {x_test_tree.shape}')\n",
    "print(f'shape x_test_query: {x_test_query.shape}')\n",
    "print(f'shape y_test      : {y_test.shape}')\n",
    "print(\"-----------------------\")\n",
    "print(\"\")\n",
    "print(\"----------RL DATA----------\")\n",
    "print(f'shape x_rl_tree : {x_rl_tree.shape}')\n",
    "print(f'shape x_rl_query: {x_rl_query.shape}')\n",
    "print(f'shape y_rl      : {y_rl.shape}')\n",
    "print(\"-----------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxcardinality = 0\n",
    "#maxcardinality =  x_train_query['json_cardinality'].apply(lambda x: json.loads(x)).apply(lambda x: getmax(x)).max()\n",
    "maxcardinality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeoRegression\n",
    "Esta en model_trees_algebra.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aec_dir = ''\n",
    "learning_rate = 0.00010\n",
    "batch_size = 64\n",
    "verbose=True\n",
    "reg = NeoRegression(\n",
    "     aec={'train_aec': False, 'use_aec': True,'aec_file': '', 'aec_epochs': 200},\n",
    "     epochs=400,\n",
    "     maxcardinality=maxcardinality,\n",
    "     in_channels_neo_net=64,\n",
    "     tree_units=[64,32],\n",
    "     tree_units_dense=[16],\n",
    "     early_stop_patience=10,\n",
    "     early_stop_initial_patience=180,\n",
    "     tree_activation_tree=nn.LeakyReLU,\n",
    "     tree_activation_dense=nn.ReLU,\n",
    "     optimizer={'optimizer': optimizer,'args':{\"lr\":learning_rate}},\n",
    "     figimage_size=(18,18),\n",
    "     start_history_from_epoch=3,\n",
    "     batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.utils import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fit the transformer tree data\n",
    "reg.fit_transform_tree_data(ds_train, ds_val, ds_test, ds_rl)\n",
    "print(\"Trees tranformed!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with io.capture_output() as captured:\n",
    "#x_train_query, x_val_query, x_test_query, x_rl_query =  prepare_query_level_data_full(x_train_query, x_val_query, x_test_query, x_rl_query)\n",
    "x_train_query, x_val_query, x_test_query, x_rl_query =  prepare_query_level_data_full_no_jc(x_train_query, x_val_query, x_test_query, x_rl_query)\n",
    "print(\"END PREPARE QUERY LEVEL DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(float)\n",
    "y_val = y_val.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Fit model\n",
    "reg.fit(x_train_tree, x_train_query.values, y_train, x_val_tree, x_val_query.values, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save best model\n",
    "import torch\n",
    "torch.save(reg.best_model.state_dict(), \"./best_model_fullADAM-7.0_V3_el_mejor_hasta_ahora.pt\")\n",
    "torch.save(reg.best_model.state_dict(), \"./best_model_brigido.pt\")\n",
    "#Save stats in val set\n",
    "file_to_store = open(\"./execution_model_stats_fullADAM-7.0_V3_el_mejor_hasta_ahora.pickle\", \"wb\")\n",
    "pickle.dump(getpredictions_info_nojc(x_val_tree, x_val_query, y_val), file_to_store)\n",
    "file_to_store.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#val_stats = getpredictions_info(x_val_tree, x_val_query, y_val)\n",
    "val_stats = getpredictions_info_nojc(x_val_tree, x_val_query, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "ds_val['y_pred'] = val_stats['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_stats = getpredictions_info_nojc(x_test_tree, x_test_query, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "ds_test['y_realcheck'] = test_stats['real']\n",
    "ds_test['y_pred'] = test_stats['pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def tag_points(x):\n",
    "    \"\"\"Add quality tags of predictions. Used to plot with plotly\"\"\"\n",
    "    difference = x['time'] - x['y_pred'][0]\n",
    "    abs_diff = np.abs(difference)\n",
    "    x['y_pred'] = x['y_pred'][0]\n",
    "    x['query2'] = x['query'].replace(\" . \", ' . <br>').replace(\" FILTER\", '<br> FILTER').replace(\" { \", ' { <br>').replace(\" } \", ' <br> }').replace(\" ; \", ' ; <br>') \n",
    "    p20 = x['time'] * 0.2\n",
    "    p40 = x['time'] * 0.4\n",
    "    if abs_diff < p20:\n",
    "        x['color'] = \"good prediction\"\n",
    "    elif abs_diff < p40:\n",
    "        x['color'] = \"aceptable prediction\"\n",
    "    else:\n",
    "        x['color'] = \"bad prediction\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_points2(x):\n",
    "    \"\"\"Add quality tags of predictions. Used to plot with plotly\"\"\"\n",
    "    difference = x['time'] - x['y_realcheck'][0]\n",
    "    abs_diff = np.abs(difference)\n",
    "    x['y_realcheck'] = x['y_realcheck'][0]\n",
    "    x['query2'] = x['query'].replace(\" . \", ' . <br>').replace(\" FILTER\", '<br> FILTER').replace(\" { \", ' { <br>').replace(\" } \", ' <br> }').replace(\" ; \", ' ; <br>') \n",
    "    p20 = x['time'] * 0.2\n",
    "    p40 = x['time'] * 0.4\n",
    "    if abs_diff < p20:\n",
    "        x['color'] = \"good prediction\"\n",
    "    elif abs_diff < p40:\n",
    "        x['color'] = \"aceptable prediction\"\n",
    "    else:\n",
    "        x['color'] = \"bad prediction\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#tag_points(ds_test.values[0])\n",
    "other = ds_test.apply(lambda x: tag_points(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "other.to_pickle(\"./predictions_test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "otherval = ds_val.apply(lambda x: tag_points(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "otherval.to_pickle(\"./predictions_val.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = px.scatter(otherval[['query','query2','time','y_pred','color']], x=\"y_pred\", y=\"time\", color=\"color\", hover_data=['query2'])\n",
    "fig.update_layout(height=800, width=1000, title_text=\"Predictions on Val Set\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = px.scatter(other[['query','query2','time','y_pred','y_realcheck','color']], x=\"y_pred\", y=\"time\", color=\"color\", hover_data=['query2'])\n",
    "fig.update_layout(height=800, width=1000, title_text=\"Predictions on Test Set\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_val,b_val,a_val,g_val,bp_val,ap_val,gp_val = MetricTotalAccuraccy(otherval)\n",
    "print(f\"Total predictions: {tot_val}\")\n",
    "print(f\"Bad predictions: {b_val}, percentage {bp_val}%\")\n",
    "print(f\"Acceptable predictions: {a_val}, percentage {ap_val}%\")\n",
    "print(f\"Good predictions: {g_val}, percentage {gp_val}%\")\n",
    "\n",
    "print(f\"Accuraccy: {100*(a_val+g_val)/tot_val}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_val,b_val,a_val,g_val,bp_val,ap_val,gp_val = MetricTotalAccuraccy(other)\n",
    "print(f\"Total predictions: {tot_val}\")\n",
    "print(f\"Bad predictions: {b_val}, percentage {bp_val}%\")\n",
    "print(f\"Acceptable predictions: {a_val}, percentage {ap_val}%\")\n",
    "print(f\"Good predictions: {g_val}, percentage {gp_val}%\")\n",
    "\n",
    "print(f\"Accuraccy: {100*(a_val+g_val)/tot_val}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTriplesSubtree(subtree_as_str):\n",
    "    code_tpf = ['VAR_VAR_VAR', 'VAR_VAR_URI', 'VAR_URI_VAR', 'VAR_URI_URI', 'VAR_URI_LITERAL', 'VAR_VAR_LITERAL',\n",
    "                'URI_URI_LITERAL', 'URI_URI_VAR', 'URI_URI_URI', 'URI_VAR_VAR', 'URI_VAR_URI', 'URI_VAR_LITERAL',\n",
    "                'LITERAL_URI_VAR', 'LITERAL_URI_URI', 'LITERAL_URI_LITERAL']\n",
    "    total_triples = 0\n",
    "    for i in code_tpf:\n",
    "        total_triples += subtree_as_str.count(i)\n",
    "    #if subtree_as_str in code_tpf:\n",
    "    #    total_triples += 1\n",
    "    return total_triples\n",
    "\n",
    "def GetTreeSize(subtrees, treesize):\n",
    "    if len(subtrees) == 1:\n",
    "        return treesize\n",
    "    else:\n",
    "        treesize += 1\n",
    "        left_treesize = GetTreeSize(subtrees[1], treesize)\n",
    "        right_treesize = GetTreeSize(subtrees[2], treesize)\n",
    "        treesize = max(left_treesize,right_treesize)\n",
    "    return treesize\n",
    "\n",
    "def GetAllJoins(subtree_as_str):\n",
    "    join = subtree_as_str.count('JOIN')\n",
    "    left_join = subtree_as_str.count('LEFT_JOIN')\n",
    "    return join-left_join, left_join\n",
    "\n",
    "def GetIter(subtree_as_str):\n",
    "    if \"iter\" in subtree_as_str:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def GetTotalBgp(state):\n",
    "    bgp_list = []\n",
    "    for s in state:\n",
    "        bgp_list.append(s[1])\n",
    "    bgp_list = set(bgp_list)\n",
    "    return len(bgp_list)\n",
    "\n",
    "def GetDataframe(subtree, state, columns):\n",
    "    subtree_str = str(subtree)\n",
    "    subtree_list = ast.literal_eval(subtree[0].tolist())\n",
    "    total_bgps = GetTotalBgp(state)\n",
    "    treesize = GetTreeSize(subtree_list, 1)\n",
    "    triples = GetTriplesSubtree(subtree_str)\n",
    "    join, left_join = GetAllJoins(subtree_str)\n",
    "    iters = GetIter(subtree_str)\n",
    "    \n",
    "    values = [total_bgps, triples, treesize, join, left_join]\n",
    "    x_rl_query = pd.DataFrame([values], columns = columns)\n",
    "    return x_rl_query\n",
    "    #print(x_rl_query)\n",
    "    #scalerx = StandardScaler()\n",
    "    #values_scaled = scalerx.fit_transform(x_rl_query)\n",
    "    #x_rl_query = pd.DataFrame(values_scaled, columns = columns)\n",
    "    #print(x_rl_query)\n",
    "    \n",
    "def RL_Actions(bgp):\n",
    "    actions = []\n",
    "    idx = 0\n",
    "    for k,v in bgp.items():\n",
    "        bgp_list = v['bgp_list']\n",
    "        for b in v['bgp_list']:\n",
    "            actions.append((idx,k,b['P'],b['triple_type']))\n",
    "            idx += 1\n",
    "    return actions\n",
    "\n",
    "\n",
    "def RL_Initial_Step(actions):\n",
    "    initial_state = []\n",
    "    random_index = random.randint(0, len(actions)-1)\n",
    "    random_action = actions[random_index]\n",
    "    initial_state.append(random_action)\n",
    "    return initial_state\n",
    "\n",
    "def RL_available_actions(actions, current_state):\n",
    "    if not current_state:\n",
    "        return actions\n",
    "    available_actions = sorted(list(set(actions) - set(current_state)))\n",
    "    same_bgp_actions = []\n",
    "    other_bgp_actions = []\n",
    "    for i in available_actions:\n",
    "        if current_state[-1][1] == i[1]:\n",
    "            same_bgp_actions.append(i)\n",
    "        else:\n",
    "            other_bgp_actions.append(i)\n",
    "    if same_bgp_actions:\n",
    "        available_actions = same_bgp_actions\n",
    "    else:\n",
    "        available_actions = other_bgp_actions\n",
    "    return available_actions\n",
    "\n",
    "\n",
    "def RL_Argmax(array):\n",
    "    argmax_list = np.argwhere(array == np.amax(array))\n",
    "    argmax_list_flatten = [item for sublist in argmax_list for item in sublist]\n",
    "    return int(random.choice(argmax_list_flatten))\n",
    "\n",
    "def RL_Argmax_available(q_value,available_actions):\n",
    "    available_idx = [i[0] for i in available_actions]\n",
    "    #print(\"available_actions\",available_actions)\n",
    "    #print(\"available_idx\",available_idx)\n",
    "    q_value_available = [q_value[i] for i in available_idx]\n",
    "    #print(\"q_values\",q_value, type(q_value))\n",
    "    #print(\"q_values_available\",q_value_available, type(q_value_available))\n",
    "    \n",
    "    argmax_list = np.argwhere(q_value_available == np.amax(q_value_available))\n",
    "    argmax_list_flatten = [item for sublist in argmax_list for item in sublist]\n",
    "    #print(\"argmax_list\",argmax_list, type(argmax_list))\n",
    "    #print(\"argmax_list_flatten\",argmax_list_flatten, type(argmax_list_flatten))\n",
    "    value = int(random.choice(argmax_list_flatten))\n",
    "    #print(\"value\",value)\n",
    "    \n",
    "    #print(\"..........................\")\n",
    "    return value\n",
    "\n",
    "\n",
    "def RL_Next_step(actions,\n",
    "                current_state,\n",
    "                reward,\n",
    "                q_value,\n",
    "                x_rl_tree_ind,\n",
    "                x_rl_query_ind,\n",
    "                y_rl_ind,\n",
    "                bgp_ind,\n",
    "                epsilon\n",
    "                ):\n",
    "    random_num = np.random.random()\n",
    "    available_actions = RL_available_actions(actions, current_state)\n",
    "    actual_state = len(current_state) - 1\n",
    "    \n",
    "    if random_num > epsilon:\n",
    "        chosen_action_available_idx = random.randint(0, len(available_actions)-1)\n",
    "        chosen_action = available_actions[chosen_action_available_idx]\n",
    "        chosen_action_idx = chosen_action[0]\n",
    "        #print(chosen_action_idx)\n",
    "        current_state.append(chosen_action)\n",
    "        reward, terminal = RL_Reward(actions,\n",
    "                                     available_actions,\n",
    "                                     current_state,\n",
    "                                     chosen_action,\n",
    "                                     reward,\n",
    "                                     x_rl_tree_ind,\n",
    "                                     x_rl_query_ind,\n",
    "                                     y_rl_ind,\n",
    "                                     bgp_ind)\n",
    "    else:\n",
    "        #print(current_state)\n",
    "        chosen_action_available_idx = RL_Argmax_available(q_value[actual_state],available_actions)\n",
    "        chosen_action = available_actions[chosen_action_available_idx]\n",
    "        chosen_action_idx = chosen_action[0]\n",
    "        current_state.append(chosen_action)\n",
    "        reward, terminal = RL_Reward(actions,\n",
    "                                     available_actions,\n",
    "                                     current_state,\n",
    "                                     chosen_action,\n",
    "                                     reward,\n",
    "                                     x_rl_tree_ind,\n",
    "                                     x_rl_query_ind,\n",
    "                                     y_rl_ind,\n",
    "                                     bgp_ind)\n",
    "    return current_state, reward, terminal, chosen_action, chosen_action_idx\n",
    "\n",
    "def RL_Reward(actions,\n",
    "              available_actions,\n",
    "              current_state,\n",
    "              chosen_action,\n",
    "              reward,\n",
    "              x_rl_tree_ind,\n",
    "              x_rl_query_ind,\n",
    "              y_rl_ind,\n",
    "              bgp_ind):\n",
    "    terminal = False\n",
    "    #print(available_actions)\n",
    "    #if chosen_action not in available_actions:\n",
    "    #    reward -= 100000\n",
    "    if len(current_state) == len(actions):\n",
    "        terminal = True\n",
    "    new_dicto = RL_Rebuild_Dictionary(bgp_ind, current_state)\n",
    "    tree_format = TreeFormat(new_dicto,symbol)\n",
    "    #tree_format = ast.literal_eval(tree_format)\n",
    "    new_tree = np.array([str(tree_format).replace('\"', ';').replace(\"'\", '\"')])\n",
    "    x_rl_query = GetDataframe(new_tree, current_state, x_rl_query_ind.columns)\n",
    "    pred = getpredictions_info_nojc(new_tree, x_rl_query, y_rl_ind)['pred']\n",
    "    reward -= pred[0][0]\n",
    "    return reward, terminal\n",
    "\n",
    "def RL_Rebuild_Dictionary(bgp, final_state):\n",
    "    new_dicto = {}\n",
    "    bgp_names = list(set([i[1] for i in final_state]))\n",
    "    ### Keys\n",
    "    for i in bgp_names:\n",
    "        new_dicto[i] = {\"bgp_list\" : [], \"opt\" : bgp[i]['opt']}\n",
    "    for i in final_state:\n",
    "        new_dicto[i[1]][\"bgp_list\"].append({'P' : i[2], 'triple_type' : i[3]})\n",
    "\n",
    "    return new_dicto\n",
    "\n",
    "\n",
    "def RL_First_Policy(actions):\n",
    "    actions_length = len(actions)\n",
    "    return np.zeros((actions_length,actions_length))\n",
    "\n",
    "def RL_bgp_format(ds_rl):\n",
    "    bgps_rl = ds_rl['bgps'].values\n",
    "    bgps_rl = [ast.literal_eval(bgp) for bgp in bgps_rl]\n",
    "    return bgps_rl\n",
    "\n",
    "def RL_get_data(x_rl_tree, x_rl_query, y_rl, bgps_rl,idx):\n",
    "    x_rl_tree_c = x_rl_tree.copy()\n",
    "    x_rl_query_c = x_rl_query.copy()\n",
    "    y_rl_c = y_rl.copy()\n",
    "    bgps_rl_c = bgps_rl.copy()\n",
    "    x_rl_query_c = x_rl_query_c.reset_index(drop=True)\n",
    "    columns = x_rl_query_c.columns\n",
    "    values = x_rl_query_c.values[idx]    \n",
    "    x_rl_tree_test = np.array([x_rl_tree_c[idx]])\n",
    "    x_rl_query_test = pd.DataFrame([values],columns=columns)\n",
    "    y_rl_test = np.array([y_rl_c[idx]])\n",
    "    bgps_test = bgps_rl_c[idx]\n",
    "    return x_rl_tree_test, x_rl_query_test, y_rl_test, bgps_test\n",
    "\n",
    "\n",
    "def RL_get_final_state_bgp_tree(q_value,actions,bgp,symbol):\n",
    "    q_value = q_value.tolist()\n",
    "    best_state = []\n",
    "    arg_max = RL_Argmax(q_value[0])\n",
    "    best_state.append(actions[arg_max])\n",
    "        \n",
    "    for q_val in q_value[1:]:\n",
    "        available_actions = RL_available_actions(actions, best_state)\n",
    "        chosen_action_available_idx = RL_Argmax_available(q_val,available_actions)\n",
    "        chosen_action = available_actions[chosen_action_available_idx]\n",
    "        chosen_action_idx = chosen_action[0]\n",
    "        best_state.append(chosen_action) \n",
    "    new_dicto = RL_Rebuild_Dictionary(bgp, best_state)\n",
    "    new_tree = np.array([str(TreeFormat(new_dicto,symbol)).replace('\"', ';').replace(\"'\", '\"')])\n",
    "    \n",
    "    return best_state, new_dicto, new_tree\n",
    "\n",
    "\n",
    "def RL_results_functions(idx,pred_old, pred_new,prl_tol1,prl_tol2):\n",
    "    difference = pred_new - pred_old\n",
    "    abs_diff = np.abs(difference)\n",
    "    best_of_the_pred = []\n",
    "    similar_of_the_pred = []\n",
    "    worst_of_the_pred = []\n",
    "    if abs_diff < prl_tol1:\n",
    "        best_of_the_pred.append(idx)\n",
    "    elif abs_diff < prl_tol2:\n",
    "        similar_of_the_pred.append(idx)\n",
    "    else:\n",
    "        worst_of_the_pred.append(idx)\n",
    "    return best_of_the_pred, similar_of_the_pred, worst_of_the_pred\n",
    "    \n",
    "def RLNeo_Next_step(actions,\n",
    "                current_state,\n",
    "                reward,\n",
    "                q_value,\n",
    "                x_rl_tree_ind,\n",
    "                x_rl_query_ind,\n",
    "                y_rl_ind,\n",
    "                bgp_ind,\n",
    "                epsilon\n",
    "                ):\n",
    "    random_num = np.random.random()\n",
    "    available_actions = RL_available_actions(actions, current_state)\n",
    "    actual_state = len(current_state) - 1\n",
    "    \n",
    "    if random_num > epsilon:\n",
    "        chosen_action_available_idx = random.randint(0, len(available_actions)-1)\n",
    "        chosen_action = available_actions[chosen_action_available_idx]\n",
    "        chosen_action_idx = chosen_action[0]\n",
    "        #print(chosen_action_idx)\n",
    "        current_state.append(chosen_action)\n",
    "        reward, terminal = RLNeo_Reward(actions,\n",
    "                                     available_actions,\n",
    "                                     current_state,\n",
    "                                     chosen_action,\n",
    "                                     reward,\n",
    "                                     x_rl_tree_ind,\n",
    "                                     x_rl_query_ind,\n",
    "                                     y_rl_ind,\n",
    "                                     bgp_ind)\n",
    "    else:\n",
    "        #print(current_state)\n",
    "        chosen_action_available_idx = RL_Argmax_available(q_value[actual_state],available_actions)\n",
    "        chosen_action = available_actions[chosen_action_available_idx]\n",
    "        chosen_action_idx = chosen_action[0]\n",
    "        current_state.append(chosen_action)\n",
    "        reward, terminal = RLNeo_Reward(actions,\n",
    "                                     available_actions,\n",
    "                                     current_state,\n",
    "                                     chosen_action,\n",
    "                                     reward,\n",
    "                                     x_rl_tree_ind,\n",
    "                                     x_rl_query_ind,\n",
    "                                     y_rl_ind,\n",
    "                                     bgp_ind)\n",
    "    return current_state, reward, terminal, chosen_action, chosen_action_idx\n",
    "\n",
    "def RLNeo_Reward(actions,\n",
    "              available_actions,\n",
    "              current_state,\n",
    "              chosen_action,\n",
    "              reward,\n",
    "              x_rl_tree_ind,\n",
    "              x_rl_query_ind,\n",
    "              y_rl_ind,\n",
    "              bgp_ind):\n",
    "    terminal = False\n",
    "    reward = 0\n",
    "    #print(available_actions)\n",
    "    #if chosen_action not in available_actions:\n",
    "    #    reward -= 100000\n",
    "    if len(current_state) == len(actions):\n",
    "        terminal = True\n",
    "        new_dicto = RL_Rebuild_Dictionary(bgp_ind, current_state)\n",
    "        tree_format = TreeFormat(new_dicto,symbol)\n",
    "        #tree_format = ast.literal_eval(tree_format)\n",
    "        new_tree = np.array([str(tree_format).replace('\"', ';').replace(\"'\", '\"')])\n",
    "        x_rl_query = GetDataframe(new_tree, current_state, x_rl_query_ind.columns)\n",
    "        pred = getpredictions_info_nojc(new_tree, x_rl_query, y_rl_ind)['pred']\n",
    "        reward -= pred[0][0]\n",
    "    return reward, terminal\n",
    "    \n",
    "    \n",
    "def RLNeo_q_values(q_value_neo,current_state, reward):\n",
    "    for q_val, act in zip(q_value_neo,current_state):\n",
    "        q_val[act[0]] = max(q_val[act[0]],reward)\n",
    "    return q_value_neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"----------RL DATA----------\")\n",
    "print(f'shape ds_rl : {ds_rl.shape}')\n",
    "print(f'shape x_rl_tree : {x_rl_tree.shape}')\n",
    "print(f'shape x_rl_query: {x_rl_query.shape}')\n",
    "print(f'shape y_rl      : {y_rl.shape}')\n",
    "print(\"-----------------------\")\n",
    "bgps_rl = RL_bgp_format(ds_rl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buenos_idx = []\n",
    "for i in range(0,x_rl_tree.shape[0]):\n",
    "    x_rl_tree_ind, x_rl_query_ind, y_rl_ind, bgp_ind = RL_get_data(x_rl_tree, x_rl_query, y_rl, bgps_rl,i)\n",
    "    actions = RL_Actions(bgp_ind)\n",
    "    if len(actions) >= 5:\n",
    "        buenos_idx.append(i)\n",
    "random_value = random.choice(buenos_idx)\n",
    "random_value = 1309\n",
    "print(buenos_idx)\n",
    "print(len(buenos_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Loop - 1 Val - General Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_rl_tree_ind, x_rl_query_ind, y_rl_ind, bgp_ind = RL_get_data(x_rl_tree, x_rl_query, y_rl, bgps_rl,random_value)\n",
    "actions = RL_Actions(bgp_ind)\n",
    "num_actions = len(actions)\n",
    "print(\"actions\")\n",
    "print(actions)\n",
    "print(\"num_actions\", num_actions)\n",
    "print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning - 1 Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_value_ql = RL_First_Policy(actions) ##INITIAL POLICITY\n",
    "epsilon_ql = 0.55 ## EPSILON\n",
    "lr_ql = 0.9 ## LEARNING RATE\n",
    "disc_fac_ql = 0.9 ## DISCOUNT FACTOR\n",
    "epoch_ql = 100 ### TOTAL EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "values_ql = []\n",
    "for i in range(0,epoch_ql):\n",
    "    current_state = []\n",
    "    reward = 0\n",
    "    terminal = False\n",
    "    current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,   \n",
    "                                                                                    current_state,\n",
    "                                                                                    reward,\n",
    "                                                                                    q_value_ql,\n",
    "                                                                                    x_rl_tree_ind,\n",
    "                                                                                    x_rl_query_ind,\n",
    "                                                                                    y_rl_ind,\n",
    "                                                                                    bgp_ind,\n",
    "                                                                                    epsilon_ql)\n",
    "    old_state = len(current_state)-1\n",
    "    prev_action = chosen_action_idx\n",
    "    while not terminal:\n",
    "        current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,\n",
    "                                                                                        current_state,\n",
    "                                                                                        reward,\n",
    "                                                                                        q_value_ql,\n",
    "                                                                                        x_rl_tree_ind,\n",
    "                                                                                        x_rl_query_ind,\n",
    "                                                                                        y_rl_ind,\n",
    "                                                                                        bgp_ind,\n",
    "                                                                                        epsilon_ql)\n",
    "        \n",
    "        new_state = len(current_state)-1\n",
    "        if not terminal:\n",
    "            old_q_value = q_value_ql[old_state,prev_action]\n",
    "            temporal_difference = reward + disc_fac_ql * np.max(q_value_ql[new_state,:]) - old_q_value\n",
    "            new_q_value = old_q_value + lr_ql * temporal_difference\n",
    "            q_value_ql[old_state,prev_action] = new_q_value\n",
    "        old_state = new_state\n",
    "        prev_action = chosen_action_idx\n",
    "    old_q_value = q_value_ql[old_state,prev_action]\n",
    "    new_q_value = old_q_value + lr_ql * (reward - old_q_value)\n",
    "    q_value_ql[old_state,prev_action] = new_q_value\n",
    "    best_state, new_dicto, new_tree = RL_get_final_state_bgp_tree(q_value_ql,actions,bgp_ind,symbol)\n",
    "    rl_stats_old = getpredictions_info_nojc(x_rl_tree_ind, x_rl_query_ind, y_rl_ind)\n",
    "    rl_stats_new = getpredictions_info_nojc(new_tree, x_rl_query_ind, y_rl_ind)\n",
    "    row = [\n",
    "                    random_value,\n",
    "                    i,\n",
    "                    rl_stats_old['pred'][0][0],\n",
    "                    rl_stats_new['pred'][0][0],\n",
    "                    float(rl_stats_old['real'][0][0]),\n",
    "                    float(rl_stats_new['real'][0][0]), \n",
    "                    rl_stats_old['mse'],\n",
    "                    rl_stats_new['mse'],\n",
    "                    rl_stats_old['mae'],\n",
    "                    rl_stats_new['mae'],\n",
    "                    rl_stats_old['rmse'],\n",
    "                    rl_stats_new['rmse'],\n",
    "                    rl_stats_old['history'],\n",
    "                    rl_stats_new['history'],\n",
    "           ]\n",
    "\n",
    "    values_ql.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_state, new_dicto, new_tree = RL_get_final_state_bgp_tree(q_value_ql,actions,bgp_ind,symbol)\n",
    "print(\"best_state \\n\",best_state)\n",
    "print(\"new_dicto \\n\",new_dicto)\n",
    "print(\"new_tree \\n\",new_tree)\n",
    "print(\"old_tree \\n\", x_rl_tree_ind)\n",
    "print(\"\\n\")\n",
    "print(\"VALORES CON VIEJO ARBOL\")\n",
    "rl_stats_old = getpredictions_info_nojc(x_rl_tree_ind, x_rl_query_ind, y_rl_ind)\n",
    "print(\"valor predict: \",rl_stats_old['pred'][0][0])\n",
    "print(\"valor real: \",float(rl_stats_old['real'][0][0]))\n",
    "\n",
    "print(\"VALORES CON NUEVO ARBOL\")\n",
    "rl_stats_new = getpredictions_info_nojc(new_tree, x_rl_query_ind, y_rl_ind)\n",
    "print(\"valor predict: \",rl_stats_new['pred'][0][0])\n",
    "print(\"valor real: \",float(rl_stats_new['real'][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "values_ql[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_value_sar = RL_First_Policy(actions) ##INITIAL POLICITY\n",
    "epsilon_sar = 0.75 ## EPSILON\n",
    "lr_sar = 0.9 ## LEARNING RATE\n",
    "disc_fac_sar = 0.9 ## DISCOUNT FACTOR\n",
    "epoch_sar = 1000 ### TOTAL EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loop 1 Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,epoch_sar):\n",
    "    current_state = []\n",
    "    reward = 0\n",
    "    terminal = False\n",
    "    current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,\n",
    "                                                                                    current_state,\n",
    "                                                                                    reward,\n",
    "                                                                                    q_value_sar,\n",
    "                                                                                    x_rl_tree_ind,\n",
    "                                                                                    x_rl_query_ind,\n",
    "                                                                                    y_rl_ind,\n",
    "                                                                                    bgp_ind,\n",
    "                                                                                    epsilon_sar)\n",
    "    old_state = len(current_state)-1\n",
    "    prev_action = chosen_action_idx\n",
    "    while not terminal:\n",
    "        current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,\n",
    "                                                                                        current_state,\n",
    "                                                                                        reward,\n",
    "                                                                                        q_value_sar,\n",
    "                                                                                        x_rl_tree_ind,\n",
    "                                                                                        x_rl_query_ind,\n",
    "                                                                                        y_rl_ind,\n",
    "                                                                                        bgp_ind,\n",
    "                                                                                        epsilon_sar)\n",
    "        \n",
    "        new_state = len(current_state)-1\n",
    "        if not terminal:\n",
    "            old_q_value = q_value_sar[old_state,prev_action]\n",
    "            temporal_difference = reward + (disc_fac_sar * q_value_sar[new_state,chosen_action_idx]) - old_q_value\n",
    "            new_q_value = old_q_value + lr_sar * temporal_difference\n",
    "            q_value_sar[old_state,prev_action] = new_q_value\n",
    "        old_state = new_state\n",
    "        prev_action = chosen_action_idx\n",
    "    old_q_value = q_value_sar[old_state,prev_action]\n",
    "    new_q_value = old_q_value + lr_sar * (reward - old_q_value)\n",
    "    q_value_sar[old_state,prev_action] = new_q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_state, new_dicto, new_tree = RL_get_final_state_bgp_tree(q_value_sar,actions,bgp_ind,symbol)\n",
    "print(\"best_state \\n\",best_state)\n",
    "print(\"new_dicto \\n\",new_dicto)\n",
    "print(\"new_tree \\n\",new_tree)\n",
    "print(\"old_tree \\n\", x_rl_tree_ind)\n",
    "print(\"\\n\")\n",
    "print(\"VALORES CON VIEJO ARBOL\")\n",
    "rl_stats_old = getpredictions_info_nojc(x_rl_tree_ind, x_rl_query_ind, y_rl_ind)\n",
    "print(\"valor predict: \",rl_stats_old['pred'][0][0])\n",
    "print(\"valor real: \",int(rl_stats_old['real'][0][0]))\n",
    "\n",
    "print(\"VALORES CON NUEVO ARBOL\")\n",
    "rl_stats_new = getpredictions_info_nojc(new_tree, x_rl_query_ind, y_rl_ind)\n",
    "print(\"valor predict: \",rl_stats_new['pred'][0][0])\n",
    "print(\"valor real: \",int(rl_stats_new['real'][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_value_exsar = RL_First_Policy(actions) ##INITIAL POLICITY\n",
    "epsilon_exsar = 0.75 ## EPSILON\n",
    "lr_exsar = 0.9 ## LEARNING RATE\n",
    "disc_fac_exsar = 0.9 ## DISCOUNT FACTOR\n",
    "epoch_exsar = 1000 ### TOTAL EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loop 1 Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,epoch_exsar):\n",
    "    current_state = []\n",
    "    reward = 0\n",
    "    terminal = False\n",
    "    current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,\n",
    "                                                                                    current_state,\n",
    "                                                                                    reward,\n",
    "                                                                                    q_value_exsar,\n",
    "                                                                                    x_rl_tree_ind,\n",
    "                                                                                    x_rl_query_ind,\n",
    "                                                                                    y_rl_ind,\n",
    "                                                                                    bgp_ind,\n",
    "                                                                                    epsilon_exsar)    \n",
    "    old_state = len(current_state)-1\n",
    "    prev_action = chosen_action_idx\n",
    "    while not terminal:\n",
    "        current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,\n",
    "                                                                                        current_state,\n",
    "                                                                                        reward,\n",
    "                                                                                        q_value_exsar,\n",
    "                                                                                        x_rl_tree_ind,\n",
    "                                                                                        x_rl_query_ind,\n",
    "                                                                                        y_rl_ind,\n",
    "                                                                                        bgp_ind,\n",
    "                                                                                        epsilon_exsar)\n",
    "        new_state = len(current_state)-1\n",
    "        array_state = np.array(current_state)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        expected_q = 0\n",
    "        old_q_value = q_value_exsar[old_state,prev_action]\n",
    "        \n",
    "        q_max = np.max(q_value_exsar[new_state,:])\n",
    "        pi = np.ones(num_actions)*epsilon_exsar/num_actions \\\n",
    "            + (q_value_exsar[new_state,:] == q_max)*(1-epsilon_exsar)/np.sum(q_value_exsar[new_state,:] == q_max)\n",
    "        \n",
    "        expected_q = np.sum(q_value_exsar[new_state,:]*pi)\n",
    "        temporal_difference = reward + disc_fac_exsar * expected_q - old_q_value\n",
    "        new_q_value = old_q_value +  lr_exsar*temporal_difference\n",
    "        q_value_exsar[old_state,prev_action] = new_q_value\n",
    "\n",
    "        old_state = new_state\n",
    "        prev_action = chosen_action_idx\n",
    "    old_q_value = q_value_exsar[old_state,prev_action]\n",
    "    new_q_value = old_q_value + lr_exsar * (reward - old_q_value)\n",
    "    q_value_exsar[old_state,prev_action] = new_q_value\n",
    "print(q_value_exsar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_state, new_dicto, new_tree = RL_get_final_state_bgp_tree(q_value_exsar,actions,bgp_ind,symbol)\n",
    "print(\"best_state \\n\",best_state)\n",
    "print(\"new_dicto \\n\",new_dicto)\n",
    "print(\"new_tree \\n\",new_tree)\n",
    "print(\"old_tree \\n\", x_rl_tree_ind)\n",
    "print(\"\\n\")\n",
    "print(\"VALORES CON VIEJO ARBOL\")\n",
    "rl_stats_old = getpredictions_info_nojc(x_rl_tree_ind, x_rl_query_ind, y_rl_ind)\n",
    "print(\"valor predict: \",rl_stats_old['pred'][0][0])\n",
    "print(\"valor real: \",int(rl_stats_old['real'][0][0]))\n",
    "\n",
    "print(\"VALORES CON NUEVO ARBOL\")\n",
    "rl_stats_new = getpredictions_info_nojc(new_tree, x_rl_query_ind, y_rl_ind)\n",
    "print(\"valor predict: \",rl_stats_new['pred'][0][0])\n",
    "print(\"valor real: \",int(rl_stats_new['real'][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL-Neo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_value_neo = RL_First_Policy(actions)-99999 ##INITIAL POLICITY\n",
    "epsilon_neo = 0.55 ## EPSILON\n",
    "epoch_neo = 1000 ### TOTAL EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loop 1 Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,epoch_neo):\n",
    "    current_state = []\n",
    "    reward = 0\n",
    "    terminal = False\n",
    "    current_state, reward, terminal, chosen_action, chosen_action_idx = RLNeo_Next_step(actions,\n",
    "                                                                                    current_state,\n",
    "                                                                                    reward,\n",
    "                                                                                    q_value_neo,\n",
    "                                                                                    x_rl_tree_ind,\n",
    "                                                                                    x_rl_query_ind,\n",
    "                                                                                    y_rl_ind,\n",
    "                                                                                    bgp_ind,\n",
    "                                                                                    epsilon_neo)    \n",
    "    old_state = len(current_state)-1\n",
    "    while not terminal:\n",
    "        current_state, reward, terminal, chosen_action, chosen_action_idx = RLNeo_Next_step(actions,\n",
    "                                                                                        current_state,\n",
    "                                                                                        reward,\n",
    "                                                                                        q_value_neo,\n",
    "                                                                                        x_rl_tree_ind,\n",
    "                                                                                        x_rl_query_ind,\n",
    "                                                                                        y_rl_ind,\n",
    "                                                                                        bgp_ind,\n",
    "                                                                                        epsilon_neo)\n",
    "    q_value_neo = RLNeo_q_values(q_value_neo,current_state, reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_state, new_dicto, new_tree = RL_get_final_state_bgp_tree(q_value_neo,actions,bgp_ind,symbol)\n",
    "print(\"best_state \\n\",best_state)\n",
    "print(\"new_dicto \\n\",new_dicto)\n",
    "print(\"new_tree \\n\",new_tree)\n",
    "print(\"old_tree \\n\", x_rl_tree_ind)\n",
    "print(\"\\n\")\n",
    "print(\"VALORES CON VIEJO ARBOL\")\n",
    "rl_stats_old = getpredictions_info_nojc(x_rl_tree_ind, x_rl_query_ind, y_rl_ind)\n",
    "print(\"valor predict: \",rl_stats_old['pred'][0][0])\n",
    "print(\"valor real: \",int(rl_stats_old['real'][0][0]))\n",
    "\n",
    "print(\"VALORES CON NUEVO ARBOL\")\n",
    "rl_stats_new = getpredictions_info_nojc(new_tree, x_rl_query_ind, y_rl_ind)\n",
    "print(\"valor predict: \",rl_stats_new['pred'][0][0])\n",
    "print(\"valor real: \",int(rl_stats_new['real'][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTI-VAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Seleccionar data con mas de 1 o más acciones y parámetros generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_num_actions = -1\n",
    "buenos_idx = []\n",
    "for i in range(0,x_rl_tree.shape[0]):\n",
    "    x_rl_tree_ind, x_rl_query_ind, y_rl_ind, bgp_ind = RL_get_data(x_rl_tree, x_rl_query, y_rl, bgps_rl,i)\n",
    "    actions = RL_Actions(bgp_ind)\n",
    "    if len(actions) >= min_num_actions:\n",
    "        buenos_idx.append(i)\n",
    "total_buenos_idx = len(buenos_idx)\n",
    "print(\"total_buenos_idx\",total_buenos_idx)\n",
    "print(\"buenos_idx\")\n",
    "print(buenos_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q - Learning - multival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_ql = 0.75 ## EPSILON\n",
    "lr_ql = 0.9 ## LEARNING RATE\n",
    "disc_fac_ql = 0.9 ## DISCOUNT FACTOR\n",
    "epoch_ql = 100 ### TOTAL EPOCH POR DATA\n",
    "rl_columns = [\n",
    "            'index',\n",
    "            'iteration',\n",
    "            'pred_old',\n",
    "            'pred_new',\n",
    "            'real_old',\n",
    "            'real_new',\n",
    "            'mse_old',\n",
    "            'mse_new',\n",
    "            'mae_old',\n",
    "            'mae_new',\n",
    "            'rmse_old',\n",
    "            'rmse_new',\n",
    "            'history_old',\n",
    "            'history_new'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_ql = []\n",
    "values_final_ql = []\n",
    "c_ql = 1\n",
    "print(f'Start: {str(datetime.datetime.now())}')\n",
    "for idx in buenos_idx:\n",
    "    x_rl_tree_ind, x_rl_query_ind, y_rl_ind, bgp_ind = RL_get_data(x_rl_tree, x_rl_query, y_rl, bgps_rl,idx)\n",
    "    actions = RL_Actions(bgp_ind)\n",
    "    num_actions = len(actions)\n",
    "    q_value_ql = RL_First_Policy(actions) ##INITIAL POLICITY\n",
    "    if c_ql % 10 == 0:\n",
    "        print(f'{str(datetime.datetime.now())} : {c_ql}/{total_buenos_idx}, index {idx} evaluating')\n",
    "    ####################################Q LEARNING###############################################\n",
    "    for i in range(0,epoch_ql):\n",
    "        current_state = []\n",
    "        reward = 0\n",
    "        terminal = False\n",
    "        current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,   \n",
    "                                                                                        current_state,\n",
    "                                                                                        reward,\n",
    "                                                                                        q_value_ql,\n",
    "                                                                                        x_rl_tree_ind,\n",
    "                                                                                        x_rl_query_ind,\n",
    "                                                                                        y_rl_ind,\n",
    "                                                                                        bgp_ind,\n",
    "                                                                                        epsilon_ql)\n",
    "        old_state = len(current_state)-1\n",
    "        prev_action = chosen_action_idx\n",
    "        while not terminal:\n",
    "            current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,\n",
    "                                                                                            current_state,\n",
    "                                                                                            reward,\n",
    "                                                                                            q_value_ql,\n",
    "                                                                                            x_rl_tree_ind,\n",
    "                                                                                            x_rl_query_ind,\n",
    "                                                                                            y_rl_ind,\n",
    "                                                                                            bgp_ind,\n",
    "                                                                                            epsilon_ql)\n",
    "\n",
    "            new_state = len(current_state)-1\n",
    "            if not terminal:\n",
    "                old_q_value = q_value_ql[old_state,prev_action]\n",
    "                temporal_difference = reward + disc_fac_ql * np.max(q_value_ql[new_state,:]) - old_q_value\n",
    "                new_q_value = old_q_value + lr_ql * temporal_difference\n",
    "                q_value_ql[old_state,prev_action] = new_q_value\n",
    "            old_state = new_state\n",
    "            prev_action = chosen_action_idx\n",
    "        old_q_value = q_value_ql[old_state,prev_action]\n",
    "        new_q_value = old_q_value + lr_ql * (reward - old_q_value)\n",
    "        q_value_ql[old_state,prev_action] = new_q_value\n",
    "        ################################################################################################\n",
    "        best_state, new_dicto, new_tree = RL_get_final_state_bgp_tree(q_value_ql,actions,bgp_ind,symbol)\n",
    "        rl_stats_old = getpredictions_info_nojc(x_rl_tree_ind, x_rl_query_ind, y_rl_ind)\n",
    "        rl_stats_new = getpredictions_info_nojc(new_tree, x_rl_query_ind, y_rl_ind)\n",
    "        row = [\n",
    "                    idx,\n",
    "                    i,\n",
    "                    rl_stats_old['pred'][0][0],\n",
    "                    rl_stats_new['pred'][0][0],\n",
    "                    float(rl_stats_old['real'][0][0]),\n",
    "                    float(rl_stats_new['real'][0][0]), \n",
    "                    rl_stats_old['mse'],\n",
    "                    rl_stats_new['mse'],\n",
    "                    rl_stats_old['mae'],\n",
    "                    rl_stats_new['mae'],\n",
    "                    rl_stats_old['rmse'],\n",
    "                    rl_stats_new['rmse'],\n",
    "                    rl_stats_old['history'],\n",
    "                    rl_stats_new['history'],\n",
    "           ]\n",
    "\n",
    "        values_ql.append(row)\n",
    "    values_final_ql.append(row)\n",
    "    c_ql += 1\n",
    "print(\"len(values_ql[0])\", len(values_ql[0]))\n",
    "print(\"len(values_ql)\", len(values_ql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql_df = pd.DataFrame(values_ql,columns=rl_columns)\n",
    "ql_df_no_history = ql_df[['index','iteration','pred_old','pred_new','real_old','real_new','mse_old','mse_new','mae_old','mae_new','rmse_old','rmse_new']]\n",
    "ql_df_no_history.to_csv(URL + 'rl_csvs/ql_df_no_history.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql_df_final = pd.DataFrame(values_final_ql,columns=rl_columns)\n",
    "ql_df_final.to_csv(URL + 'rl_csvs/ql_df_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL-Neo Multival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_neo = 0.75 ## EPSILON\n",
    "epoch_neo = 100 ### TOTAL EPOCH POR DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_neo = []\n",
    "values_final_neo = []\n",
    "c_neo = 1\n",
    "print(f'Start: {str(datetime.datetime.now())}')\n",
    "for idx in buenos_idx:\n",
    "    x_rl_tree_ind, x_rl_query_ind, y_rl_ind, bgp_ind = RL_get_data(x_rl_tree, x_rl_query, y_rl, bgps_rl,idx)\n",
    "    actions = RL_Actions(bgp_ind)\n",
    "    num_actions = len(actions)\n",
    "    q_value_neo = RL_First_Policy(actions) ##INITIAL POLICITY\n",
    "    if c_neo % 10 == 0:\n",
    "        print(f'{str(datetime.datetime.now())} : {c_neo}/{total_buenos_idx}, index {idx} evaluating')\n",
    "    ############# ALGORITMO NEO #####################3\n",
    "    for i in range(0,epoch_neo):\n",
    "        current_state = []\n",
    "        reward = 0\n",
    "        terminal = False\n",
    "        current_state, reward, terminal, chosen_action, chosen_action_idx = RLNeo_Next_step(actions,\n",
    "                                                                                        current_state,\n",
    "                                                                                        reward,\n",
    "                                                                                        q_value_neo,\n",
    "                                                                                        x_rl_tree_ind,\n",
    "                                                                                        x_rl_query_ind,\n",
    "                                                                                        y_rl_ind,\n",
    "                                                                                        bgp_ind,\n",
    "                                                                                        epsilon_neo)    \n",
    "        old_state = len(current_state)-1\n",
    "        while not terminal:\n",
    "            current_state, reward, terminal, chosen_action, chosen_action_idx = RLNeo_Next_step(actions,\n",
    "                                                                                            current_state,\n",
    "                                                                                            reward,\n",
    "                                                                                            q_value_neo,\n",
    "                                                                                            x_rl_tree_ind,\n",
    "                                                                                            x_rl_query_ind,\n",
    "                                                                                            y_rl_ind,\n",
    "                                                                                            bgp_ind,\n",
    "                                                                                            epsilon_neo)\n",
    "        q_value_neo = RLNeo_q_values(q_value_neo,current_state, reward)\n",
    "        ##############################################################\n",
    "        best_state, new_dicto, new_tree = RL_get_final_state_bgp_tree(q_value_neo,actions,bgp_ind,symbol)\n",
    "        rl_stats_old = getpredictions_info_nojc(x_rl_tree_ind, x_rl_query_ind, y_rl_ind)\n",
    "        row = [\n",
    "                    idx,\n",
    "                    i,\n",
    "                    rl_stats_old['pred'][0][0],\n",
    "                    rl_stats_new['pred'][0][0],\n",
    "                    float(rl_stats_old['real'][0][0]),\n",
    "                    float(rl_stats_new['real'][0][0]), \n",
    "                    rl_stats_old['mse'],\n",
    "                    rl_stats_new['mse'],\n",
    "                    rl_stats_old['mae'],\n",
    "                    rl_stats_new['mae'],\n",
    "                    rl_stats_old['rmse'],\n",
    "                    rl_stats_new['rmse'],\n",
    "                    rl_stats_old['history'],\n",
    "                    rl_stats_new['history'],\n",
    "           ]\n",
    "\n",
    "        values_neo.append(row)\n",
    "    values_final_neo.append(row)\n",
    "    c_neo += 1\n",
    "#print(\"values_neo \", values_neo)\n",
    "print(\"len(values_neo[0])\",len(values_neo[0]) )\n",
    "print(\"len(values_neo)\", len(values_neo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo_df = pd.DataFrame(values_neo,columns=rl_columns)\n",
    "neo_df_no_history = neo_df[['index','iteration','pred_old','pred_new','real_old','real_new','mse_old','mse_new','mae_old','mae_new','rmse_old','rmse_new']]\n",
    "neo_df_no_history.to_csv(URL + 'rl_csvs/neo_df_no_history.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo_df_final = pd.DataFrame(values_final_neo,columns=rl_columns)\n",
    "neo_df_final.to_csv(URL + 'rl_csvs/neo_df_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa Multival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_value_sar = RL_First_Policy(actions) ##INITIAL POLICITY\n",
    "epsilon_sar = 0.75 ## EPSILON\n",
    "lr_sar = 0.9 ## LEARNING RATE\n",
    "disc_fac_sar = 0.9 ## DISCOUNT FACTOR\n",
    "epoch_sar = 100 ### TOTAL EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_sar = []\n",
    "values_final_sar = []\n",
    "c_sar = 1\n",
    "print(f'Start: {str(datetime.datetime.now())}')\n",
    "for idx in buenos_idx:\n",
    "    x_rl_tree_ind, x_rl_query_ind, y_rl_ind, bgp_ind = RL_get_data(x_rl_tree, x_rl_query, y_rl, bgps_rl,idx)\n",
    "    actions = RL_Actions(bgp_ind)\n",
    "    num_actions = len(actions)\n",
    "    q_value_sar = RL_First_Policy(actions) ##INITIAL POLICITY\n",
    "    if c_sar % 10 == 0:\n",
    "        print(f'{str(datetime.datetime.now())} : {c_sar}/{total_buenos_idx}, index {idx} evaluating')\n",
    "    ############################ALGORITMO SARSA##########################################\n",
    "    for i in range(0,epoch_sar):\n",
    "        current_state = []\n",
    "        reward = 0\n",
    "        terminal = False\n",
    "        current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,\n",
    "                                                                                        current_state,\n",
    "                                                                                        reward,\n",
    "                                                                                        q_value_sar,\n",
    "                                                                                        x_rl_tree_ind,\n",
    "                                                                                        x_rl_query_ind,\n",
    "                                                                                        y_rl_ind,\n",
    "                                                                                        bgp_ind,\n",
    "                                                                                        epsilon_sar)\n",
    "        old_state = len(current_state)-1\n",
    "        prev_action = chosen_action_idx\n",
    "        while not terminal:\n",
    "            current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,\n",
    "                                                                                            current_state,\n",
    "                                                                                            reward,\n",
    "                                                                                            q_value_sar,\n",
    "                                                                                            x_rl_tree_ind,\n",
    "                                                                                            x_rl_query_ind,\n",
    "                                                                                            y_rl_ind,\n",
    "                                                                                            bgp_ind,\n",
    "                                                                                            epsilon_sar)\n",
    "\n",
    "            new_state = len(current_state)-1\n",
    "            if not terminal:\n",
    "                old_q_value = q_value_sar[old_state,prev_action]\n",
    "                temporal_difference = reward + (disc_fac_sar * q_value_sar[new_state,chosen_action_idx]) - old_q_value\n",
    "                new_q_value = old_q_value + lr_sar * temporal_difference\n",
    "                q_value_sar[old_state,prev_action] = new_q_value\n",
    "            old_state = new_state\n",
    "            prev_action = chosen_action_idx\n",
    "        old_q_value = q_value_sar[old_state,prev_action]\n",
    "        new_q_value = old_q_value + lr_sar * (reward - old_q_value)\n",
    "        q_value_sar[old_state,prev_action] = new_q_value\n",
    "        ###############################################################################################33\n",
    "        best_state, new_dicto, new_tree = RL_get_final_state_bgp_tree(q_value_sar,actions,bgp_ind,symbol)\n",
    "        rl_stats_old = getpredictions_info_nojc(x_rl_tree_ind, x_rl_query_ind, y_rl_ind)\n",
    "        rl_stats_new = getpredictions_info_nojc(new_tree, x_rl_query_ind, y_rl_ind)\n",
    "        row = [\n",
    "                    idx,\n",
    "                    i,\n",
    "                    rl_stats_old['pred'][0][0],\n",
    "                    rl_stats_new['pred'][0][0],\n",
    "                    float(rl_stats_old['real'][0][0]),\n",
    "                    float(rl_stats_new['real'][0][0]), \n",
    "                    rl_stats_old['mse'],\n",
    "                    rl_stats_new['mse'],\n",
    "                    rl_stats_old['mae'],\n",
    "                    rl_stats_new['mae'],\n",
    "                    rl_stats_old['rmse'],\n",
    "                    rl_stats_new['rmse'],\n",
    "                    rl_stats_old['history'],\n",
    "                    rl_stats_new['history'],\n",
    "           ]\n",
    "\n",
    "        values_sar.append(row)\n",
    "    values_final_sar.append(row)\n",
    "    c_sar += 1\n",
    "#print(\"values_sar \", values_sar)\n",
    "print(\"len(values_sar[0])\",len(values_sar[0]) )\n",
    "print(\"len(values_sar)\", len(values_sar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_df = pd.DataFrame(values_sarsa,columns=rl_columns)\n",
    "sarsa_df_no_history = sarsa_df[['index','iteration','pred_old','pred_new','real_old','real_new','mse_old','mse_new','mae_old','mae_new','rmse_old','rmse_new']]\n",
    "sarsa_df_no_history.to_csv(URL + 'rl_csvs/sarsa_df_no_history.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_df_final = pd.DataFrame(values_final_sarsa,columns=rl_columns)\n",
    "sarsa_df_final.to_csv(URL + 'rl_csvs/sarsa_df_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected-Sarsa Multival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q_value_exsar = RL_First_Policy(actions) ##INITIAL POLICITY\n",
    "epsilon_exsar = 0.75 ## EPSILON\n",
    "lr_exsar = 0.9 ## LEARNING RATE\n",
    "disc_fac_exsar = 0.9 ## DISCOUNT FACTOR\n",
    "epoch_exsar = 100 ### TOTAL EPOCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_exsar = []\n",
    "values_final_exsar = []\n",
    "c_exsar = 1\n",
    "print(f'Start: {str(datetime.datetime.now())}')\n",
    "for idx in buenos_idx:\n",
    "    x_rl_tree_ind, x_rl_query_ind, y_rl_ind, bgp_ind = RL_get_data(x_rl_tree, x_rl_query, y_rl, bgps_rl,idx)\n",
    "    actions = RL_Actions(bgp_ind)\n",
    "    num_actions = len(actions)\n",
    "    q_value_exsar = RL_First_Policy(actions) ##INITIAL POLICITY\n",
    "    if c_exsar % 10 == 0:\n",
    "        print(f'{str(datetime.datetime.now())} : {c_exsar}/{total_buenos_idx}, index {idx} evaluating')\n",
    "        \n",
    "    ##### ALGORITMO EXPECTED SARSA\n",
    "    for i in range(0,epoch_exsar):\n",
    "        current_state = []\n",
    "        reward = 0\n",
    "        terminal = False\n",
    "        current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,\n",
    "                                                                                        current_state,\n",
    "                                                                                        reward,\n",
    "                                                                                        q_value_exsar,\n",
    "                                                                                        x_rl_tree_ind,\n",
    "                                                                                        x_rl_query_ind,\n",
    "                                                                                        y_rl_ind,\n",
    "                                                                                        bgp_ind,\n",
    "                                                                                        epsilon_exsar)    \n",
    "        old_state = len(current_state)-1\n",
    "        prev_action = chosen_action_idx\n",
    "        while not terminal:\n",
    "            current_state, reward, terminal, chosen_action, chosen_action_idx = RL_Next_step(actions,\n",
    "                                                                                            current_state,\n",
    "                                                                                            reward,\n",
    "                                                                                            q_value_exsar,\n",
    "                                                                                            x_rl_tree_ind,\n",
    "                                                                                            x_rl_query_ind,\n",
    "                                                                                            y_rl_ind,\n",
    "                                                                                            bgp_ind,\n",
    "                                                                                            epsilon_exsar)\n",
    "            new_state = len(current_state)-1\n",
    "            array_state = np.array(current_state)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            expected_q = 0\n",
    "            old_q_value = q_value_exsar[old_state,prev_action]\n",
    "\n",
    "            q_max = np.max(q_value_exsar[new_state,:])\n",
    "            pi = np.ones(num_actions)*epsilon_exsar/num_actions \\\n",
    "                + (q_value_exsar[new_state,:] == q_max)*(1-epsilon_exsar)/np.sum(q_value_exsar[new_state,:] == q_max)\n",
    "    \n",
    "            expected_q = np.sum(q_value_exsar[new_state,:]*pi)\n",
    "            temporal_difference = reward + disc_fac_exsar * expected_q - old_q_value\n",
    "            new_q_value = old_q_value +  lr_exsar*temporal_difference\n",
    "            q_value_exsar[old_state,prev_action] = new_q_value\n",
    "            old_state = new_state\n",
    "            prev_action = chosen_action_idx\n",
    "        #########################################################\n",
    "        old_q_value = q_value_exsar[old_state,prev_action]\n",
    "        new_q_value = old_q_value + lr_exsar * (reward - old_q_value)\n",
    "        q_value_exsar[old_state,prev_action] = new_q_value\n",
    "\n",
    "        best_state, new_dicto, new_tree = RL_get_final_state_bgp_tree(q_value_exsar,actions,bgp_ind,symbol)\n",
    "        rl_stats_old = getpredictions_info_nojc(x_rl_tree_ind, x_rl_query_ind, y_rl_ind)\n",
    "        rl_stats_new = getpredictions_info_nojc(new_tree, x_rl_query_ind, y_rl_ind)\n",
    "        row = [\n",
    "                    idx,\n",
    "                    i,\n",
    "                    rl_stats_old['pred'][0][0],\n",
    "                    rl_stats_new['pred'][0][0],\n",
    "                    float(rl_stats_old['real'][0][0]),\n",
    "                    float(rl_stats_new['real'][0][0]), \n",
    "                    rl_stats_old['mse'],\n",
    "                    rl_stats_new['mse'],\n",
    "                    rl_stats_old['mae'],\n",
    "                    rl_stats_new['mae'],\n",
    "                    rl_stats_old['rmse'],\n",
    "                    rl_stats_new['rmse'],\n",
    "                    rl_stats_old['history'],\n",
    "                    rl_stats_new['history'],\n",
    "           ]\n",
    "\n",
    "        values_exsar.append(row)\n",
    "    values_final_exsar.append(row)\n",
    "    c_exsar += 1\n",
    "#print(\"values_exsar \", values_exsar)\n",
    "print(\"len(values_exsar[0])\",len(values_exsar[0]) )\n",
    "print(\"len(values_exsar)\", len(values_exsar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exsarsa_df = pd.DataFrame(values_exsarsa,columns=rl_columns)\n",
    "exsarsa_df_no_history = exsarsa_df[['index','iteration','pred_old','pred_new','real_old','real_new','mse_old','mse_new','mae_old','mae_new','rmse_old','rmse_new']]\n",
    "exsarsa_df_no_history.to_csv(URL + 'rl_csvs/exsarsa_df_no_history.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exsarsa_df_final = pd.DataFrame(values_final_exsarsa,columns=rl_columns)\n",
    "exsarsa_df_final.to_csv(URL + 'rl_csvs/exsarsa_df_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archivos enteros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ql_df.to_csv(URL + 'rl_csvs/ql_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_df.to_csv(URL + 'rl_csvs/neo_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo_df.to_csv(URL + 'rl_csvs/neo_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exsarsa_df.to_csv(URL + 'rl_csvs/neo_df.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
