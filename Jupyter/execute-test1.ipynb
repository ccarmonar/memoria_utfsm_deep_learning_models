{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.preprocessing import StandardScaler#para escalar caracteristicas\n",
    "from sklearn.model_selection import train_test_split # separar mas facil la data de train y test\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from ast import literal_eval\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "from scipy.signal import convolve2d\n",
    "from functions.auxiliares import PaddingSameSize, MatrixFormat_To_Vector, RemoveOversizedMatrix, StandardSize_Padding, CNN_Features_Format, batch_format, KernelSize, StandardSize_InitialConvolution, StandardSize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Esto garantiza que se ejecutara en GPU si esta disponible\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_r = 30 ## Maximo numero de subconsultas\n",
    "max_c = 40 ## Maximo numero de subcarateristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unique_id', 'filename', 'sparql_file', 'profile', 'limit', 'msec', 'cpu_p', 'rnd', 'seq', 'same_seg_p', 'same_page_p', 'disk_reads', 'read_ahead', 'wait', 'comp_msec', 'comp_reads', 'comp_read_p', 'comp_messages', 'comp_clw', 'ql_id', 'ql_start_dt', 'ql_rt_msec', 'ql_rt_clocks', 'ql_client_ip', 'ql_user', 'ql_sqlstate', 'ql_error', 'ql_swap', 'ql_user_cpu', 'ql_sys_cpu', 'ql_params', 'ql_plan_hash', 'ql_c_clocks', 'ql_c_msec', 'ql_c_disk_reads', 'ql_c_disk_wait', 'ql_c_cl_wait', 'ql_cl_messages', 'ql_c_rnd_rows', 'ql_rnd_rows', 'ql_seq_rows', 'ql_same_seg', 'ql_same_page', 'ql_same_parent', 'ql_thread_clocks', 'ql_disk_wait_clocks', 'ql_cl_wait_clocks', 'ql_pg_wait_clocks', 'ql_disk_reads', 'ql_spec_disk_reads', 'ql_messages', 'ql_message_bytes', 'ql_qp_threads', 'ql_memory', 'ql_memory_max', 'ql_lock_waits', 'ql_lock_wait_msec', 'ql_node_stat', 'ql_c_memory', 'ql_rows_affected', 'matrix_format', 'binary_tree']\n",
      "shape df_raw: (29155, 62)\n",
      "shape df_clean: (17099, 62)\n",
      "shape new_df_clean: (17099, 62)\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv('D:/.Memoria/modelo/memoria_utfsm_deep_learning_models/Jupyter/dataset/test_example2.csv')\n",
    "columns = list(df_raw.columns)\n",
    "print(columns)\n",
    "#  Eliminar los con tiempos de ejecución 0 o muy altos\n",
    "df_clean = shuffle(df_raw[(df_raw['msec'] > 0) & (df_raw['msec'] < 1e10)])\n",
    "df_clean.to_csv('D:/.Memoria/Test/Jupyter/test_example_clean.csv',index=False)\n",
    "\n",
    "### PASAR DE MSEC A SEGUNDOS\n",
    "df_clean['msec'] = df_clean['msec'].apply(lambda x: x*0.001)\n",
    "\n",
    "### HAY QUE CONVERTIR ALGUNAS COLUMNAS Q ESTAN EN PORCENTAJE A FLOAT\n",
    "#df_clean['cpu_p'] = df_clean['cpu_p'].apply(lambda x: float(x.strip('%'))/100)\n",
    "#df_clean['same_seg_p'] = df_clean['same_seg_p'].apply(lambda x: float(x.strip('%'))/100)\n",
    "#df_clean['same_page_p'] = df_clean['same_page_p'].apply(lambda x: float(x.strip('%'))/100)\n",
    "#df_clean['wait'] = df_clean['wait'].apply(lambda x: float(x.strip('%'))/100)\n",
    "#df_clean['comp_read_p'] = df_clean['comp_read_p'].apply(lambda x: float(x.strip('%'))/100)\n",
    "#df_clean['comp_clw'] = df_clean['comp_clw'].apply(lambda x: float(x.strip('%'))/100)\n",
    "\n",
    "##SIN PASAR A FLOAT PERO QUITANDO EL %\n",
    "df_clean['cpu_p'] = df_clean['cpu_p'].apply(lambda x: float(x.strip('%')))\n",
    "df_clean['same_seg_p'] = df_clean['same_seg_p'].apply(lambda x: float(x.strip('%')))\n",
    "df_clean['same_page_p'] = df_clean['same_page_p'].apply(lambda x: float(x.strip('%')))\n",
    "df_clean['wait'] = df_clean['wait'].apply(lambda x: float(x.strip('%')))\n",
    "df_clean['comp_read_p'] = df_clean['comp_read_p'].apply(lambda x: float(x.strip('%')))\n",
    "df_clean['comp_clw'] = df_clean['comp_clw'].apply(lambda x: float(x.strip('%')))\n",
    "\n",
    "\n",
    "# Convertir MatrixFormat de STR a np.array\n",
    "df_clean['matrix_format'] = df_clean['matrix_format'].apply(lambda x: np.asarray(literal_eval(x)).astype(np.float32)) \n",
    "\n",
    "\n",
    "## shape de raw y clean\n",
    "print(f'shape df_raw: {df_raw.shape}')\n",
    "print(f'shape df_clean: {df_clean.shape}')\n",
    "\n",
    "\n",
    "#new_df_clean,max_new_r,max_new_c,min_new_r,min_new_c,mean_new_r,mean_new_c = RemoveOversizedMatrix(df_clean,max_r,max_c)\n",
    "new_df_clean = df_clean\n",
    "\n",
    "#num_standard_rows = max_new_r\n",
    "#num_standard_columns = max_new_c\n",
    "num_standard_rows = 20\n",
    "num_standard_columns = 32\n",
    "\n",
    "#new_df_clean['matrix_format'] = df_clean['matrix_format'].apply(lambda x: StandardSize_Padding(x,num_standard_rows,num_standard_columns)) \n",
    "new_df_clean['matrix_format'] = df_clean['matrix_format'].apply(lambda x: StandardSize(x,num_standard_rows,num_standard_columns)) \n",
    "new_df_clean['matrix_format'] = new_df_clean['matrix_format'].apply(lambda x: x.astype(np.float32)) \n",
    "\n",
    "\n",
    "print(f'shape new_df_clean: {new_df_clean.shape}')\n",
    "\n",
    "msk = np.random.rand(len(new_df_clean)) <= 0.8\n",
    "df_train = new_df_clean[msk]\n",
    "df_test = new_df_clean[~msk]\n",
    "\n",
    "df_train.to_csv('D:/.Memoria/Test/Jupyter/test_example_clean_train.csv',index=False)\n",
    "df_test.to_csv('D:/.Memoria/Test/Jupyter/test_example_clean_test.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "shape X_numpy_train: (13767, 1)\n",
      "shape X_numpy_test: (3332, 1)\n",
      "shape y_numpy_train: (13767,)\n",
      "shape y_numpy_test: (3332,)\n",
      "-----------------------\n",
      "shape X_train: torch.Size([13767, 1, 20, 32])\n",
      "shape X_test: torch.Size([3332, 1, 20, 32])\n",
      "shape y_train: torch.Size([13767, 1])\n",
      "shape y_test: torch.Size([3332, 1])\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    'matrix_format'\n",
    "] \n",
    "\n",
    "## FEATURES - TRAIN Y TEST\n",
    "X_df_train = df_train[features]\n",
    "X_df_test = df_test[features]\n",
    "X_numpy_train = X_df_train.to_numpy()\n",
    "X_numpy_test = X_df_test.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "## TARGETS - TRAIN Y TEST\n",
    "y_df_train = df_train['msec']\n",
    "y_df_test = df_test['msec']\n",
    "\n",
    "y_numpy_train = y_df_train.to_numpy().astype(np.float32)\n",
    "y_numpy_test = y_df_test.to_numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "print(\"-----------------------\")\n",
    "print(f'shape X_numpy_train: {X_numpy_train.shape}')\n",
    "print(f'shape X_numpy_test: {X_numpy_test.shape}')\n",
    "print(f'shape y_numpy_train: {y_numpy_train.shape}')\n",
    "print(f'shape y_numpy_test: {y_numpy_test.shape}')\n",
    "print(\"-----------------------\")\n",
    "\n",
    "\n",
    "# Pasarlos a Torch. \n",
    "#X_train = torch.from_numpy(X_numpy_train)\n",
    "#X_test = torch.from_numpy(X_numpy_test)\n",
    "\n",
    "#X_train = torch.tensor(df_train['matrix_format'].values)\n",
    "#X_test = torch.tensor(df_test['matrix_format'].values)\n",
    "\n",
    "\n",
    "X_train = CNN_Features_Format(X_numpy_train)\n",
    "X_test = CNN_Features_Format(X_numpy_test)\n",
    "\n",
    "y_train = torch.from_numpy(y_numpy_train)\n",
    "y_test = torch.from_numpy(y_numpy_test)\n",
    "\n",
    "\n",
    "# También pasar los targets de vector fila a vector columna\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "\n",
    "print(f'shape X_train: {X_train.shape}')\n",
    "print(f'shape X_test: {X_test.shape}')\n",
    "print(f'shape y_train: {y_train.shape}')\n",
    "print(f'shape y_test: {y_test.shape}')\n",
    "print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0020],\n",
       "        [0.0040],\n",
       "        [0.0010],\n",
       "        ...,\n",
       "        [0.0020],\n",
       "        [0.0430],\n",
       "        [0.0570]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 1, 20, 32])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([267, 1, 20, 32])\n",
      "torch.Size([267, 1])\n",
      "torch.Size([13767, 1, 20, 32])\n",
      "torch.Size([13767, 1])\n"
     ]
    }
   ],
   "source": [
    "#n_samples_train, n_features_train = X_train.shape\n",
    "#n_samples_test, n_features_test = X_test.shape\n",
    "#\n",
    "#hidden_size = 100000\n",
    "#input_size = n_features_train \n",
    "#print(f'n_samples_train: {n_samples_train}')\n",
    "#print(f'n_features_train: {n_features_train}')\n",
    "#print(f'n_samples_test: {n_samples_test}')\n",
    "#print(f'n_features_test: {n_features_test}')\n",
    "#print(f'learning_rate: {learning_rate}')\n",
    "#print(f'hidden_size: {hidden_size}')\n",
    "#print(f'input_size: {input_size}')\n",
    "learning_rate = 0.001\n",
    "batches_size = 500\n",
    "num_epochs = 5000\n",
    "num_batches = math.ceil(X_train.shape[0]/batches_size)\n",
    "batches = batch_format(X_train,y_train,batches_size)\n",
    "for i in range(num_batches):\n",
    "    print(batches[\"features\"][i].shape)\n",
    "    print(batches[\"labels\"][i].shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # SE DEFINEN LAS CAPAS A UTILIZAR, TANTO LAS CNN, FCL-ANN CY LAS DE POOLING\n",
    "        self.conv1 = nn.Conv2d(1, 6, (3,5),stride=(1,1)) \n",
    "        self.pool = nn.MaxPool2d((2,2), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(6, 12, (3,5),stride=(1,1)) \n",
    "        self.fc1 = nn.Linear(12*3*5, 162)\n",
    "        self.fc2 = nn.Linear(162, 84)\n",
    "        self.fc3 = nn.Linear(84, 42)\n",
    "        self.fc4 = nn.Linear(42, 20)\n",
    "        self.fc5 = nn.Linear(20, 1)\n",
    "        \n",
    "        #Se pueden agregar mas capas o mas neurones, cambiar tamaños etc.. pero siempre debo terminar con\n",
    "        # una salida del tamaño que busco\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"input \",x.shape)\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.01, inplace=False)\n",
    "        #print(\"conv1 \",x.shape)\n",
    "        x = self.pool(x)\n",
    "        #print(\"pooling \",x.shape)\n",
    "        \n",
    "        \n",
    "        #print(\"input \",x.shape)\n",
    "        x = F.leaky_relu(self.conv2(x), negative_slope=0.01, inplace=False)\n",
    "        #print(\"conv2 \",x.shape)\n",
    "        x = self.pool(x)\n",
    "        #print(\"pooling2 \",x.shape)\n",
    "        \n",
    "        \n",
    "        x = x.view(-1,12*3*5)           \n",
    "        #print(\"view: \",x.shape)\n",
    "        x = F.leaky_relu(self.fc1(x), negative_slope=0.01, inplace=False)\n",
    "        #print(\"fc1: \",x.shape)\n",
    "        x = F.leaky_relu(self.fc2(x), negative_slope=0.01, inplace=False)\n",
    "        #print(\"fc2: \",x.shape)\n",
    "        x = F.leaky_relu(self.fc3(x), negative_slope=0.01, inplace=False)\n",
    "        #print(\"fc3: \",x.shape)\n",
    "        x = F.leaky_relu(self.fc4(x), negative_slope=0.01, inplace=False)\n",
    "        #print(\"fc4: \",x.shape)\n",
    "        x =self.fc5(x)\n",
    "        #print(\"fc5: \",x.shape)\n",
    "        #print(\"---------------------------------------------------------------\")\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss y Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: [100/5000], loss = 0.07055002450942993\n",
      "epoch: [200/5000], loss = 0.10458239912986755\n",
      "epoch: [300/5000], loss = 0.09148646146059036\n",
      "epoch: [400/5000], loss = 0.32074078917503357\n",
      "epoch: [500/5000], loss = 5.8807597160339355\n",
      "epoch: [600/5000], loss = 0.1130882278084755\n",
      "epoch: [700/5000], loss = 0.37568745017051697\n",
      "epoch: [800/5000], loss = 0.27396291494369507\n",
      "epoch: [900/5000], loss = 0.037042517215013504\n",
      "epoch: [1000/5000], loss = 0.43837496638298035\n",
      "epoch: [1100/5000], loss = 0.18599535524845123\n",
      "epoch: [1200/5000], loss = 0.050574589520692825\n",
      "epoch: [1300/5000], loss = 0.04174162819981575\n",
      "epoch: [1400/5000], loss = 0.1337428241968155\n",
      "epoch: [1500/5000], loss = 0.0305082555860281\n",
      "epoch: [1600/5000], loss = 0.3281300663948059\n",
      "epoch: [1700/5000], loss = 1.4097442626953125\n",
      "epoch: [1800/5000], loss = 0.6369524598121643\n",
      "epoch: [1900/5000], loss = 0.09455065429210663\n",
      "epoch: [2000/5000], loss = 0.02452850714325905\n",
      "epoch: [2100/5000], loss = 0.2893696129322052\n",
      "epoch: [2200/5000], loss = 0.06663250178098679\n",
      "epoch: [2300/5000], loss = 0.030932676047086716\n",
      "epoch: [2400/5000], loss = 0.1335611194372177\n",
      "epoch: [2500/5000], loss = 0.16582763195037842\n",
      "epoch: [2600/5000], loss = 0.11733971536159515\n",
      "epoch: [2700/5000], loss = 0.04313943162560463\n",
      "epoch: [2800/5000], loss = 0.07776334881782532\n",
      "epoch: [2900/5000], loss = 0.022297143936157227\n",
      "epoch: [3000/5000], loss = 0.018989650532603264\n",
      "epoch: [3100/5000], loss = 0.0341680534183979\n",
      "epoch: [3200/5000], loss = 0.04785184934735298\n",
      "epoch: [3300/5000], loss = 0.10074686259031296\n",
      "epoch: [3400/5000], loss = 0.054612964391708374\n",
      "epoch: [3500/5000], loss = 0.04297950118780136\n",
      "epoch: [3600/5000], loss = 0.6747717261314392\n",
      "epoch: [3700/5000], loss = 1.2899260520935059\n",
      "epoch: [3800/5000], loss = 0.3388659656047821\n",
      "epoch: [3900/5000], loss = 0.2546762526035309\n",
      "epoch: [4000/5000], loss = 0.6781009435653687\n",
      "epoch: [4100/5000], loss = 0.05326911434531212\n",
      "epoch: [4200/5000], loss = 0.26670894026756287\n",
      "epoch: [4300/5000], loss = 0.41827815771102905\n",
      "epoch: [4400/5000], loss = 0.7102356553077698\n",
      "epoch: [4500/5000], loss = 0.1540796160697937\n",
      "epoch: [4600/5000], loss = 0.06054629758000374\n",
      "epoch: [4700/5000], loss = 0.03328399360179901\n",
      "epoch: [4800/5000], loss = 2.5481207370758057\n",
      "epoch: [4900/5000], loss = 0.17358656227588654\n",
      "epoch: [5000/5000], loss = 0.14202775061130524\n"
     ]
    }
   ],
   "source": [
    "#print(num_batches)\n",
    "for epoch in range(num_epochs):\n",
    "    for bt in range(num_batches):\n",
    "        y_hat = model(batches[\"features\"][bt])\n",
    "        loss = criterion(y_hat,batches[\"labels\"][bt])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if bt+1 == num_batches and (epoch+1) % 100 == 0:\n",
    "            print(f'epoch: [{epoch+1}/{num_epochs}], loss = {loss.item()}')\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3332, 1, 20, 32])\n",
      "torch.Size([3332, 1])\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "# BASICO\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(batches_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0253]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.0150]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batches_size_test = 1\n",
    "num_batches_test = math.ceil(X_test.shape[0]/batches_size_test)\n",
    "batches_test = batch_format(X_test,y_test,batches_size_test)\n",
    "#for i in range(num_batches_test):\n",
    "#    print(batches_test[\"features\"][i].shape)\n",
    "#    print(batches_test[\"labels\"][i].shape)\n",
    "#print(X_test.shape)\n",
    "#print(y_test.shape)\n",
    "\n",
    "y_pred = model(batches_test[\"features\"][0])\n",
    "y = batches_test[\"labels\"][0]\n",
    "print(y_pred)\n",
    "\n",
    "print(y)\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "achuntes: [3237/3332.0]\n",
      "accuracy: 97.14885954381752\n"
     ]
    }
   ],
   "source": [
    "tol = 1 # segundos de tolerancia\n",
    "num_aciertos = 0\n",
    "num_test = float(y_test.shape[0])\n",
    "with torch.no_grad():\n",
    "    for bt in range(num_batches_test):\n",
    "        #print(batches[\"features\"][bt].shape)\n",
    "        #print(batches[\"labels\"][bt].shape)\n",
    "        y_pred_tensor = model(batches_test[\"features\"][bt])\n",
    "        y_tensor = batches_test[\"labels\"][bt]\n",
    "        for i in range(y_tensor.shape[0]):\n",
    "            y_pred = y_pred_tensor[i].item()\n",
    "            y = y_tensor[i].item()\n",
    "            #print(y, y_pred)\n",
    "            if abs(y_pred-y) < tol:\n",
    "                num_aciertos += 1 \n",
    "            \n",
    "    accuracy = num_aciertos/num_test\n",
    "\n",
    "print(f'achuntes: [{num_aciertos}/{num_test}]')\n",
    "print(f'accuracy: {accuracy*100}')\n",
    "      \n",
    "      \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "achuntes: [3237/3332.0]\n",
      "accuracy: 97.14885954381752\n"
     ]
    }
   ],
   "source": [
    "print(f'achuntes: [{num_aciertos}/{num_test}]')\n",
    "print(f'accuracy: {accuracy*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
