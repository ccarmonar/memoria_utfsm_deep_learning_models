{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.preprocessing import StandardScaler#para escalar caracteristicas\n",
    "from sklearn.model_selection import train_test_split # separar mas facil la data de train y test\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PaddingSameSize(matrix,padd_type='constant',constant_value=0):\n",
    "    length = np.array([len(matrix[i]) for i in range(len(matrix))])\n",
    "    width = length.max()\n",
    "    return_list=[]\n",
    "    for i in range(len(matrix)):\n",
    "        if len(matrix[i]) != width:\n",
    "            if padd_type == 'constant':\n",
    "                padd = np.pad(matrix[i], (0,width-len(matrix[i])), 'constant',constant_values = 0)\n",
    "            else:\n",
    "                padd = np.pad(matrix[i], (0,width-len(matrix[i])), padd_type)\n",
    "        else:\n",
    "            padd = matrix[i]\n",
    "        return_list.append(padd)\n",
    "    return_list = np.array(return_list)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatrixFormat_To_Vector(df_matrix_format):\n",
    "    df_matrix_format = df_matrix_format.apply(lambda x: np.asarray(literal_eval(x)).astype(np.float32)) \n",
    "    max_x,max_y = {\"index\":0,\"value\":0},{\"index\":0,\"value\":0}\n",
    "    vector_list = []\n",
    "    vector_list_pad = []\n",
    "    max_len = 0\n",
    "    for i, matrix in df_matrix_format.items():\n",
    "    #    if max_x['value'] < matrix.shape[0]:\n",
    "    #        max_x['value'] = matrix.shape[0]\n",
    "    #        max_x['index'] = i\n",
    "    #    if max_y['value'] < matrix.shape[1]:\n",
    "    #        max_y['value'] = matrix.shape[1]\n",
    "    #        max_y['index'] = i\n",
    "        vector = np.reshape(matrix, -1)\n",
    "        vector_list.append(vector)\n",
    "\n",
    "    #for v in vector_list:\n",
    "    #    if max_len < len(v):\n",
    "    #        max_len = len(v)\n",
    "    #print(\"max len\", max_len)\n",
    "    vector_list_pad = PaddingSameSize(vector_list, 'constant')      \n",
    "    \n",
    "    #for i in vector_list_pad:\n",
    "    #    print(i.shape)\n",
    "    #    print(i)\n",
    "    \n",
    "    return vector_list_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveOversizedMatrix(df_clean,max_r=20,max_c=40):\n",
    "    arr_shape_r = []\n",
    "    arr_shape_c = []\n",
    "    print(\"Total df_clean: \",len(df_clean))\n",
    "    eliminados = []\n",
    "    for i, v in df_clean['matrix_format'].iteritems():\n",
    "        r,c = v.shape\n",
    "        if r > 20 or c > 40:\n",
    "            eliminados.append(i)\n",
    "    print(\"Eliminados por Oversized:\", len(eliminados))\n",
    "\n",
    "    new_df_clean = df_clean.drop(eliminados)\n",
    "\n",
    "    for i, v in new_df_clean['matrix_format'].iteritems():\n",
    "        r,c = v.shape\n",
    "        arr_shape_r.append(r)\n",
    "        arr_shape_c.append(c)\n",
    "        \n",
    "    max_new_r = max(arr_shape_r)\n",
    "    max_new_c = max(arr_shape_c)\n",
    "    min_new_r = min(arr_shape_r)\n",
    "    min_new_c = min(arr_shape_c)\n",
    "    mean_new_r = np.ceil(np.mean(arr_shape_r))\n",
    "    mean_new_c = np.ceil(np.mean(arr_shape_c))\n",
    "\n",
    "    return new_df_clean,max_new_r,max_new_c,min_new_r,min_new_c,mean_new_r,mean_new_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StandardSize_Padding(matrix,num_standard_rows,num_standard_columns):\n",
    "    padd_columns = []\n",
    "    for i in range(len(matrix)):\n",
    "        if len(matrix[i]) < num_standard_columns:\n",
    "            padd = np.pad(matrix[i], (0,num_standard_columns-len(matrix[i])), 'mean')\n",
    "            padd_columns.append(padd)\n",
    "        else:\n",
    "            padd = matrix[i]\n",
    "            padd_columns.append(padd)\n",
    "    padd_rows = []\n",
    "    tp_matrix = np.asarray(padd_columns, dtype=np.float32).T\n",
    "    for i in range(len(tp_matrix)):\n",
    "        if len(tp_matrix[i]) < num_standard_rows:\n",
    "            padd = np.pad(tp_matrix[i], (0,num_standard_rows-len(tp_matrix[i])), 'mean')\n",
    "            padd_rows.append(padd)\n",
    "        else:\n",
    "            padd = tp_matrix[i]\n",
    "            padd_rows.append(padd)\n",
    "    return np.asarray(padd_rows, dtype=np.float32).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Features_Format(numpy_array):\n",
    "    ### Pasarlos a 1 \"canal\"\n",
    "    canal = []\n",
    "    #print(\"shape X_numpy_train: \",numpy_array.shape)\n",
    "    #print(\"type X_numpy_train: \",type(numpy_array), numpy_array.dtype)\n",
    "    for i in numpy_array:\n",
    "        for j in i:\n",
    "            k = np.array([j])\n",
    "            canal.append(k)\n",
    "    canal = np.array(canal)\n",
    "    torch_canal = torch.from_numpy(canal)\n",
    "    return torch_canal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Esto garantiza que se ejecutara en GPU si esta disponible\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unique_id', 'filename', 'sparql_file', 'profile', 'limit', 'ql_rt_msec', 'ql_rt_clocks', 'ql_rnd_rows', 'ql_seq_rows', 'ql_same_seg', 'ql_same_page', 'ql_disk_reads', 'ql_spec_disk_reads', 'ql_cl_wait_clocks', 'ql_c_msec', 'ql_c_disk', 'ql_c_clocks', 'ql_cl_messages', 'ql_c_cl_wait', 'matrix_format', 'binary_tree']\n",
      "shape df_raw: (3711, 21)\n",
      "shape df_clean: (2253, 21)\n",
      "Total df_clean:  2253\n",
      "Eliminados por Oversized: 32\n",
      "shape new_df_clean: (2221, 21)\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv('D:/.Memoria/Test/Jupyter/test_example.csv')\n",
    "columns = list(df_raw.columns)\n",
    "print(columns)\n",
    "#  Eliminar los con tiempos de ejecución 0 o muy altos\n",
    "df_clean = shuffle(df_raw[(df_raw['ql_rt_msec'] > 0) & (df_raw['ql_rt_msec'] < 1e4)])\n",
    "df_clean.to_csv('D:/.Memoria/Test/Jupyter/test_example_clean.csv',index=False)\n",
    "\n",
    "### HAY QUE CONVERTIR ALGUNAS COLUMNAS Q ESTAN EN PORCENTAJE A FLOAT\n",
    "df_clean['ql_rt_clocks'] = df_clean['ql_rt_clocks'].apply(lambda x: float(x.strip('%'))/100)\n",
    "df_clean['ql_same_seg'] = df_clean['ql_same_seg'].apply(lambda x: float(x.strip('%'))/100)\n",
    "df_clean['ql_same_page'] = df_clean['ql_same_page'].apply(lambda x: float(x.strip('%'))/100)\n",
    "df_clean['ql_cl_wait_clocks'] = df_clean['ql_cl_wait_clocks'].apply(lambda x: float(x.strip('%'))/100)\n",
    "df_clean['ql_c_clocks'] = df_clean['ql_c_clocks'].apply(lambda x: float(x.strip('%'))/100)\n",
    "df_clean['ql_c_cl_wait'] = df_clean['ql_c_cl_wait'].apply(lambda x: float(x.strip('%'))/100)\n",
    "\n",
    "# Convertir MatrixFormat de STR a np.array\n",
    "df_clean['matrix_format'] = df_clean['matrix_format'].apply(lambda x: np.asarray(literal_eval(x)).astype(np.float32)) \n",
    "\n",
    "## shape de raw y clean\n",
    "print(f'shape df_raw: {df_raw.shape}')\n",
    "print(f'shape df_clean: {df_clean.shape}')\n",
    "\n",
    "\n",
    "max_r = 20 ## Maximo numero de subconsultas\n",
    "max_c = 40 ## Maximo numero de subcarateristicas\n",
    "new_df_clean,max_new_r,max_new_c,min_new_r,min_new_c,mean_new_r,mean_new_c = RemoveOversizedMatrix(df_clean,max_r,max_c)\n",
    "\n",
    "num_standard_rows = max_new_r\n",
    "num_standard_columns = max_new_c\n",
    "new_df_clean['matrix_format'] = df_clean['matrix_format'].apply(lambda x: StandardSize_Padding(x,num_standard_rows,num_standard_columns)) \n",
    "\n",
    "print(f'shape new_df_clean: {new_df_clean.shape}')\n",
    "\n",
    "msk = np.random.rand(len(new_df_clean)) <= 0.8\n",
    "df_train = new_df_clean[msk]\n",
    "df_test = new_df_clean[~msk]\n",
    "\n",
    "df_train.to_csv('D:/.Memoria/Test/Jupyter/test_example_clean_train.csv',index=False)\n",
    "df_test.to_csv('D:/.Memoria/Test/Jupyter/test_example_clean_test.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2.]\n",
      "[3. 4. 5.]\n",
      "[6. 7. 8.]\n",
      "[ 9. 10. 11.]\n",
      "[12. 13. 14.]\n",
      "[15.]\n"
     ]
    }
   ],
   "source": [
    "data = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12,13,14,15], dtype=np.float32) # list of data \n",
    "batches = []\n",
    "for x in batch(data, 3):\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "shape X_numpy_train: (1814, 1)\n",
      "shape X_numpy_test: (407, 1)\n",
      "shape y_numpy_train: (1814,)\n",
      "shape y_numpy_test: (407,)\n",
      "-----------------------\n",
      "shape X_train: torch.Size([1814, 1, 20, 34])\n",
      "shape X_test: torch.Size([407, 1, 20, 34])\n",
      "shape y_train: torch.Size([1814, 1])\n",
      "shape y_test: torch.Size([407, 1])\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    'matrix_format'\n",
    "] \n",
    "\n",
    "## FEATURES - TRAIN Y TEST\n",
    "X_df_train = df_train[features]\n",
    "X_df_test = df_test[features]\n",
    "X_numpy_train = X_df_train.to_numpy()\n",
    "X_numpy_test = X_df_test.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "## TARGETS - TRAIN Y TEST\n",
    "y_df_train = df_train['ql_rt_msec']\n",
    "y_df_test = df_test['ql_rt_msec']\n",
    "\n",
    "y_numpy_train = y_df_train.to_numpy().astype(np.float32)\n",
    "y_numpy_test = y_df_test.to_numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "print(\"-----------------------\")\n",
    "print(f'shape X_numpy_train: {X_numpy_train.shape}')\n",
    "print(f'shape X_numpy_test: {X_numpy_test.shape}')\n",
    "print(f'shape y_numpy_train: {y_numpy_train.shape}')\n",
    "print(f'shape y_numpy_test: {y_numpy_test.shape}')\n",
    "print(\"-----------------------\")\n",
    "\n",
    "\n",
    "# Pasarlos a Torch. \n",
    "#X_train = torch.from_numpy(X_numpy_train)\n",
    "#X_test = torch.from_numpy(X_numpy_test)\n",
    "\n",
    "#X_train = torch.tensor(df_train['matrix_format'].values)\n",
    "#X_test = torch.tensor(df_test['matrix_format'].values)\n",
    "\n",
    "\n",
    "X_train = CNN_Features_Format(X_numpy_train)\n",
    "X_test = CNN_Features_Format(X_numpy_test)\n",
    "\n",
    "y_train = torch.from_numpy(y_numpy_train)\n",
    "y_test = torch.from_numpy(y_numpy_test)\n",
    "\n",
    "\n",
    "# También pasar los targets de vector fila a vector columna\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "\n",
    "print(f'shape X_train: {X_train.shape}')\n",
    "print(f'shape X_test: {X_test.shape}')\n",
    "print(f'shape y_train: {y_train.shape}')\n",
    "print(f'shape y_test: {y_test.shape}')\n",
    "print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1814, 1, 20, 34])\n",
      "torch.Size([1814, 1])\n"
     ]
    }
   ],
   "source": [
    "#n_samples_train, n_features_train = X_train.shape\n",
    "#n_samples_test, n_features_test = X_test.shape\n",
    "#learning_rate = 0.01\n",
    "#hidden_size = 100000\n",
    "#input_size = n_features_train \n",
    "#print(f'n_samples_train: {n_samples_train}')\n",
    "#print(f'n_features_train: {n_features_train}')\n",
    "#print(f'n_samples_test: {n_samples_test}')\n",
    "#print(f'n_features_test: {n_features_test}')\n",
    "#print(f'learning_rate: {learning_rate}')\n",
    "#print(f'hidden_size: {hidden_size}')\n",
    "#print(f'input_size: {input_size}')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # SE DEFINEN LAS CAPAS A UTILIZAR, TANTO LAS CNN, FCL-ANN CY LAS DE POOLING\n",
    "        self.conv1 = nn.Conv2d(1, 3, (2,3),stride=(1,1)) \n",
    "        self.pool = nn.MaxPool2d((2,2), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(3, 9, (2,3),stride=(1,1)) \n",
    "        self.fc1 = nn.Linear(9*4*7, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 24)\n",
    "        self.fc4 = nn.Linear(24, 1)\n",
    "        \n",
    "        #Se pueden agregar mas capas o mas neurones, cambiar tamaños etc.. pero siempre debo terminar con\n",
    "        # una salida del tamaño que busco\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"input \",x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(\"conv1 \",x.shape)\n",
    "        x = self.pool(x)\n",
    "        #print(\"pooling \",x.shape)\n",
    "        \n",
    "        \n",
    "        #print(\"input \",x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(\"conv2 \",x.shape)\n",
    "        x = self.pool(x)\n",
    "        #print(\"pooling2 \",x.shape)\n",
    "        \n",
    "        \n",
    "        x = x.view(-1,9*4*7)           \n",
    "        #print(\"view: \",x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(\"fc1: \",x.shape)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #print(\"fc2: \",x.shape)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        #print(\"fc3: \",x.shape)\n",
    "        x =self.fc4(x)\n",
    "        #print(\"fc4: \",x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss y Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1000, loss = 514489.65625\n",
      "epoch: 2000, loss = 457755.96875\n",
      "epoch: 3000, loss = 435825.1875\n",
      "epoch: 4000, loss = 403251.6875\n",
      "epoch: 5000, loss = 366670.84375\n",
      "epoch: 6000, loss = 344630.21875\n",
      "epoch: 7000, loss = 325483.53125\n",
      "epoch: 8000, loss = 312953.25\n",
      "epoch: 9000, loss = 336783.84375\n",
      "epoch: 10000, loss = 265024.4375\n",
      "epoch: 11000, loss = 237643.234375\n",
      "epoch: 12000, loss = 207085.484375\n",
      "epoch: 13000, loss = 178938.4375\n",
      "epoch: 14000, loss = 161074.5625\n",
      "epoch: 15000, loss = 148166.5625\n",
      "epoch: 16000, loss = 159616.4375\n",
      "epoch: 17000, loss = 131810.171875\n",
      "epoch: 18000, loss = 126966.4921875\n",
      "epoch: 19000, loss = 125272.7421875\n",
      "epoch: 20000, loss = 120890.171875\n",
      "epoch: 21000, loss = 118686.7734375\n",
      "epoch: 22000, loss = 131188.875\n",
      "epoch: 23000, loss = 148847.84375\n",
      "epoch: 24000, loss = 123029.8671875\n",
      "epoch: 25000, loss = 119601.7734375\n",
      "epoch: 26000, loss = 115479.0546875\n",
      "epoch: 27000, loss = 110623.4921875\n",
      "epoch: 28000, loss = 106097.90625\n",
      "epoch: 29000, loss = 102528.296875\n",
      "epoch: 30000, loss = 97777.5859375\n",
      "epoch: 31000, loss = 94949.1953125\n",
      "epoch: 32000, loss = 92318.1328125\n",
      "epoch: 33000, loss = 89329.59375\n",
      "epoch: 34000, loss = 86794.265625\n",
      "epoch: 35000, loss = 107643.4453125\n",
      "epoch: 36000, loss = 82395.5\n",
      "epoch: 37000, loss = 101658.8671875\n",
      "epoch: 38000, loss = 83787.5390625\n",
      "epoch: 39000, loss = 83489.390625\n",
      "epoch: 40000, loss = 77693.4609375\n",
      "epoch: 41000, loss = 72698.296875\n",
      "epoch: 42000, loss = 71096.7578125\n",
      "epoch: 43000, loss = 67741.359375\n",
      "epoch: 44000, loss = 65537.0078125\n",
      "epoch: 45000, loss = 63085.37890625\n",
      "epoch: 46000, loss = 60616.98046875\n",
      "epoch: 47000, loss = 60132.53515625\n",
      "epoch: 48000, loss = 56822.7109375\n",
      "epoch: 49000, loss = 54833.78125\n",
      "epoch: 50000, loss = 52732.55078125\n",
      "epoch: 51000, loss = 51173.12109375\n",
      "epoch: 52000, loss = 52865.4453125\n",
      "epoch: 53000, loss = 50954.19921875\n",
      "epoch: 54000, loss = 49491.25390625\n",
      "epoch: 55000, loss = 48020.0703125\n",
      "epoch: 56000, loss = 47084.34765625\n",
      "epoch: 57000, loss = 45973.609375\n",
      "epoch: 58000, loss = 47506.76953125\n",
      "epoch: 59000, loss = 43634.47265625\n",
      "epoch: 60000, loss = 42748.19921875\n",
      "epoch: 61000, loss = 41748.1796875\n",
      "epoch: 62000, loss = 42637.76953125\n",
      "epoch: 63000, loss = 40445.38671875\n",
      "epoch: 64000, loss = 40446.90234375\n",
      "epoch: 65000, loss = 43844.46875\n",
      "epoch: 66000, loss = 41122.09765625\n",
      "epoch: 67000, loss = 38498.640625\n",
      "epoch: 68000, loss = 38286.12109375\n",
      "epoch: 69000, loss = 38804.48828125\n",
      "epoch: 70000, loss = 37770.9453125\n",
      "epoch: 71000, loss = 37155.86328125\n",
      "epoch: 72000, loss = 50128.01171875\n",
      "epoch: 73000, loss = 48483.109375\n",
      "epoch: 74000, loss = 47665.2421875\n",
      "epoch: 75000, loss = 46599.41796875\n",
      "epoch: 76000, loss = 47439.13671875\n",
      "epoch: 77000, loss = 45162.70703125\n",
      "epoch: 78000, loss = 48839.76171875\n",
      "epoch: 79000, loss = 47019.58984375\n",
      "epoch: 80000, loss = 43442.734375\n",
      "epoch: 81000, loss = 42933.16015625\n",
      "epoch: 82000, loss = 42202.203125\n",
      "epoch: 83000, loss = 42068.65234375\n",
      "epoch: 84000, loss = 41195.5859375\n",
      "epoch: 85000, loss = 40673.125\n",
      "epoch: 86000, loss = 40035.59375\n",
      "epoch: 87000, loss = 39448.34765625\n",
      "epoch: 88000, loss = 38692.03125\n",
      "epoch: 89000, loss = 38316.328125\n",
      "epoch: 90000, loss = 37763.65625\n",
      "epoch: 91000, loss = 51135.2578125\n",
      "epoch: 92000, loss = 45557.16796875\n",
      "epoch: 93000, loss = 43991.63671875\n",
      "epoch: 94000, loss = 41388.484375\n",
      "epoch: 95000, loss = 39173.05078125\n",
      "epoch: 96000, loss = 37946.40625\n",
      "epoch: 97000, loss = 36412.34765625\n",
      "epoch: 98000, loss = 36911.72265625\n",
      "epoch: 99000, loss = 35889.6171875\n",
      "epoch: 100000, loss = 40801.359375\n",
      "epoch: 101000, loss = 36290.61328125\n",
      "epoch: 102000, loss = 36254.47265625\n",
      "epoch: 103000, loss = 34543.5625\n",
      "epoch: 104000, loss = 33804.25\n",
      "epoch: 105000, loss = 33222.5234375\n",
      "epoch: 106000, loss = 32791.98046875\n",
      "epoch: 107000, loss = 32871.69921875\n",
      "epoch: 108000, loss = 32068.73046875\n",
      "epoch: 109000, loss = 36431.26171875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-edd547aff028>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#print(y_train.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_iterations = 1000000\n",
    "for epoch in range(num_iterations):\n",
    "    y_hat = model(X_train)\n",
    "    #print(y_hat.shape)\n",
    "    #print(y_hat.shape)\n",
    "    #print(y_train.shape)\n",
    "    loss = criterion(y_hat,y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if (epoch+1)%1000==0:\n",
    "        print(f'epoch: {epoch+1}, loss = {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
