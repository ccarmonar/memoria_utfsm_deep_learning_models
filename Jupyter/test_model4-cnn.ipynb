{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.preprocessing import StandardScaler#para escalar caracteristicas\n",
    "from sklearn.model_selection import train_test_split # separar mas facil la data de train y test\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from ast import literal_eval\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PaddingSameSize(matrix,padd_type='constant',constant_value=0):\n",
    "    length = np.array([len(matrix[i]) for i in range(len(matrix))])\n",
    "    width = length.max()\n",
    "    return_list=[]\n",
    "    for i in range(len(matrix)):\n",
    "        if len(matrix[i]) != width:\n",
    "            if padd_type == 'constant':\n",
    "                padd = np.pad(matrix[i], (0,width-len(matrix[i])), 'constant',constant_values = 0)\n",
    "            else:\n",
    "                padd = np.pad(matrix[i], (0,width-len(matrix[i])), padd_type)\n",
    "        else:\n",
    "            padd = matrix[i]\n",
    "        return_list.append(padd)\n",
    "    return_list = np.array(return_list)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MatrixFormat_To_Vector(df_matrix_format):\n",
    "    df_matrix_format = df_matrix_format.apply(lambda x: np.asarray(literal_eval(x)).astype(np.float32)) \n",
    "    max_x,max_y = {\"index\":0,\"value\":0},{\"index\":0,\"value\":0}\n",
    "    vector_list = []\n",
    "    vector_list_pad = []\n",
    "    max_len = 0\n",
    "    for i, matrix in df_matrix_format.items():\n",
    "    #    if max_x['value'] < matrix.shape[0]:\n",
    "    #        max_x['value'] = matrix.shape[0]\n",
    "    #        max_x['index'] = i\n",
    "    #    if max_y['value'] < matrix.shape[1]:\n",
    "    #        max_y['value'] = matrix.shape[1]\n",
    "    #        max_y['index'] = i\n",
    "        vector = np.reshape(matrix, -1)\n",
    "        vector_list.append(vector)\n",
    "\n",
    "    #for v in vector_list:\n",
    "    #    if max_len < len(v):\n",
    "    #        max_len = len(v)\n",
    "    #print(\"max len\", max_len)\n",
    "    vector_list_pad = PaddingSameSize(vector_list, 'constant')      \n",
    "    \n",
    "    #for i in vector_list_pad:\n",
    "    #    print(i.shape)\n",
    "    #    print(i)\n",
    "    \n",
    "    return vector_list_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveOversizedMatrix(df_clean,max_r=20,max_c=40):\n",
    "    arr_shape_r = []\n",
    "    arr_shape_c = []\n",
    "    print(\"Total df_clean: \",len(df_clean))\n",
    "    eliminados = []\n",
    "    for i, v in df_clean['matrix_format'].iteritems():\n",
    "        r,c = v.shape\n",
    "        if r > 20 or c > 40:\n",
    "            eliminados.append(i)\n",
    "    print(\"Eliminados por Oversized:\", len(eliminados))\n",
    "\n",
    "    new_df_clean = df_clean.drop(eliminados)\n",
    "\n",
    "    for i, v in new_df_clean['matrix_format'].iteritems():\n",
    "        r,c = v.shape\n",
    "        arr_shape_r.append(r)\n",
    "        arr_shape_c.append(c)\n",
    "        \n",
    "    max_new_r = max(arr_shape_r)\n",
    "    max_new_c = max(arr_shape_c)\n",
    "    min_new_r = min(arr_shape_r)\n",
    "    min_new_c = min(arr_shape_c)\n",
    "    mean_new_r = np.ceil(np.mean(arr_shape_r))\n",
    "    mean_new_c = np.ceil(np.mean(arr_shape_c))\n",
    "\n",
    "    return new_df_clean,max_new_r,max_new_c,min_new_r,min_new_c,mean_new_r,mean_new_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StandardSize_Padding(matrix,num_standard_rows,num_standard_columns):\n",
    "    padd_columns = []\n",
    "    for i in range(len(matrix)):\n",
    "        if len(matrix[i]) < num_standard_columns:\n",
    "            padd = np.pad(matrix[i], (0,num_standard_columns-len(matrix[i])), 'mean')\n",
    "            padd_columns.append(padd)\n",
    "        else:\n",
    "            padd = matrix[i]\n",
    "            padd_columns.append(padd)\n",
    "    padd_rows = []\n",
    "    tp_matrix = np.asarray(padd_columns, dtype=np.float32).T\n",
    "    for i in range(len(tp_matrix)):\n",
    "        if len(tp_matrix[i]) < num_standard_rows:\n",
    "            padd = np.pad(tp_matrix[i], (0,num_standard_rows-len(tp_matrix[i])), 'mean')\n",
    "            padd_rows.append(padd)\n",
    "        else:\n",
    "            padd = tp_matrix[i]\n",
    "            padd_rows.append(padd)\n",
    "    return np.asarray(padd_rows, dtype=np.float32).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_Features_Format(numpy_array):\n",
    "    ### Pasarlos a 1 \"canal\"\n",
    "    canal = []\n",
    "    #print(\"shape X_numpy_train: \",numpy_array.shape)\n",
    "    #print(\"type X_numpy_train: \",type(numpy_array), numpy_array.dtype)\n",
    "    for i in numpy_array:\n",
    "        for j in i:\n",
    "            k = np.array([j])\n",
    "            canal.append(k)\n",
    "    canal = np.array(canal)\n",
    "    torch_canal = torch.from_numpy(canal)\n",
    "    return torch_canal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_format(X_train, y_train, n=1):\n",
    "    batches = {\"features\":[],\"labels\":[],}\n",
    "    for x in batch(X_train, n):\n",
    "        batches[\"features\"].append(x)\n",
    "    for y in batch(y_train, n):\n",
    "        batches[\"labels\"].append(y)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Esto garantiza que se ejecutara en GPU si esta disponible\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_r = 40 ## Maximo numero de subconsultas\n",
    "max_c = 50 ## Maximo numero de subcarateristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('D:/.Memoria/Test/Jupyter/test_example.csv')\n",
    "columns = list(df_raw.columns)\n",
    "print(columns)\n",
    "#  Eliminar los con tiempos de ejecución 0 o muy altos\n",
    "df_clean = shuffle(df_raw[(df_raw['ql_rt_msec'] > -1) & (df_raw['ql_rt_msec'] < 1e3)])\n",
    "df_clean.to_csv('D:/.Memoria/Test/Jupyter/test_example_clean.csv',index=False)\n",
    "\n",
    "### HAY QUE CONVERTIR ALGUNAS COLUMNAS Q ESTAN EN PORCENTAJE A FLOAT\n",
    "df_clean['ql_rt_clocks'] = df_clean['ql_rt_clocks'].apply(lambda x: float(x.strip('%'))/100)\n",
    "df_clean['ql_same_seg'] = df_clean['ql_same_seg'].apply(lambda x: float(x.strip('%'))/100)\n",
    "df_clean['ql_same_page'] = df_clean['ql_same_page'].apply(lambda x: float(x.strip('%'))/100)\n",
    "df_clean['ql_cl_wait_clocks'] = df_clean['ql_cl_wait_clocks'].apply(lambda x: float(x.strip('%'))/100)\n",
    "df_clean['ql_c_clocks'] = df_clean['ql_c_clocks'].apply(lambda x: float(x.strip('%'))/100)\n",
    "df_clean['ql_c_cl_wait'] = df_clean['ql_c_cl_wait'].apply(lambda x: float(x.strip('%'))/100)\n",
    "\n",
    "# Convertir MatrixFormat de STR a np.array\n",
    "df_clean['matrix_format'] = df_clean['matrix_format'].apply(lambda x: np.asarray(literal_eval(x)).astype(np.float32)) \n",
    "\n",
    "## shape de raw y clean\n",
    "print(f'shape df_raw: {df_raw.shape}')\n",
    "print(f'shape df_clean: {df_clean.shape}')\n",
    "\n",
    "\n",
    "new_df_clean,max_new_r,max_new_c,min_new_r,min_new_c,mean_new_r,mean_new_c = RemoveOversizedMatrix(df_clean,max_r,max_c)\n",
    "\n",
    "num_standard_rows = max_new_r\n",
    "num_standard_columns = max_new_c\n",
    "new_df_clean['matrix_format'] = df_clean['matrix_format'].apply(lambda x: StandardSize_Padding(x,num_standard_rows,num_standard_columns)) \n",
    "\n",
    "print(f'shape new_df_clean: {new_df_clean.shape}')\n",
    "\n",
    "msk = np.random.rand(len(new_df_clean)) <= 0.8\n",
    "df_train = new_df_clean[msk]\n",
    "df_test = new_df_clean[~msk]\n",
    "\n",
    "df_train.to_csv('D:/.Memoria/Test/Jupyter/test_example_clean_train.csv',index=False)\n",
    "df_test.to_csv('D:/.Memoria/Test/Jupyter/test_example_clean_test.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    'matrix_format'\n",
    "] \n",
    "\n",
    "## FEATURES - TRAIN Y TEST\n",
    "X_df_train = df_train[features]\n",
    "X_df_test = df_test[features]\n",
    "X_numpy_train = X_df_train.to_numpy()\n",
    "X_numpy_test = X_df_test.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "## TARGETS - TRAIN Y TEST\n",
    "y_df_train = df_train['ql_rt_msec']\n",
    "y_df_test = df_test['ql_rt_msec']\n",
    "\n",
    "y_numpy_train = y_df_train.to_numpy().astype(np.float32)\n",
    "y_numpy_test = y_df_test.to_numpy().astype(np.float32)\n",
    "\n",
    "\n",
    "print(\"-----------------------\")\n",
    "print(f'shape X_numpy_train: {X_numpy_train.shape}')\n",
    "print(f'shape X_numpy_test: {X_numpy_test.shape}')\n",
    "print(f'shape y_numpy_train: {y_numpy_train.shape}')\n",
    "print(f'shape y_numpy_test: {y_numpy_test.shape}')\n",
    "print(\"-----------------------\")\n",
    "\n",
    "\n",
    "# Pasarlos a Torch. \n",
    "#X_train = torch.from_numpy(X_numpy_train)\n",
    "#X_test = torch.from_numpy(X_numpy_test)\n",
    "\n",
    "#X_train = torch.tensor(df_train['matrix_format'].values)\n",
    "#X_test = torch.tensor(df_test['matrix_format'].values)\n",
    "\n",
    "\n",
    "X_train = CNN_Features_Format(X_numpy_train)\n",
    "X_test = CNN_Features_Format(X_numpy_test)\n",
    "\n",
    "y_train = torch.from_numpy(y_numpy_train)\n",
    "y_test = torch.from_numpy(y_numpy_test)\n",
    "\n",
    "\n",
    "# También pasar los targets de vector fila a vector columna\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "\n",
    "print(f'shape X_train: {X_train.shape}')\n",
    "print(f'shape X_test: {X_test.shape}')\n",
    "print(f'shape y_train: {y_train.shape}')\n",
    "print(f'shape y_test: {y_test.shape}')\n",
    "print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_samples_train, n_features_train = X_train.shape\n",
    "#n_samples_test, n_features_test = X_test.shape\n",
    "#\n",
    "#hidden_size = 100000\n",
    "#input_size = n_features_train \n",
    "#print(f'n_samples_train: {n_samples_train}')\n",
    "#print(f'n_features_train: {n_features_train}')\n",
    "#print(f'n_samples_test: {n_samples_test}')\n",
    "#print(f'n_features_test: {n_features_test}')\n",
    "#print(f'learning_rate: {learning_rate}')\n",
    "#print(f'hidden_size: {hidden_size}')\n",
    "#print(f'input_size: {input_size}')\n",
    "learning_rate = 0.001\n",
    "batches_size = 100\n",
    "num_epochs = 10000\n",
    "num_batches = math.ceil(X_train.shape[0]/batches_size)\n",
    "batches = batch_format(X_train,y_train,batches_size)\n",
    "for i in range(num_batches):\n",
    "    print(batches[\"features\"][i].shape)\n",
    "    print(batches[\"labels\"][i].shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # SE DEFINEN LAS CAPAS A UTILIZAR, TANTO LAS CNN, FCL-ANN CY LAS DE POOLING\n",
    "        self.conv1 = nn.Conv2d(1, 6, (3,5),stride=(1,1)) \n",
    "        self.pool = nn.MaxPool2d((2,2), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(6, 12, (3,5),stride=(1,1)) \n",
    "        self.fc1 = nn.Linear(12*3*6, 162)\n",
    "        self.fc2 = nn.Linear(162, 84)\n",
    "        self.fc3 = nn.Linear(84, 42)\n",
    "        self.fc4 = nn.Linear(42, 20)\n",
    "        self.fc5 = nn.Linear(20, 1)\n",
    "        \n",
    "        #Se pueden agregar mas capas o mas neurones, cambiar tamaños etc.. pero siempre debo terminar con\n",
    "        # una salida del tamaño que busco\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"input \",x.shape)\n",
    "        x = F.leaky_relu(self.conv1(x), negative_slope=0.01, inplace=False)\n",
    "        #print(\"conv1 \",x.shape)\n",
    "        x = self.pool(x)\n",
    "        #print(\"pooling \",x.shape)\n",
    "        \n",
    "        \n",
    "        #print(\"input \",x.shape)\n",
    "        x = F.leaky_relu(self.conv2(x), negative_slope=0.01, inplace=False)\n",
    "        #print(\"conv2 \",x.shape)\n",
    "        x = self.pool(x)\n",
    "        #print(\"pooling2 \",x.shape)\n",
    "        \n",
    "        \n",
    "        x = x.view(-1,12*3*6)           \n",
    "        #print(\"view: \",x.shape)\n",
    "        x = F.leaky_relu(self.fc1(x), negative_slope=0.01, inplace=False)\n",
    "        #print(\"fc1: \",x.shape)\n",
    "        x = F.leaky_relu(self.fc2(x), negative_slope=0.01, inplace=False)\n",
    "        #print(\"fc2: \",x.shape)\n",
    "        x = F.leaky_relu(self.fc3(x), negative_slope=0.01, inplace=False)\n",
    "        #print(\"fc3: \",x.shape)\n",
    "        x = F.leaky_relu(self.fc4(x), negative_slope=0.01, inplace=False)\n",
    "        #print(\"fc4: \",x.shape)\n",
    "        x =self.fc5(x)\n",
    "        #print(\"fc5: \",x.shape)\n",
    "        #print(\"---------------------------------------------------------------\")\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss y Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(num_batches)\n",
    "for epoch in range(num_epochs):\n",
    "    for bt in range(num_batches):\n",
    "        #print(batches[\"features\"][bt].shape)\n",
    "        #print(batches[\"labels\"][bt].shape)\n",
    "        y_hat = model(batches[\"features\"][i])\n",
    "        loss = criterion(y_hat,batches[\"labels\"][i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if bt+1 == num_batches and (epoch+1) % 100 == 0:\n",
    "            print(f'epoch: [{epoch+1}/{num_epochs}], loss = {loss.item()}')\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASICO\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(batches_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batches_size_test = 5\n",
    "num_batches_test = math.ceil(X_test.shape[0]/batches_size_test)\n",
    "batches_test = batch_format(X_test,y_test,batches_size_test)\n",
    "#for i in range(num_batches_test):\n",
    "#    print(batches_test[\"features\"][i].shape)\n",
    "#    print(batches_test[\"labels\"][i].shape)\n",
    "#print(X_test.shape)\n",
    "#print(y_test.shape)\n",
    "\n",
    "y_pred = model(batches_test[\"features\"][0])\n",
    "y = batches_test[\"labels\"][0]\n",
    "print(y_pred)\n",
    "\n",
    "print(y)\n",
    "\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 100 # milisegundos de tolerancia\n",
    "num_aciertos = 0\n",
    "num_test = float(y_test.shape[0])\n",
    "with torch.no_grad():\n",
    "    for bt in range(num_batches_test):\n",
    "        #print(batches[\"features\"][bt].shape)\n",
    "        #print(batches[\"labels\"][bt].shape)\n",
    "        y_pred_tensor = model(batches_test[\"features\"][bt])\n",
    "        y_tensor = batches_test[\"labels\"][bt]\n",
    "        for i in range(y_tensor.shape[0]):\n",
    "            y_pred = y_pred_tensor[i].item()\n",
    "            y = y_tensor[i].item()\n",
    "            print(y, y_pred)\n",
    "            if abs(y_pred-y) < tol:\n",
    "                num_aciertos += 1 \n",
    "            \n",
    "    accuracy = num_aciertos/num_test\n",
    "\n",
    "print(f'achuntes: [{num_aciertos}/{num_test}]')\n",
    "print(f'accuracy: {accuracy*100}')\n",
    "      \n",
    "      \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
