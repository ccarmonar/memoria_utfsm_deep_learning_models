{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcular gradientes con pytorch\n",
    "Se utiliza el paquete autograd de pytroch que simplifica mucho el proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supongamos que quieremos los gradientes de un tensor random\n",
      "tensor([0.9408, 0.2847, 0.5905], requires_grad=True)\n",
      "Cada vez que hacemos operaciones en este tensor pytorch creara un grafo computacional para nosotros, por ejemplo\n",
      "en sintesis, tenemos un nodo con entradas y salidas\n",
      "revisar https://youtu.be/c36lUUr864M?list=RDCMUCbXgNpp0jedKWcQiULLbDTA&t=1666\n",
      "sirve harto pal forward/backpropagation, concepto de ML\n",
      "si vemos la operaciones que hicimos se agrega el AddBackward\n",
      "tensor([2.9408, 2.2847, 2.5905], grad_fn=<AddBackward0>)\n",
      "Lo mismo con la multiplicaciones MultBackward\n",
      "tensor([17.2968, 10.4400, 13.4212], grad_fn=<MulBackward0>)\n",
      "incluso la media\n",
      "tensor(13.7193, grad_fn=<MeanBackward0>)\n",
      "para calcular el o los gradiente basta con call la funcion backward\n",
      "tensor([3.9211, 3.0463, 3.4540])\n",
      "Recuerde que en el fondo estamos creando el Jacobiano y no solo 1 vetor gradiente\n",
      "revisar https://youtu.be/c36lUUr864M?list=RDCMUCbXgNpp0jedKWcQiULLbDTA&t=1898\n",
      "como este caso es escalar no hay que especifivar nada en z backward\n"
     ]
    }
   ],
   "source": [
    "print(\"supongamos que quieremos los gradientes de un tensor random\")\n",
    "x = torch.rand(3, requires_grad=True) #Recordar el requires_grad, si es False genera error\n",
    "print(x)\n",
    "\n",
    "print(\"Cada vez que hacemos operaciones en este tensor pytorch creara un grafo computacional para nosotros, por ejemplo\")\n",
    "y = x + 2\n",
    "\n",
    "print(\"en sintesis, tenemos un nodo con entradas y salidas\")\n",
    "print(\"revisar https://youtu.be/c36lUUr864M?list=RDCMUCbXgNpp0jedKWcQiULLbDTA&t=1666\")\n",
    "print(\"sirve harto pal forward/backpropagation, concepto de ML\")\n",
    "print(\"si vemos la operaciones que hicimos se agrega el AddBackward\")\n",
    "print(y) #observe que se agrega el backward\n",
    "print(\"Lo mismo con la multiplicaciones MultBackward\")\n",
    "z = y*y*2\n",
    "print(z)\n",
    "print(\"incluso la media\")\n",
    "z = z.mean()\n",
    "print(z)\n",
    "print(\"para calcular el o los gradiente basta con call la funcion backward\")\n",
    "z.backward() # dz/dx\n",
    "print(x.grad) \n",
    "print(\"Recuerde que en el fondo estamos creando el Jacobiano y no solo 1 vetor gradiente\")\n",
    "print(\"revisar https://youtu.be/c36lUUr864M?list=RDCMUCbXgNpp0jedKWcQiULLbDTA&t=1898\")\n",
    "print(\"como este caso es escalar no hay que especifivar nada en z backward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismo caso pero ahora z no es escalar (quitamos el mean), el backward generará error\n",
      "tensor([0.3977, 0.5265, 0.8670], requires_grad=True)\n",
      "tensor([2.3977, 2.5265, 2.8670], grad_fn=<AddBackward0>)\n",
      "tensor([11.4977, 12.7669, 16.4398], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-2cf75a12fdd2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#z = z.mean()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#print(z)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# dz/dx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "print(\"Mismo caso pero ahora z no es escalar (quitamos el mean), el backward generará error, ya que estamos calculando el JACOBIANO\")\n",
    "x = torch.rand(3, requires_grad=True) #Recordar el requires_grad, si es False genera error\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "\n",
    "print(y) #observe que se agrega el backward\n",
    "\n",
    "z = y*y*2\n",
    "print(z)\n",
    "#z = z.mean()\n",
    "#print(z)\n",
    "z.backward() # dz/dx\n",
    "print(x.grad) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En este caso tenemos que darle de argumento un vector del mismo tamaño\n",
      "x =  tensor([1., 1., 1.], requires_grad=True)\n",
      "y =  tensor([3., 3., 3.], grad_fn=<AddBackward0>)\n",
      "z =  tensor([18., 18., 18.], grad_fn=<MulBackward0>)\n",
      "x_grad =  tensor([ 1.2000, 12.0000,  0.0120])\n"
     ]
    }
   ],
   "source": [
    "print(\"En este caso tenemos que darle de argumento un vector del mismo tamaño\")\n",
    "x = torch.ones(3, requires_grad=True) #Recordar el requires_grad, si es False genera error\n",
    "print(\"x = \",x)\n",
    "\n",
    "y = x + 2\n",
    "\n",
    "print(\"y = \",y) #observe que se agrega el backward\n",
    "\n",
    "z = y*y*2\n",
    "print(\"z = \",z)\n",
    "#z = z.mean()\n",
    "#print(z)\n",
    "v = torch.tensor([0.1,1.0,0.001],dtype=torch.float32) #aqui le pase un vector con constantes distintas\n",
    "#v = torch.tensor([1.0,1.0,1.0],dtype=torch.float32) #aqui le pase un vector con constantes IGUALES\n",
    "z.backward(v) # dz/dx \n",
    "print(\"x_grad = \",x.grad) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prevenir el 'tracking' de gradiantes: https://youtu.be/c36lUUr864M?list=RDCMUCbXgNpp0jedKWcQiULLbDTA&t=2062\n",
      "tensor([0.2569, 0.5557, 0.0789], requires_grad=True)\n",
      "opcion 3 with torch.no_grad():\n",
      "tensor([2.2569, 2.5557, 2.0789])\n"
     ]
    }
   ],
   "source": [
    "  print(\"prevenir el 'tracking' de gradiantes: https://youtu.be/c36lUUr864M?list=RDCMUCbXgNpp0jedKWcQiULLbDTA&t=2062\")\n",
    "x = torch.rand(3, requires_grad=True)\n",
    "print(x)\n",
    "#print(\"opcion 1 x.requires_grad_(False)\")\n",
    "#x.requires_grad_(False)\n",
    "#print(x)\n",
    "#print(x+9)\n",
    "#print(\"opcion 2 x.detach(): crear un nuevo vector pero sin grad\")\n",
    "#x.detach()\n",
    "#v = x.detach()\n",
    "#print(v)\n",
    "print(\"opcion 3 with torch.no_grad():\")\n",
    "with torch.no_grad():\n",
    "    y = x + 2\n",
    "    print(y)\n",
    "\n",
    "print(\"Cada vez que hacemos backrpro entonces el gradientes de un tensor determinado se ira acumulanndo. Hay que tener cuidadp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ejemplito\n",
      "tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "trining loop\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n",
      "Se acomulan los valores de grad!!, dando valores incorrectos!!\n",
      "con optimizacion\n",
      "tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "trining loop\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "print(\"ejemplito\")\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "print(weights)\n",
    "print(\"trining loop\")\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output=(weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "\n",
    "print(\"Se acomulan los valores de grad!!, dando valores incorrectos!!\")\n",
    "\n",
    "print(\"con optimizacion\")\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "print(weights)\n",
    "print(\"trining loop\")\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output=(weights*3).sum()\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
